from pydantic import BaseModel, Field
from typing_extensions import TypedDict, Required, NotRequired
from typing import Optional
from uuid import UUID
from app.models.SQL_models import ConversationType, ChatRole


# models for chat requests
class Tool(BaseModel):
    name: str
    enabled: bool


class MetadataModel(BaseModel):
    language: str = Field(
        description="Language in which the user's question is written and in which the Assistant should respond"
    )
    session_id: Optional[UUID] = Field(
        description="session id which can be used to retrieve conversation history later. Should be generated at the start of each new conversation"
    )
    tools: list[Tool] = Field(description="List of tools the LLM agent can use.")


class UserModel(BaseModel):
    user_id: int
    question: str
    context: Optional[list[dict]] = []


class RequestModel(BaseModel):
    metadata: object
    user: object


# models for chat response
class SourceDict(TypedDict):
    id: Required[int]
    type: Required[str]
    url: NotRequired[Optional[str]]
    uri: NotRequired[Optional[str]]
    text: NotRequired[Optional[str]]


class ResponseModel(BaseModel):
    content: Optional[str] = Field(
        description="The answer generated by the LLM in response to the user's question.",
        default=None,
    )
    able_to_answer: Optional[bool] = Field(
        description="Indicates whether the LLM was able to generate a confident answer.",
        default=None,
    )
    question_classification: Optional[str] = Field(
        description="The category or classification of the question.", default=None
    )
    follow_up_questions: Optional[list[str]] = Field(
        description="Suggested follow-up questions for the user.", default=None
    )


class ResponseMetadata(BaseModel):
    sources: list[SourceDict] = Field(
        description="List of sources referenced by the LLM to answer the question."
    )
    tools_used: list[str] = Field(
        description="List of tools that were used to generate the response."
    )
    session_id: UUID = Field(
        description="The session ID associated with the interaction."
    )
    trace_id: str = Field(
        description="The trace ID for debugging or monitoring purposes."
    )


# models for database requests
class SearchRequest(BaseModel):
    query: str
    tool_call_attempt: int
    limit: int = 5


# conversation history
class MessageModel(BaseModel):
    role: ChatRole
    content: str
    context: Optional[list[dict]] = []


class ChatHistoryModel(BaseModel):
    metadata: MetadataModel
    messages: list[MessageModel]


class ConversationModel(BaseModel):
    session_id: UUID
    user_id: int
    type: ConversationType
    title: str
    metadata: MetadataModel


# models for evaluation
class JudgeInput(BaseModel):
    question: str = Field(description="Question to evaluate")
    golden_answer: str = Field(
        description="Ideal answer to the question to compare the actual answer with"
    )
    llm_answer: str = Field(description="Actual answer the LLM generated to evaluate.")
    source_documents: list[str]


class JudgeOutput(BaseModel):
    score: float = Field(description="Final score of the generated answer between 0-10")
    summary: str = Field(
        description="Brief evaluation of the generated answer and explanation of how you decided on the score"
    )


# Evaluation criteria models
class CriterionScore(BaseModel):
    score: float = Field(..., ge=0, le=10, description="Score from 0-10")
    explanation: str = Field(..., description="Brief explanation for the score")


class ResponseEvaluation(BaseModel):
    factual_accuracy: CriterionScore = Field(
        ..., description="Measures correctness of information"
    )
    completeness: CriterionScore = Field(
        ..., description="Measures if all aspects of the question are addressed"
    )
    relevance: CriterionScore = Field(
        ..., description="Measures if information is directly relevant"
    )
    clarity: CriterionScore = Field(
        ..., description="Measures if the response is clear and understandable"
    )
    conciseness: CriterionScore = Field(
        ..., description="Measures if the response is appropriately concise"
    )
    total_score: float = Field(..., ge=0, le=50, description="Sum of all scores")
    evaluation_summary: str = Field(..., description="Brief overall assessment")
