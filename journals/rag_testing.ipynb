{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel\n",
    "from lancedb.embeddings import get_registry\n",
    "from typing import List\n",
    "from pydantic_ai import Agent\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai.models.gemini import GeminiModel\n",
    "import os\n",
    "import numpy as np\n",
    "import enum\n",
    "\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "\n",
    "import re\n",
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "from urllib.parse import quote\n",
    "from uuid import UUID, uuid4\n",
    "import cohere\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sqlalchemy import create_engine, Column, Integer, String, LargeBinary\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker, Session, scoped_session, Mapped, relationship\n",
    "from sqlalchemy.dialects.postgresql import UUID, TSVECTOR\n",
    "from pgvector.sqlalchemy import Vector\n",
    "from sqlalchemy.sql import func\n",
    "\n",
    "from sqlalchemy import (\n",
    "    Column,\n",
    "    Integer,\n",
    "    String,\n",
    "    Boolean,\n",
    "    DateTime,\n",
    "    ForeignKey,\n",
    "    JSON,\n",
    "    Text,\n",
    "    VARCHAR,\n",
    "    Enum,\n",
    "    Index,\n",
    "    desc,\n",
    "    text\n",
    "\n",
    "    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBEDDING_SIZE = 1536\n",
    "Base = declarative_base()\n",
    "\n",
    "class DocumentType(enum.Enum):\n",
    "    MD = 'markdown'\n",
    "    TXT = 'text'\n",
    "    PDF = 'pdf'\n",
    "\n",
    "class DocumentSubject(enum.Enum):\n",
    "    FINANCE = \"finance\"\n",
    "    HR = 'hr'\n",
    "    TECH = 'tech'\n",
    "\n",
    "# declare models\n",
    "class Chunk(Base):\n",
    "    __tablename__ = \"chunks\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    document_id = Column(UUID(as_uuid=True), ForeignKey(\"documents.id\"))\n",
    "    chunk_metadata = Column(JSON)\n",
    "    chunk = Column(String, nullable=False)\n",
    "    embedding = Column(Vector(1536))  # Adjust vector dimension to match your embeddings\n",
    "    document: Mapped[\"Document\"] = relationship(back_populates=\"chunks\") # creates a link to related document\n",
    "    \n",
    "    # ---- ADDED FOR FTS ----\n",
    "    chunk_tsv = Column(TSVECTOR) # The new column for Full-Text Search vectors\n",
    "    # ---- END ADDED FOR FTS ----\n",
    "\n",
    "    # ---- ADDED/MODIFIED FOR INDEXES ----\n",
    "    __table_args__ = (\n",
    "        # Index for Full-Text Search on the tsvector column\n",
    "        Index(\n",
    "            'idx_gin_chunk_tsv',        # Index name\n",
    "            'chunk_tsv',                # Column to index\n",
    "            postgresql_using='gin'      # Index type (GIN is best for tsvector)\n",
    "        ),\n",
    "        # Index for Vector Search on the embedding column (choose ONE method)\n",
    "        # Option 1: HNSW (Good balance, requires pgvector >= 0.5.0)\n",
    "        Index(\n",
    "            'idx_hnsw_embedding',       # Index name\n",
    "            'embedding',                # Column to index\n",
    "            postgresql_using='hnsw',    # Index type\n",
    "            postgresql_with={'m': 16, 'ef_construction': 64}, # Example parameters (tune these)\n",
    "            postgresql_ops={'embedding': 'vector_cosine_ops'} # Operator class (use cosine, l2, or ip based on your distance metric)\n",
    "        ))\n",
    "    \n",
    "    class Config:\n",
    "        orm_mode = True\n",
    "\n",
    "class Document(Base):\n",
    "    __tablename__ = \"documents\"\n",
    "\n",
    "    pk = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    id = Column(UUID(as_uuid=True), nullable=False, unique=True)\n",
    "    title = Column(String(255))\n",
    "    type = Column(Enum(DocumentType), name = \"filetype of document\")\n",
    "    subject = Column(Enum(DocumentSubject), name = \"subject of document\")\n",
    "    location = Column(String(255))\n",
    "    created_at = Column(DateTime(timezone=True), server_default=func.now())\n",
    "    chunks: Mapped[list[\"Chunk\"]] = relationship()\n",
    "    \n",
    "    class Config:\n",
    "        orm_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_db_url = \"postgresql+psycopg://postgres:password@localhost:5432/test_db\"\n",
    "handbook_db_url = \"postgresql+psycopg://postgres:password@localhost:5432/handbook_db\"\n",
    "engine = create_engine(\n",
    "    url = handbook_db_url,\n",
    "    )\n",
    "sessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "Base.metadata.create_all(engine)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = \"../data/handbook-main-content\"\n",
    "RESULT_PATH = \"../data/rag-results/\"\n",
    "GOLDEN_TEST_PATH = \"../data/gitlab-handbook-golden-test-set.csv\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "def split_markdown_text(text: str, embedding_model: str, document_id: UUID, min_tokens: int = 200, max_tokens: int = 1000, overlap: int = 50):\n",
    "    md_regex = r\"(^#+\\s*.*)\" #regex which captures all levels of headers in markdown.\n",
    "    tokenizer = tiktoken.encoding_for_model(embedding_model)\n",
    "    chunks = []\n",
    "    temp_chunk = \"\"\n",
    "    temp_headers = \"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size = max_tokens, chunk_overlap = overlap)\n",
    "\n",
    "\n",
    "    # helper function to create and add chunk to list. Splits chunk if it exceeds max token size\n",
    "    def add_chunk(chunk, context = \"\"):\n",
    "        chunk = f\"{context} \\n {chunk}\"\n",
    "        if len(tokenizer.encode(chunk)) < max_tokens:\n",
    "                chunks.append(Chunk(\n",
    "                    document_id=document_id,\n",
    "                    context=context.strip(),\n",
    "                    chunk=chunk.strip()\n",
    "                ))\n",
    "        else:\n",
    "            split_chunks = splitter.split_text(chunk)\n",
    "            for part in split_chunks:\n",
    "                chunks.append(Chunk(\n",
    "                document_id=document_id,\n",
    "                context=context,\n",
    "                chunk=part\n",
    "            ))\n",
    "                \n",
    "    \n",
    "    # Helper function to add a merged chunk if present.\n",
    "    def flush_temp_chunk():\n",
    "        nonlocal temp_chunk, temp_headers\n",
    "        if temp_chunk:\n",
    "            add_chunk(temp_chunk, temp_headers)\n",
    "            temp_chunk, temp_headers = \"\", \"\"\n",
    "\n",
    "    #split text by headers\n",
    "    sections = re.split(md_regex, text, flags=re.MULTILINE)\n",
    "\n",
    "    #capture first text which often does not start with a header\n",
    "    if len(tokenizer.encode(sections[0])) < min_tokens:\n",
    "        temp_chunk += sections[0] + \"\\n\"\n",
    "    else:\n",
    "        add_chunk(sections[0])\n",
    "\n",
    "    for i in range(1, len(sections), 2): # loop through headers and text in sections\n",
    "        header = sections[i].strip()\n",
    "        content = sections[i+1].strip() if i + 1 <= len(sections) else \"\"\n",
    "\n",
    "        token_count = len(tokenizer.encode(content))\n",
    "\n",
    "        # add chunk to chunk list or to temporary chunk to combine with other chunks\n",
    "        if token_count < min_tokens:\n",
    "            temp_chunk += content + \"\\n\"  \n",
    "            temp_headers += header +\"\\n\"        \n",
    "        else:\n",
    "            # add temp chunk if it exists\n",
    "            flush_temp_chunk()\n",
    "\n",
    "            add_chunk(content, header)\n",
    "           \n",
    "    # add remaining temp chunk if it exists\n",
    "    flush_temp_chunk()\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def parse_documents(folder_path, db, embed_model: str = EMBEDDING_MODEL, min_tokens: int = 200, max_tokens: int = 1000):\n",
    "    chunks = []\n",
    "    HANDBOOK_ROOT_URL = \"https://gitlab.com/gitlab-com/content-sites/handbook/-/tree/main/content\"\n",
    "\n",
    "    # clear tables\n",
    "    db.query(Chunk).delete()\n",
    "    db.query(Document).delete()\n",
    "    db.commit()\n",
    "\n",
    "    # walk through all folders and subfolders\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.md'):                  #only extract text from markdown files\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                # fix url to properly link to the handbook source\n",
    "                file_url = file_path.replace(\"../data/handbook-trimmed\\\\content\", HANDBOOK_ROOT_URL)\n",
    "                file_url = file_url.replace('\\\\', '/')\n",
    "                \n",
    "\n",
    "                # add document to database\n",
    "                doc = Document(\n",
    "                    id = uuid4(),\n",
    "                    title = file.split(\".md\")[0],\n",
    "                    type = DocumentType.MD,\n",
    "                    location = file_url)\n",
    "                db.add(doc)\n",
    "\n",
    "                with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                    chunks.extend(split_markdown_text(content, embed_model, doc.id, min_tokens, max_tokens))\n",
    "\n",
    "    print(f\"Total amount of chunks: {len(chunks)}\")\n",
    "    db.commit()\n",
    "    return chunks\n",
    "\n",
    "def remove_small_chunks(chunks, min_tokens = 200, embedding_model = EMBEDDING_MODEL):\n",
    "    tokenizer = tiktoken.encoding_for_model(embedding_model)\n",
    "    total_count = 0\n",
    "    trimmed_chunks = []\n",
    "    for chunk in chunks:\n",
    "        \n",
    "        count = len(tokenizer.encode(chunk.chunk))\n",
    "        if count > min_tokens:\n",
    "            trimmed_chunks.append(chunk)\n",
    "\n",
    "    print(f\"amount of trimmed chunks: {len(trimmed_chunks)}\")\n",
    "    return trimmed_chunks\n",
    "\n",
    "def create_embeddings(chunks, batch_size = 500, model=EMBEDDING_MODEL):\n",
    "    client = OpenAI()\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        texts = [chunk.chunk for chunk in batch]\n",
    "        try:\n",
    "            response = client.embeddings.create(input = texts, model=model)\n",
    "            print(\"batch done\")\n",
    "                        # Extract the embeddings from the response\n",
    "            embeddings = [entry.embedding for entry in response.data]\n",
    "            for chunk, embedding in zip(batch, embeddings):\n",
    "                chunk.embedding = embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Embedding failed with error: {e}\")\n",
    "    \n",
    "def add_chunks_to_db(chunks, db):\n",
    "   \n",
    "    for chunk in chunks:\n",
    "        db.add(chunk)\n",
    "\n",
    "    db.commit()\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedded_chunks(db: Session, data_path: str = DATA_PATH, embed_model: str = EMBEDDING_MODEL, min_tokens: int = 200, max_tokens: int = 1000, trim_chunks: bool = True):\n",
    "    \n",
    "    #parse chunks from documents\n",
    "    chunks = parse_documents(DATA_PATH, db, embed_model, min_tokens, max_tokens)\n",
    "\n",
    "    # remove short chunks\n",
    "    if trim_chunks:\n",
    "        chunks= remove_small_chunks(chunks, min_tokens)\n",
    "\n",
    "    # create embeddings with openAI API\n",
    "    create_embeddings(chunks, batch_size=500, model = embed_model)\n",
    "\n",
    "    # add chunks to database\n",
    "    add_chunks_to_db(chunks, db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "import statistics\n",
    "\n",
    "def evaluate_rag(results, reranked_results, retrieval):\n",
    "    \"\"\"\n",
    "    Evaluates RAG retrieval performance using normalized relevance scores,\n",
    "    Spearman's rank correlation, and Mean Reciprocal Rank (MRR).\n",
    "    \n",
    "    :param results: List of retrieved chunks (original order)\n",
    "    :param reranked_results: Cohere reranking results (ideal order)\n",
    "    :return: Dictionary with evaluation metrics\n",
    "    \"\"\"\n",
    "    relevance_scores = []\n",
    "    normalized_scores = []\n",
    "    scores_dict = {\n",
    "        \"dense\": \"similarity_score\",\n",
    "        \"sparse\": \"rank\",\n",
    "        \"hybrid\": \"hybrid_score\"\n",
    "    }\n",
    "    \n",
    "\n",
    "    # Extract Cohere relevance scores and normalize them\n",
    "    for i, chunk in enumerate(results):\n",
    "        relevance_score = next((item.relevance_score for item in reranked_results.results if item.index == i), 0)\n",
    "\n",
    "         # Store scores for averaging later\n",
    "        relevance_scores.append(relevance_score)\n",
    "        \n",
    "        if retrieval == \"hybrid\":\n",
    "            print(f\"Chunk {chunk['chunk'].id} - Relevance Score: {relevance_score} - dense_score: {chunk['dense_score']} - sparse_score: {chunk['sparse_score']} - hybrid_score: {chunk['hybrid_score']}\")\n",
    "        else:\n",
    "            print(f\"Chunk {chunk['chunk'].id} - Relevance Score: {relevance_score} - {scores_dict[retrieval]}: {chunk[scores_dict[retrieval]]}\")\n",
    "\n",
    "\n",
    "    # Compute Spearman Rank Correlation\n",
    "    retrieved_ranks = list(range(len(results)))  # Original order (0, 1, 2, ...)\n",
    "    cohere_ranks = [item.index for item in reranked_results.results]  # Ideal order\n",
    "    spearman_corr, _ = spearmanr(retrieved_ranks, cohere_ranks)\n",
    "\n",
    "    # Compute Mean Reciprocal Rank (MRR)\n",
    "    def reciprocal_rank(retrieved, ideal):\n",
    "        for i, chunk in enumerate(retrieved):\n",
    "            if chunk['chunk'].id == ideal[0]['chunk'].id:  # Best Cohere chunk\n",
    "                return 1 / (i + 1)\n",
    "        return 0  # Not found\n",
    "\n",
    "    best_chunk = results[reranked_results.results[0].index]  # Best chunk from Cohere\n",
    "    mrr = reciprocal_rank(results, [best_chunk])\n",
    "    \n",
    "    # Return metrics\n",
    "    evaluation_metrics = {\n",
    "        \"Spearman Rank Correlation\": spearman_corr,\n",
    "        \"Mean Reciprocal Rank (MRR)\": mrr,\n",
    "        \"Average Relevance Score\": statistics.mean(relevance_scores),\n",
    "        \"Average Normalised Score\": statistics.mean(relevance_scores)\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüîç Evaluation Metrics:\")\n",
    "    for key, value in evaluation_metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "    return evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dense_search(query: str, query_method: str = \"cosine_distance\", n: int = 10, explain = False):\n",
    "#     db = sessionLocal()\n",
    "#     client = OpenAI()\n",
    "           \n",
    "#     # perform vector search on database\n",
    "#     query_embedding = client.embeddings.create(\n",
    "#         input = query,\n",
    "#         model=EMBEDDING_MODEL\n",
    "#         ).data[0].embedding\n",
    "\n",
    "#     query_vector = np.array(query_embedding).tolist()\n",
    "    \n",
    "#     # TOO EXPENSIVE NEEDS ALTERNATIVE SOLUTION\n",
    "#     # # rerank all chunks to get the chunk with highest relevance score \n",
    "#     # all_results = (\n",
    "#     #     db.query(Chunk)\n",
    "#     #     .all()\n",
    "#     # )\n",
    "\n",
    "#     # # Get list of strings to rerank with Cohere\n",
    "#     # docs = [chunk.chunk for chunk in all_results]\n",
    "\n",
    "#     # # rerank all results\n",
    "#     # reranked_results = co.rerank(model=\"rerank-v3.5\", query = query, documents = docs, top_n = 1)\n",
    "#     # max_score = reranked_results.results[0].relevance_score\n",
    "\n",
    "#     # Define comparator options\n",
    "#     comparators = {\n",
    "#         \"l2_distance\": Chunk.embedding.l2_distance(query_vector),\n",
    "#         \"l1_distance\": Chunk.embedding.l1_distance(query_vector),\n",
    "#         \"cosine_distance\": Chunk.embedding.cosine_distance(query_vector),\n",
    "#         \"dot_product\": Chunk.embedding.max_inner_product(query_vector)  # Dot product needs descending order\n",
    "#     }\n",
    "\n",
    "#     if explain:\n",
    "#         orm_query = db.query(Chunk, comparators[query_method].label(\"cosine_distance\")).order_by(comparators[query_method]).limit(n)\n",
    "#         print(f\"\\n--- Running EXPLAIN ANALYZE for query_method='{query_method}' (probes={100 if query_method=='cosine_distance' else 'N/A'}) ---\")\n",
    "        \n",
    "#         # Compile the ORM query to SQL\n",
    "#         # Use the session's dialect to ensure correct parameter handling\n",
    "\n",
    "#         #db.execute(text(\"SET enable_indexscan = off;\"))\n",
    "#         # db.execute(text(\"SET ivf.probes = 100;\"))\n",
    "#         # db.execute(text(\"ANALYZE chunks;\"))\n",
    "\n",
    "#         compiled = orm_query.statement.compile(dialect=db.bind.dialect, compile_kwargs ={\"literal_binds\": True})\n",
    "#         params = compiled.params\n",
    "#         # Prepend EXPLAIN ANALYZE\n",
    "#         explain_sql = \"EXPLAIN ANALYZE \" + str(compiled)\n",
    "\n",
    "#         print(f\"Compiled SQL for EXPLAIN: {explain_sql}\")\n",
    "\n",
    "\n",
    "#         # Execute EXPLAIN ANALYZE\n",
    "#         explain_result = db.execute(text(explain_sql), params)\n",
    "\n",
    "#         # Fetch and print the plan output\n",
    "#         plan_output = \"\\n\".join([row[0] for row in explain_result.fetchall()])\n",
    "#         print(\"\\n--- EXPLAIN ANALYZE Output: ---\")\n",
    "#         print(plan_output)\n",
    "#         print(\"-------------------------------\\n\")\n",
    "        \n",
    "#         # Return an empty list or None when explaining, as we didn't fetch results\n",
    "#         return None \n",
    "\n",
    "#     #db.execute(text(\"SET enable_indexscan = off;\"))\n",
    "#     query_results = (\n",
    "#         db.query(Chunk, comparators[query_method].label(\"cosine_distance\"))\n",
    "#         .order_by(comparators[query_method])\n",
    "#         .limit(n)\n",
    "#         .all()\n",
    "#     )\n",
    "    \n",
    "#     # Convert tuples to chunks with scores\n",
    "#     scored_chunks = [\n",
    "#         {\n",
    "#             \"chunk\": chunk,\n",
    "#             \"similarity_score\": 1 - cosine_distance # Attach similarity score\n",
    "#         }\n",
    "#         for chunk, cosine_distance in query_results\n",
    "#     ]\n",
    "        \n",
    "#     return scored_chunks\n",
    "\n",
    "# def sparse_search(query: str, n: int = 10):\n",
    "#     db = sessionLocal()\n",
    "#     try:\n",
    "#         # query_tsquery = func.plainto_tsquery('english', query_text)\n",
    "#         tsquery = func.websearch_to_tsquery('english', query)\n",
    "\n",
    "#         rank_func = func.ts_rank_cd(Chunk.chunk_tsv, tsquery).label('rank')\n",
    "        \n",
    "#         results = (db.query(Chunk, rank_func)\n",
    "#             .where(Chunk.chunk_tsv.op('@@')(tsquery))\n",
    "#             .order_by(desc(rank_func)) # Higher rank is better\n",
    "#             .limit(n).all()\n",
    "#         )\n",
    "#         # Convert tuples to chunks with scores\n",
    "#         scored_chunks = [\n",
    "#             {\n",
    "#                 \"chunk\": chunk,\n",
    "#                 \"rank\": rank_score  # Attach ranking score\n",
    "#             }\n",
    "#             for chunk, rank_score in results\n",
    "#         ]\n",
    "       \n",
    "#         return scored_chunks\n",
    "#     except Exception as e:\n",
    "#         print(f\"Something went wrong during sparse search: {e}\")\n",
    "\n",
    "    \n",
    "# def hybrid_search(query: str, dense_comparator: str = \"cosine_distance\", alpha: float = 0.5, n: int = 10):\n",
    "#     \"\"\"\n",
    "#     Performs a hybrid search combining dense and sparse retrieval methods.\n",
    "    \n",
    "#     :param query: Search query\n",
    "#     :param dense_comparator: Similarity metric for dense search\n",
    "#     :param alpha: Weight for combining scores (0.5 means equal weight to both)\n",
    "#     :param n: Number of top results to return\n",
    "#     :return: List of combined search results\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Perform both searches\n",
    "#     dense_results = dense_search(query, dense_comparator, n)\n",
    "#     sparse_results = sparse_search(query, n)\n",
    "\n",
    "#     # Create a dictionary to store combined scores\n",
    "#     result_dict = {}\n",
    "#     results = []\n",
    "\n",
    "#     # Normalize Dense Scores\n",
    "#     max_dense_score = max(item[\"similarity_score\"] for item in dense_results) if dense_results else 1\n",
    "#     for item in dense_results:\n",
    "#         norm_dense_score = item[\"similarity_score\"] / max_dense_score\n",
    "#         result_dict[item[\"chunk\"].id] = {\"chunk\": item[\"chunk\"], \"dense_score\": norm_dense_score, \"sparse_score\": 0}\n",
    "\n",
    "#     # Normalize Sparse Scores\n",
    "#     max_sparse_score = max(item[\"rank\"] for item in sparse_results) if sparse_results else 1\n",
    "#     for item in sparse_results:\n",
    "#         norm_sparse_score = item[\"rank\"] / max_sparse_score\n",
    "#         if item[\"chunk\"].id in result_dict:\n",
    "#             result_dict[item[\"chunk\"].id][\"sparse_score\"] = norm_sparse_score\n",
    "#         else:\n",
    "#             result_dict[item[\"chunk\"].id] = {\"chunk\": item[\"chunk\"], \"dense_score\": 0, \"sparse_score\": norm_sparse_score}\n",
    "\n",
    "#     # Compute Hybrid Score\n",
    "#     for chunk_id, values in result_dict.items():\n",
    "#         values[\"hybrid_score\"] = alpha * values[\"dense_score\"] + (1 - alpha) * values[\"sparse_score\"]\n",
    "#         results.append({\n",
    "#             \"chunk\": values[\"chunk\"],\n",
    "#             \"hybrid_score\": values[\"hybrid_score\"],\n",
    "#             \"dense_score\": values[\"dense_score\"],\n",
    "#             \"sparse_score\": values[\"sparse_score\"]      \n",
    "#         })\n",
    "\n",
    "\n",
    "#     # Sort results by hybrid score (descending)\n",
    "#     sorted_results = sorted(results, key=lambda x: x[\"hybrid_score\"], reverse=True)\n",
    "    \n",
    "#     # Return top-N results\n",
    "#     return sorted_results\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_search(\n",
    "    query: str,\n",
    "    db,\n",
    "    query_method: str = \"cosine_distance\",\n",
    "    n: int = 10\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Performs dense vector similarity search using OpenAI embeddings and pgvector.\n",
    "\n",
    "    Args:\n",
    "        query (str): The natural language query to embed and search with.\n",
    "        db (scoped_session): SQLAlchemy database session.\n",
    "        query_method (str): Distance metric to use for similarity (e.g., cosine_distance).\n",
    "        n (int): Number of top results to retrieve.\n",
    "        explain (bool): Whether to return additional debug information.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of chunks with their similarity scores.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get vector embedding for the query\n",
    "        query_embedding = openai.embeddings.create(\n",
    "            input=query,\n",
    "            model=EMBEDDING_MODEL\n",
    "        ).data[0].embedding\n",
    "        query_vector = np.array(query_embedding).tolist()\n",
    "\n",
    "        # Distance function map\n",
    "        comparators = {\n",
    "            \"l2_distance\": Chunk.embedding.l2_distance(query_vector),\n",
    "            \"l1_distance\": Chunk.embedding.l1_distance(query_vector),\n",
    "            \"cosine_distance\": Chunk.embedding.cosine_distance(query_vector),\n",
    "            \"dot_product\": Chunk.embedding.max_inner_product(query_vector)\n",
    "        }\n",
    "\n",
    "        if query_method not in comparators:\n",
    "            raise ValueError(f\"Invalid query method '{query_method}'\")\n",
    "\n",
    "        comparator = comparators[query_method]\n",
    "\n",
    "        # Run DB query with eager-loaded document\n",
    "        query_results = (\n",
    "            db.query(Chunk, comparator.label(\"score\"))\n",
    "            # .options(joinedload(Chunk.document))\n",
    "            .order_by(comparator)\n",
    "            .limit(n)\n",
    "            .all()\n",
    "        )\n",
    "\n",
    "        # Normalize cosine distance (lower = better)\n",
    "        scored_chunks = [\n",
    "            {\n",
    "                \"chunk\": chunk,\n",
    "                \"similarity_score\": 1 - score if query_method == \"cosine_distance\" else score\n",
    "            }\n",
    "            for chunk, score in query_results\n",
    "        ]\n",
    "\n",
    "        return scored_chunks\n",
    "    except Exception as e:\n",
    "        print(f\"[sparse_search] Search failed: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def sparse_search(query: str, db, n: int = 10) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Performs sparse full-text search using PostgreSQL's tsvector and ts_rank_cd.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query for text search.\n",
    "        db (scoped_session): SQLAlchemy session.\n",
    "        n (int): Number of results to return.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: Ranked chunks with relevance scores.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tsquery = func.websearch_to_tsquery('english', query)\n",
    "        rank_func = func.ts_rank_cd(Chunk.chunk_tsv, tsquery).label('rank')\n",
    "\n",
    "        results = (\n",
    "            db.query(Chunk, rank_func)\n",
    "            # .options(joinedload(Chunk.document))\n",
    "            .where(Chunk.chunk_tsv.op('@@')(tsquery))\n",
    "            .order_by(desc(rank_func))\n",
    "            .limit(n)\n",
    "            .all()\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            {\"chunk\": chunk, \"rank\": rank}\n",
    "            for chunk, rank in results\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"[sparse_search] Search failed: {e}\")\n",
    "        return []\n",
    "\n",
    "  \n",
    "    \n",
    "def hybrid_search(query: str, db, dense_comparator: str = \"cosine_distance\", alpha: float = 0.5, n: int = 10):\n",
    "    \"\"\"\n",
    "    Performs a hybrid search combining dense and sparse retrieval methods.\n",
    "    \n",
    "    :param query: Search query\n",
    "    :param dense_comparator: Similarity metric for dense search\n",
    "    :param alpha: Weight for combining scores (0.5 means equal weight to both)\n",
    "    :param n: Number of top results to return\n",
    "    :return: List of combined search results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Perform both searches\n",
    "    dense_results = dense_search(query, db, dense_comparator, n)\n",
    "    sparse_results = sparse_search(query, db, n)\n",
    "\n",
    "    # Create a dictionary to store combined scores\n",
    "    result_dict = {}\n",
    "    results = []\n",
    "\n",
    "    # Normalize Dense Scores\n",
    "    max_dense_score = max(item[\"similarity_score\"] for item in dense_results) if dense_results else 1\n",
    "    for item in dense_results:\n",
    "        norm_dense_score = item[\"similarity_score\"] / max_dense_score\n",
    "        result_dict[item[\"chunk\"].id] = {\"chunk\": item[\"chunk\"], \"dense_score\": norm_dense_score, \"sparse_score\": 0}\n",
    "\n",
    "    # Normalize Sparse Scores\n",
    "    max_sparse_score = max(item[\"rank\"] for item in sparse_results) if sparse_results else 1\n",
    "    for item in sparse_results:\n",
    "        norm_sparse_score = item[\"rank\"] / max_sparse_score\n",
    "        if item[\"chunk\"].id in result_dict:\n",
    "            result_dict[item[\"chunk\"].id][\"sparse_score\"] = norm_sparse_score\n",
    "        else:\n",
    "            result_dict[item[\"chunk\"].id] = {\"chunk\": item[\"chunk\"], \"dense_score\": 0, \"sparse_score\": norm_sparse_score}\n",
    "\n",
    "    # Compute Hybrid Score\n",
    "    for chunk_id, values in result_dict.items():\n",
    "        values[\"hybrid_score\"] = alpha * values[\"dense_score\"] + (1 - alpha) * values[\"sparse_score\"]\n",
    "        results.append({\n",
    "            \"chunk\": values[\"chunk\"],\n",
    "            \"hybrid_score\": values[\"hybrid_score\"],\n",
    "            \"dense_score\": values[\"dense_score\"],\n",
    "            \"sparse_score\": values[\"sparse_score\"]      \n",
    "        })\n",
    "\n",
    "    # Sort results by hybrid score (descending)\n",
    "    sorted_results = sorted(results, key=lambda x: x[\"hybrid_score\"], reverse=True)\n",
    "    \n",
    "    # Return top-N results\n",
    "    return sorted_results\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search(query: str, db, retrieval: str = \"dense\", dense_comparator: str = \"cosine_distance\", n: int = 10):\n",
    "    #initialise Cohere reranker\n",
    "    co = cohere.Client(os.environ.get(\"COHERE_API_KEY\"))\n",
    "    docs = []\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "\n",
    "    #perform search\n",
    "    if retrieval.lower() == \"dense\":\n",
    "        query_results = dense_search(query, db, dense_comparator)\n",
    "    elif retrieval.lower() == \"sparse\":\n",
    "        query_results = sparse_search(query, db)\n",
    "    elif retrieval.lower() == \"hybrid\":\n",
    "        query_results = hybrid_search(query, db, dense_comparator)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid retrieval method. Choose 'dense', 'sparse', or 'hybrid'.\")\n",
    "\n",
    "    # rerank query results\n",
    "    docs = [result['chunk'].chunk for result in query_results]\n",
    "    reranked_results = co.rerank(model=\"rerank-v3.5\", query = query, documents = docs, top_n = n * 2)\n",
    "\n",
    "    #evaluate results\n",
    "   \n",
    "\n",
    "    metrics = evaluate_rag(query_results, reranked_results, retrieval)\n",
    "    scored_chunks = []\n",
    "\n",
    "    # for i, chunk in enumerate(query_results):\n",
    "    #     score = next((item.relevance_score for item in reranked_results.results if item.index == i), None)\n",
    "    #     normalised_score = score/max_score\n",
    "    #     scored_chunks.append((chunk, score, normalised_score))\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dense = pd.DataFrame(columns=[\"retrieval\", \"embedding_model\", \"chunk_size\", \"query\", 'average_relevance_score', 'normalised_relevance_score', \"Spearman Rank Correlation\", \"Mean Reciprocal Rank (MRR)\"])\n",
    "df_sparse = pd.DataFrame(columns=[\"retrieval\", \"embedding_model\", \"chunk_size\", \"query\", 'average_relevance_score', 'normalised_relevance_score', \"Spearman Rank Correlation\", \"Mean Reciprocal Rank (MRR)\"])\n",
    "df_hybrid = pd.DataFrame(columns=[\"retrieval\", \"embedding_model\", \"chunk_size\", \"query\", 'average_relevance_score', 'normalised_relevance_score', \"Spearman Rank Correlation\", \"Mean Reciprocal Rank (MRR)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipeline(\n",
    "        df: pd.DataFrame,\n",
    "        create_db: bool = False,\n",
    "        data_path: str = DATA_PATH, \n",
    "        embed_model: str = EMBEDDING_MODEL, \n",
    "        min_tokens: int = 200, \n",
    "        max_tokens: int = 1000,\n",
    "        trim_chunks: bool = True, \n",
    "        retrieval = \"dense\",\n",
    "        dense_comparator = \"cosine_distance\"):\n",
    "    \n",
    "    db = sessionLocal()\n",
    "    test_queries = [\n",
    "    \"setting up development environment?\",\n",
    "    \"necessary software?\",\n",
    "    \"Paid Time Off (PTO)\",\n",
    "    \"How do I request time off\",\n",
    "    \"Sick leave\",\n",
    "    \"meal expanses limit\",\n",
    "    \"set up meeting\",\n",
    "    \"gitlabs coding standards\"]\n",
    "    # test_queries = [\n",
    "    # \"setting up development environment?\"\n",
    "    # ]\n",
    "    if create_db:\n",
    "        create_embedded_chunks(db, data_path, embed_model, min_tokens, max_tokens, trim_chunks)\n",
    "\n",
    "    \n",
    "    print(f\"Showing results from {retrieval} search:\")\n",
    "\n",
    "    for query in test_queries:\n",
    "        metrics = evaluate_search(query, db, retrieval, dense_comparator)\n",
    "        new_row = {\n",
    "            \"retrieval\": retrieval,\n",
    "            \"embedding_model\": embed_model,\n",
    "            \"chunk_size\": f\"{min_tokens} - {max_tokens}\",\n",
    "            \"query\": query,\n",
    "            \"average_relevance_score\": metrics[\"Average Relevance Score\"],\n",
    "            \"normalised_relevance_score\": metrics[\"Average Normalised Score\"],\n",
    "            \"Spearman Rank Correlation\": metrics[\"Spearman Rank Correlation\"],\n",
    "            \"Mean Reciprocal Rank (MRR)\": metrics[\"Mean Reciprocal Rank (MRR)\"]\n",
    "}       \n",
    "        df.loc[len(df)] = new_row\n",
    "\n",
    "    db.close()\n",
    "  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dense_search() missing 1 required positional argument: 'db'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdense_search\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msetting up development environment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: dense_search() missing 1 required positional argument: 'db'"
     ]
    }
   ],
   "source": [
    "db = sessionLocal()\n",
    "dense_search(\"setting up development environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing results from hybrid search:\n",
      "Query: setting up development environment?\n",
      "Chunk 5179 - Relevance Score: 0.756376 - dense_score: 0.9239201193548958 - sparse_score: 1.0 - hybrid_score: 0.9619600596774479\n",
      "Chunk 5180 - Relevance Score: 0.7300017 - dense_score: 0.9052993162905258 - sparse_score: 1.0 - hybrid_score: 0.9526496581452629\n",
      "Chunk 6242 - Relevance Score: 0.31154254 - dense_score: 1.0 - sparse_score: 0 - hybrid_score: 0.5\n",
      "Chunk 5178 - Relevance Score: 0.80405265 - dense_score: 0 - sparse_score: 0.9459437207207368 - hybrid_score: 0.4729718603603684\n",
      "Chunk 30031 - Relevance Score: 0.3554845 - dense_score: 0.9456348775261706 - sparse_score: 0 - hybrid_score: 0.4728174387630853\n",
      "Chunk 10886 - Relevance Score: 0.1682824 - dense_score: 0.9412153779665121 - sparse_score: 0 - hybrid_score: 0.47060768898325606\n",
      "Chunk 30030 - Relevance Score: 0.30062285 - dense_score: 0.9410965275813138 - sparse_score: 0 - hybrid_score: 0.4705482637906569\n",
      "Chunk 11241 - Relevance Score: 0.35307175 - dense_score: 0.9402731543084647 - sparse_score: 0 - hybrid_score: 0.47013657715423235\n",
      "Chunk 40820 - Relevance Score: 0.23641086 - dense_score: 0.9036376626352971 - sparse_score: 0 - hybrid_score: 0.45181883131764855\n",
      "Chunk 40822 - Relevance Score: 0.21960555 - dense_score: 0.901340842275521 - sparse_score: 0 - hybrid_score: 0.4506704211377605\n",
      "Chunk 40251 - Relevance Score: 0.12899646 - dense_score: 0.8859238657545612 - sparse_score: 0 - hybrid_score: 0.4429619328772806\n",
      "Chunk 5223 - Relevance Score: 0.6364215 - dense_score: 0 - sparse_score: 0.6943629799040058 - hybrid_score: 0.3471814899520029\n",
      "Chunk 5225 - Relevance Score: 0.63152665 - dense_score: 0 - sparse_score: 0.6943629799040058 - hybrid_score: 0.3471814899520029\n",
      "Chunk 5227 - Relevance Score: 0.6388584 - dense_score: 0 - sparse_score: 0.6943629799040058 - hybrid_score: 0.3471814899520029\n",
      "Chunk 5228 - Relevance Score: 0.79883134 - dense_score: 0 - sparse_score: 0.5669222234706532 - hybrid_score: 0.2834611117353266\n",
      "Chunk 21701 - Relevance Score: 0.5052006 - dense_score: 0 - sparse_score: 0.4226557121970222 - hybrid_score: 0.2113278560985111\n",
      "Chunk 21700 - Relevance Score: 0.467308 - dense_score: 0 - sparse_score: 0.4025292698379122 - hybrid_score: 0.2012646349189561\n",
      "Chunk 21702 - Relevance Score: 0.48147875 - dense_score: 0 - sparse_score: 0.4025292698379122 - hybrid_score: 0.2012646349189561\n",
      "\n",
      "üîç Evaluation Metrics:\n",
      "Spearman Rank Correlation: -0.0072\n",
      "Mean Reciprocal Rank (MRR): 0.2500\n",
      "Average Relevance Score: 0.4736\n",
      "Average Normalised Score: 0.4736\n",
      "Query: necessary software?\n",
      "Chunk 40460 - Relevance Score: 0.20098113 - dense_score: 1.0 - sparse_score: 0 - hybrid_score: 0.5\n",
      "Chunk 28289 - Relevance Score: 0.36786106 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 28284 - Relevance Score: 0.36786106 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 28287 - Relevance Score: 0.36786106 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 28292 - Relevance Score: 0.36786106 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 28279 - Relevance Score: 0.34237418 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 28281 - Relevance Score: 0.36786106 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 28298 - Relevance Score: 0.36786106 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 28295 - Relevance Score: 0.36786106 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 28300 - Relevance Score: 0.36786106 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 28303 - Relevance Score: 0.36786106 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 19345 - Relevance Score: 0.1195625 - dense_score: 0.973809753972309 - sparse_score: 0 - hybrid_score: 0.4869048769861545\n",
      "Chunk 24812 - Relevance Score: 0.10274522 - dense_score: 0.968764939062619 - sparse_score: 0 - hybrid_score: 0.4843824695313095\n",
      "Chunk 40475 - Relevance Score: 0.28028923 - dense_score: 0.9301564046886801 - sparse_score: 0 - hybrid_score: 0.46507820234434005\n",
      "Chunk 40474 - Relevance Score: 0.2618395 - dense_score: 0.928547437976629 - sparse_score: 0 - hybrid_score: 0.4642737189883145\n",
      "Chunk 40476 - Relevance Score: 0.3563576 - dense_score: 0.925410493124102 - sparse_score: 0 - hybrid_score: 0.462705246562051\n",
      "Chunk 40455 - Relevance Score: 0.25780055 - dense_score: 0.9235541075005614 - sparse_score: 0 - hybrid_score: 0.4617770537502807\n",
      "Chunk 40467 - Relevance Score: 0.2593873 - dense_score: 0.923289402006228 - sparse_score: 0 - hybrid_score: 0.461644701003114\n",
      "Chunk 40458 - Relevance Score: 0.27065203 - dense_score: 0.9222905947537892 - sparse_score: 0 - hybrid_score: 0.4611452973768946\n",
      "Chunk 40461 - Relevance Score: 0.2675019 - dense_score: 0.9221402750859674 - sparse_score: 0 - hybrid_score: 0.4610701375429837\n",
      "\n",
      "üîç Evaluation Metrics:\n",
      "Spearman Rank Correlation: 0.5850\n",
      "Mean Reciprocal Rank (MRR): 0.5000\n",
      "Average Relevance Score: 0.3015\n",
      "Average Normalised Score: 0.3015\n",
      "Query: Paid Time Off (PTO)\n",
      "Chunk 2598 - Relevance Score: 0.6369636 - dense_score: 1.0 - sparse_score: 0 - hybrid_score: 0.5\n",
      "Chunk 2632 - Relevance Score: 0.5194 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 2629 - Relevance Score: 0.51822984 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 2623 - Relevance Score: 0.52173984 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 2621 - Relevance Score: 0.53065056 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 2625 - Relevance Score: 0.53094244 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 2627 - Relevance Score: 0.5379402 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 2616 - Relevance Score: 0.5367749 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 2614 - Relevance Score: 0.53385985 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 2618 - Relevance Score: 0.5332765 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 11425 - Relevance Score: 0.477238 - dense_score: 0.9518728873911195 - sparse_score: 0 - hybrid_score: 0.47593644369555976\n",
      "Chunk 26825 - Relevance Score: 0.7001783 - dense_score: 0.950680551787625 - sparse_score: 0 - hybrid_score: 0.4753402758938125\n",
      "Chunk 26843 - Relevance Score: 0.8005209 - dense_score: 0.9196483388501216 - sparse_score: 0 - hybrid_score: 0.4598241694250608\n",
      "Chunk 26838 - Relevance Score: 0.78194773 - dense_score: 0.9195503289208539 - sparse_score: 0 - hybrid_score: 0.45977516446042693\n",
      "Chunk 26864 - Relevance Score: 0.7783298 - dense_score: 0.9188783744362187 - sparse_score: 0 - hybrid_score: 0.45943918721810934\n",
      "Chunk 26879 - Relevance Score: 0.7876867 - dense_score: 0.9150462361623241 - sparse_score: 0 - hybrid_score: 0.45752311808116203\n",
      "Chunk 26826 - Relevance Score: 0.81204927 - dense_score: 0.9145971496354437 - sparse_score: 0 - hybrid_score: 0.45729857481772185\n",
      "Chunk 26830 - Relevance Score: 0.7938904 - dense_score: 0.9132341105998302 - sparse_score: 0 - hybrid_score: 0.4566170552999151\n",
      "Chunk 26869 - Relevance Score: 0.8088086 - dense_score: 0.9126636356683115 - sparse_score: 0 - hybrid_score: 0.4563318178341558\n",
      "Chunk 31163 - Relevance Score: 0.7300017 - dense_score: 0 - sparse_score: 0.6196253253997769 - hybrid_score: 0.30981266269988844\n",
      "\n",
      "üîç Evaluation Metrics:\n",
      "Spearman Rank Correlation: -0.7684\n",
      "Mean Reciprocal Rank (MRR): 0.0588\n",
      "Average Relevance Score: 0.6435\n",
      "Average Normalised Score: 0.6435\n",
      "Query: How do I request time off\n",
      "Chunk 26873 - Relevance Score: 0.6172364 - dense_score: 1.0 - sparse_score: 0 - hybrid_score: 0.5\n",
      "Chunk 37190 - Relevance Score: 0.5634506 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 37220 - Relevance Score: 0.5634506 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 37159 - Relevance Score: 0.5634506 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 37175 - Relevance Score: 0.5634506 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 37205 - Relevance Score: 0.5634506 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 37144 - Relevance Score: 0.5634506 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 39167 - Relevance Score: 0.5634506 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 37748 - Relevance Score: 0.5634506 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 38036 - Relevance Score: 0.5634506 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 37235 - Relevance Score: 0.5634506 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 28365 - Relevance Score: 0.2235215 - dense_score: 0.9479194305975679 - sparse_score: 0 - hybrid_score: 0.47395971529878395\n",
      "Chunk 26838 - Relevance Score: 0.46935064 - dense_score: 0.9229424210597933 - sparse_score: 0 - hybrid_score: 0.46147121052989665\n",
      "Chunk 26847 - Relevance Score: 0.4413124 - dense_score: 0.9135016252224183 - sparse_score: 0 - hybrid_score: 0.45675081261120914\n",
      "Chunk 26879 - Relevance Score: 0.4338141 - dense_score: 0.9132419141140349 - sparse_score: 0 - hybrid_score: 0.45662095705701744\n",
      "Chunk 26855 - Relevance Score: 0.45246392 - dense_score: 0.912692987792981 - sparse_score: 0 - hybrid_score: 0.4563464938964905\n",
      "Chunk 26864 - Relevance Score: 0.43568602 - dense_score: 0.9121204833235425 - sparse_score: 0 - hybrid_score: 0.4560602416617712\n",
      "Chunk 26843 - Relevance Score: 0.43842506 - dense_score: 0.9105893097476395 - sparse_score: 0 - hybrid_score: 0.45529465487381976\n",
      "Chunk 26851 - Relevance Score: 0.44666424 - dense_score: 0.9101403033185492 - sparse_score: 0 - hybrid_score: 0.4550701516592746\n",
      "Chunk 26884 - Relevance Score: 0.44536126 - dense_score: 0.9101223627877362 - sparse_score: 0 - hybrid_score: 0.4550611813938681\n",
      "\n",
      "üîç Evaluation Metrics:\n",
      "Spearman Rank Correlation: 0.8902\n",
      "Mean Reciprocal Rank (MRR): 1.0000\n",
      "Average Relevance Score: 0.5019\n",
      "Average Normalised Score: 0.5019\n",
      "Query: Sick leave\n",
      "Chunk 26863 - Relevance Score: 0.57437146 - dense_score: 1.0 - sparse_score: 0 - hybrid_score: 0.5\n",
      "Chunk 41207 - Relevance Score: 0.54492307 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 41211 - Relevance Score: 0.53692055 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 41223 - Relevance Score: 0.53692055 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 41203 - Relevance Score: 0.54492307 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 41191 - Relevance Score: 0.54492307 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 41187 - Relevance Score: 0.54492307 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 41199 - Relevance Score: 0.54492307 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 41219 - Relevance Score: 0.53692055 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 41195 - Relevance Score: 0.54492307 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 41215 - Relevance Score: 0.53692055 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 41167 - Relevance Score: 0.65676886 - dense_score: 0.9689953563195145 - sparse_score: 0 - hybrid_score: 0.48449767815975725\n",
      "Chunk 26827 - Relevance Score: 0.53997856 - dense_score: 0.9529611141825427 - sparse_score: 0 - hybrid_score: 0.47648055709127135\n",
      "Chunk 26885 - Relevance Score: 0.53997856 - dense_score: 0.9529324739567432 - sparse_score: 0 - hybrid_score: 0.4764662369783716\n",
      "Chunk 26860 - Relevance Score: 0.53997856 - dense_score: 0.9529230850272806 - sparse_score: 0 - hybrid_score: 0.4764615425136403\n",
      "Chunk 26880 - Relevance Score: 0.53997856 - dense_score: 0.952913068368534 - sparse_score: 0 - hybrid_score: 0.476456534184267\n",
      "Chunk 26848 - Relevance Score: 0.53997856 - dense_score: 0.952913068368534 - sparse_score: 0 - hybrid_score: 0.476456534184267\n",
      "Chunk 26870 - Relevance Score: 0.53997856 - dense_score: 0.9528787268006124 - sparse_score: 0 - hybrid_score: 0.4764393634003062\n",
      "Chunk 26839 - Relevance Score: 0.53997856 - dense_score: 0.9528634869091074 - sparse_score: 0 - hybrid_score: 0.4764317434545537\n",
      "Chunk 26831 - Relevance Score: 0.53997856 - dense_score: 0.9528064251684454 - sparse_score: 0 - hybrid_score: 0.4764032125842227\n",
      "\n",
      "üîç Evaluation Metrics:\n",
      "Spearman Rank Correlation: 0.3744\n",
      "Mean Reciprocal Rank (MRR): 0.0833\n",
      "Average Relevance Score: 0.5484\n",
      "Average Normalised Score: 0.5484\n",
      "Query: meal expanses limit\n",
      "Chunk 22621 - Relevance Score: 0.09010118 - dense_score: 1.0 - sparse_score: 0 - hybrid_score: 0.5\n",
      "Chunk 6283 - Relevance Score: 0.08016299 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 6275 - Relevance Score: 0.11394409 - dense_score: 0 - sparse_score: 0.9499999810000004 - hybrid_score: 0.4749999905000002\n",
      "Chunk 6250 - Relevance Score: 0.11394409 - dense_score: 0 - sparse_score: 0.9499999810000004 - hybrid_score: 0.4749999905000002\n",
      "Chunk 6261 - Relevance Score: 0.11394409 - dense_score: 0 - sparse_score: 0.9499999810000004 - hybrid_score: 0.4749999905000002\n",
      "Chunk 6268 - Relevance Score: 0.11394409 - dense_score: 0 - sparse_score: 0.9499999810000004 - hybrid_score: 0.4749999905000002\n",
      "Chunk 6272 - Relevance Score: 0.1023677 - dense_score: 0 - sparse_score: 0.9499999810000004 - hybrid_score: 0.4749999905000002\n",
      "Chunk 6282 - Relevance Score: 0.11394409 - dense_score: 0 - sparse_score: 0.9499999810000004 - hybrid_score: 0.4749999905000002\n",
      "Chunk 6253 - Relevance Score: 0.11394409 - dense_score: 0 - sparse_score: 0.9499999810000004 - hybrid_score: 0.4749999905000002\n",
      "Chunk 6257 - Relevance Score: 0.11394409 - dense_score: 0 - sparse_score: 0.9499999810000004 - hybrid_score: 0.4749999905000002\n",
      "Chunk 6265 - Relevance Score: 0.11394409 - dense_score: 0 - sparse_score: 0.9499999810000004 - hybrid_score: 0.4749999905000002\n",
      "Chunk 22559 - Relevance Score: 0.024687458 - dense_score: 0.7825044109433952 - sparse_score: 0 - hybrid_score: 0.3912522054716976\n",
      "Chunk 26069 - Relevance Score: 0.010920313 - dense_score: 0.7538771833543325 - sparse_score: 0 - hybrid_score: 0.37693859167716626\n",
      "Chunk 22572 - Relevance Score: 0.02083838 - dense_score: 0.7298921947263466 - sparse_score: 0 - hybrid_score: 0.3649460973631733\n",
      "Chunk 22900 - Relevance Score: 0.19069421 - dense_score: 0.7242896651250461 - sparse_score: 0 - hybrid_score: 0.36214483256252306\n",
      "Chunk 22676 - Relevance Score: 0.19069421 - dense_score: 0.7242896651250461 - sparse_score: 0 - hybrid_score: 0.36214483256252306\n",
      "Chunk 22612 - Relevance Score: 0.19069421 - dense_score: 0.7242896651250461 - sparse_score: 0 - hybrid_score: 0.36214483256252306\n",
      "Chunk 23053 - Relevance Score: 0.19069421 - dense_score: 0.724230393244019 - sparse_score: 0 - hybrid_score: 0.3621151966220095\n",
      "Chunk 23014 - Relevance Score: 0.19069421 - dense_score: 0.724230393244019 - sparse_score: 0 - hybrid_score: 0.3621151966220095\n",
      "Chunk 23002 - Relevance Score: 0.19069421 - dense_score: 0.724230393244019 - sparse_score: 0 - hybrid_score: 0.3621151966220095\n",
      "\n",
      "üîç Evaluation Metrics:\n",
      "Spearman Rank Correlation: -0.4286\n",
      "Mean Reciprocal Rank (MRR): 0.0667\n",
      "Average Relevance Score: 0.1192\n",
      "Average Normalised Score: 0.1192\n",
      "Query: set up meeting\n",
      "Chunk 26087 - Relevance Score: 0.12788142 - dense_score: 1.0 - sparse_score: 0 - hybrid_score: 0.5\n",
      "Chunk 22395 - Relevance Score: 0.24813357 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 22382 - Relevance Score: 0.24621193 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 22509 - Relevance Score: 0.21621042 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 22344 - Relevance Score: 0.22187328 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 22357 - Relevance Score: 0.26770288 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 22370 - Relevance Score: 0.21985671 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 22445 - Relevance Score: 0.2265358 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 22496 - Relevance Score: 0.23066878 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 22521 - Relevance Score: 0.21852797 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 22408 - Relevance Score: 0.2281311 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 26069 - Relevance Score: 0.097571015 - dense_score: 0.978320870006106 - sparse_score: 0 - hybrid_score: 0.489160435003053\n",
      "Chunk 26080 - Relevance Score: 0.27472016 - dense_score: 0.9550618691543394 - sparse_score: 0 - hybrid_score: 0.4775309345771697\n",
      "Chunk 26048 - Relevance Score: 0.27472016 - dense_score: 0.9550618691543394 - sparse_score: 0 - hybrid_score: 0.4775309345771697\n",
      "Chunk 26119 - Relevance Score: 0.27472016 - dense_score: 0.9550575458396214 - sparse_score: 0 - hybrid_score: 0.4775287729198107\n",
      "Chunk 26094 - Relevance Score: 0.27472016 - dense_score: 0.9550575458396214 - sparse_score: 0 - hybrid_score: 0.4775287729198107\n",
      "Chunk 26062 - Relevance Score: 0.27472016 - dense_score: 0.9550575458396214 - sparse_score: 0 - hybrid_score: 0.4775287729198107\n",
      "Chunk 26071 - Relevance Score: 0.27472016 - dense_score: 0.9550009388780607 - sparse_score: 0 - hybrid_score: 0.47750046943903035\n",
      "Chunk 26039 - Relevance Score: 0.27472016 - dense_score: 0.9550009388780607 - sparse_score: 0 - hybrid_score: 0.47750046943903035\n",
      "Chunk 26109 - Relevance Score: 0.27472016 - dense_score: 0.9549737038028514 - sparse_score: 0 - hybrid_score: 0.4774868519014257\n",
      "\n",
      "üîç Evaluation Metrics:\n",
      "Spearman Rank Correlation: -0.6180\n",
      "Mean Reciprocal Rank (MRR): 0.0769\n",
      "Average Relevance Score: 0.2374\n",
      "Average Normalised Score: 0.2374\n",
      "Query: gitlabs coding standards\n",
      "Chunk 21659 - Relevance Score: 0.92715925 - dense_score: 1.0 - sparse_score: 0 - hybrid_score: 0.5\n",
      "Chunk 1418 - Relevance Score: 0.43496582 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 1414 - Relevance Score: 0.43496582 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 1400 - Relevance Score: 0.43496582 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 1395 - Relevance Score: 0.43496582 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 1404 - Relevance Score: 0.43496582 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 1409 - Relevance Score: 0.43496582 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 1463 - Relevance Score: 0.43496582 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 1459 - Relevance Score: 0.43496582 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 1467 - Relevance Score: 0.43496582 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 1422 - Relevance Score: 0.43496582 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 36675 - Relevance Score: 0.90849644 - dense_score: 0.9517513000406849 - sparse_score: 0 - hybrid_score: 0.47587565002034243\n",
      "Chunk 28589 - Relevance Score: 0.49450764 - dense_score: 0.8537147416168165 - sparse_score: 0 - hybrid_score: 0.42685737080840824\n",
      "Chunk 5905 - Relevance Score: 0.657561 - dense_score: 0.8459138547668221 - sparse_score: 0 - hybrid_score: 0.42295692738341106\n",
      "Chunk 9547 - Relevance Score: 0.29477343 - dense_score: 0.844790103974642 - sparse_score: 0 - hybrid_score: 0.422395051987321\n",
      "Chunk 36678 - Relevance Score: 0.76194656 - dense_score: 0.8437943225398684 - sparse_score: 0 - hybrid_score: 0.4218971612699342\n",
      "Chunk 9549 - Relevance Score: 0.3093475 - dense_score: 0.8429310470440539 - sparse_score: 0 - hybrid_score: 0.42146552352202693\n",
      "Chunk 9556 - Relevance Score: 0.2896986 - dense_score: 0.8417129627244837 - sparse_score: 0 - hybrid_score: 0.42085648136224185\n",
      "Chunk 9554 - Relevance Score: 0.2666775 - dense_score: 0.8396985607863459 - sparse_score: 0 - hybrid_score: 0.41984928039317293\n",
      "Chunk 9551 - Relevance Score: 0.26019013 - dense_score: 0.8379370060866849 - sparse_score: 0 - hybrid_score: 0.41896850304334243\n",
      "\n",
      "üîç Evaluation Metrics:\n",
      "Spearman Rank Correlation: 0.5504\n",
      "Mean Reciprocal Rank (MRR): 1.0000\n",
      "Average Relevance Score: 0.4760\n",
      "Average Normalised Score: 0.4760\n"
     ]
    }
   ],
   "source": [
    "result_df = test_pipeline(df_hybrid, retrieval='hybrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrieval</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>query</th>\n",
       "      <th>average_relevance_score</th>\n",
       "      <th>normalised_relevance_score</th>\n",
       "      <th>Spearman Rank Correlation</th>\n",
       "      <th>Mean Reciprocal Rank (MRR)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>setting up development environment?</td>\n",
       "      <td>0.473560</td>\n",
       "      <td>0.473560</td>\n",
       "      <td>-0.007224</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>necessary software?</td>\n",
       "      <td>0.301512</td>\n",
       "      <td>0.301512</td>\n",
       "      <td>0.584962</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>Paid Time Off (PTO)</td>\n",
       "      <td>0.643521</td>\n",
       "      <td>0.643521</td>\n",
       "      <td>-0.768421</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>How do I request time off</td>\n",
       "      <td>0.501917</td>\n",
       "      <td>0.501917</td>\n",
       "      <td>0.890226</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>Sick leave</td>\n",
       "      <td>0.548409</td>\n",
       "      <td>0.548409</td>\n",
       "      <td>0.374436</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>meal expanses limit</td>\n",
       "      <td>0.119240</td>\n",
       "      <td>0.119240</td>\n",
       "      <td>-0.428571</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>set up meeting</td>\n",
       "      <td>0.237353</td>\n",
       "      <td>0.237353</td>\n",
       "      <td>-0.618045</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>gitlabs coding standards</td>\n",
       "      <td>0.476001</td>\n",
       "      <td>0.476001</td>\n",
       "      <td>0.550376</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  retrieval         embedding_model  chunk_size  \\\n",
       "0    hybrid  text-embedding-3-small  200 - 1000   \n",
       "1    hybrid  text-embedding-3-small  200 - 1000   \n",
       "2    hybrid  text-embedding-3-small  200 - 1000   \n",
       "3    hybrid  text-embedding-3-small  200 - 1000   \n",
       "4    hybrid  text-embedding-3-small  200 - 1000   \n",
       "5    hybrid  text-embedding-3-small  200 - 1000   \n",
       "6    hybrid  text-embedding-3-small  200 - 1000   \n",
       "7    hybrid  text-embedding-3-small  200 - 1000   \n",
       "\n",
       "                                 query  average_relevance_score  \\\n",
       "0  setting up development environment?                 0.473560   \n",
       "1                  necessary software?                 0.301512   \n",
       "2                  Paid Time Off (PTO)                 0.643521   \n",
       "3            How do I request time off                 0.501917   \n",
       "4                           Sick leave                 0.548409   \n",
       "5                  meal expanses limit                 0.119240   \n",
       "6                       set up meeting                 0.237353   \n",
       "7             gitlabs coding standards                 0.476001   \n",
       "\n",
       "   normalised_relevance_score  Spearman Rank Correlation  \\\n",
       "0                    0.473560                  -0.007224   \n",
       "1                    0.301512                   0.584962   \n",
       "2                    0.643521                  -0.768421   \n",
       "3                    0.501917                   0.890226   \n",
       "4                    0.548409                   0.374436   \n",
       "5                    0.119240                  -0.428571   \n",
       "6                    0.237353                  -0.618045   \n",
       "7                    0.476001                   0.550376   \n",
       "\n",
       "   Mean Reciprocal Rank (MRR)  \n",
       "0                    0.250000  \n",
       "1                    0.500000  \n",
       "2                    0.058824  \n",
       "3                    1.000000  \n",
       "4                    0.083333  \n",
       "5                    0.066667  \n",
       "6                    0.076923  \n",
       "7                    1.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing results from hybrid search:\n",
      "Query: setting up development environment?\n",
      "Value of enable_seqscan JUST BEFORE executing EXPLAIN: on\n",
      "\n",
      "--- Running EXPLAIN ANALYZE for query_method='cosine_distance' (probes=100) ---\n",
      "Compiled SQL for EXPLAIN: EXPLAIN ANALYZE SELECT chunks.id, chunks.document_id, chunks.context, chunks.chunk, chunks.embedding, chunks.chunk_tsv, chunks.embedding <=> '[-0.031588368117809296,-0.04046443849802017,0.09309431910514832,-0.008797752670943737,0.01267450675368309,-0.038271527737379074,0.09251998364925385,0.03453835844993591,-0.019657885655760765,-0.009678833186626434,0.032893672585487366,0.03187553584575653,-0.010266220197081566,-0.030283063650131226,0.05503163859248161,0.017765194177627563,0.004999316297471523,0.034198977053165436,-0.040699392557144165,0.0761253610253334,0.04383212700486183,-0.010305378586053848,-0.02327357977628708,0.0058118682354688644,-0.0006110456888563931,-0.005609546322375536,-0.03727949783205986,0.05518827587366104,0.016655685380101204,0.007890565320849419,-0.050384752452373505,-0.02425255812704563,0.01557228248566389,0.0540396049618721,0.03294588625431061,-0.007218333892524242,0.024004550650715828,-0.013757909648120403,-0.012791983783245087,0.03148394450545311,-0.019475143402814865,-0.04017727077007294,-0.00237402250058949,-0.006409045308828354,-0.016433782875537872,-0.02929103374481201,-0.021850798279047012,0.02591029368340969,0.003313841763883829,-0.03881975635886192,0.0034623201936483383,-0.026406310498714447,0.026602106168866158,0.010279272682964802,-0.011865218169987202,-0.02572755143046379,-0.024082867428660393,0.01304651889950037,-0.004862259142100811,0.024174239486455917,0.06761477142572403,-0.008706380613148212,-0.03526932746171951,0.040699392557144165,0.014123395085334778,-0.00828215666115284,-0.009502616710960865,-0.026393257081508636,-0.004832889884710312,0.04320557788014412,-0.04453698918223381,-0.013562113977968693,0.025649232789874077,-0.01219154428690672,0.01657736673951149,-0.014332243241369724,-0.005599756259471178,0.04941882938146591,0.011088562197983265,-0.012635347433388233,-0.0006073744734749198,-0.027124227955937386,-0.0025502387434244156,0.005756393074989319,0.014528038911521435,-0.023808754980564117,-0.05372633412480354,0.02798572927713394,-0.029395457357168198,0.00949609000235796,-0.0026171356439590454,-0.0034133712761104107,0.02029748447239399,0.0555015467107296,0.05983515828847885,0.02367822453379631,-0.002576344646513462,0.017556345090270042,0.007329284679144621,0.009411245584487915,0.00452288007363677,-0.0313534140586853,-0.010448962450027466,-0.032475974410772324,0.008915229700505733,-0.019449036568403244,0.030648550018668175,0.004761097952723503,-0.017974043264985085,0.026314938440918922,-0.088812917470932,-0.029734836891293526,0.01546785794198513,0.013901492580771446,-0.004186764359474182,-0.015337327495217323,0.0368356928229332,-0.062497980892658234,-0.007962357252836227,-0.04059496894478798,0.018561430275440216,0.0380626805126667,0.012733245268464088,-0.019971158355474472,0.032475974410772324,-0.024265611544251442,-0.025492597371339798,-0.012915988452732563,0.0038539115339517593,-0.03200606629252434,0.05576260760426521,0.016120510175824165,0.05255156010389328,-0.01939682476222515,-0.02162889577448368,-0.016251040622591972,-0.028220683336257935,0.024826891720294952,-0.05276040732860565,0.019853681325912476,0.005482278764247894,0.03448614478111267,0.009058813564479351,0.01995810493826866,0.03357243165373802,7.123087561922148e-05,-0.03834984451532364,-0.01621188223361969,-0.0424485020339489,0.03709675371646881,0.002126014791429043,-0.01806541346013546,-0.03628746420145035,-0.01035106461495161,0.026706529781222343,-0.01674705743789673,0.01674705743789673,-0.02929103374481201,-0.02536206692457199,-0.013327158987522125,-0.0020020108204334974,-0.007264019455760717,0.025845028460025787,-0.0018421109998598695,0.017765194177627563,0.02589724212884903,0.011976168490946293,0.0180523619055748,-0.026536840945482254,-0.012256809510290623,-0.018861649557948112,-0.014110341668128967,0.018026255071163177,-0.007962357252836227,0.06677937507629395,-0.036600738763809204,0.043884336948394775,-0.03814099729061127,-0.019605673849582672,-0.022790616378188133,-0.006324200425297022,-0.005697654094547033,-0.023769594728946686,-0.0065036797896027565,-0.027254758402705193,0.026758741587400436,0.03448614478111267,-0.005475752521306276,0.00958746112883091,-0.016264094039797783,-0.025479543954133987,-0.03302420303225517,-0.036052510142326355,0.033050309866666794,0.04169142618775368,-0.022647032514214516,-0.003583060810342431,0.005847764201462269,-0.014541092328727245,-0.009124078787863255,-0.005038475152105093,0.010031265206634998,-0.03521711379289627,-0.034721098840236664,-0.06317673623561859,0.0024131815880537033,-0.012589662335813046,0.007388023659586906,0.04038612172007561,-0.015924714505672455,-0.001982431160286069,-0.021380888298153877,-0.04565955325961113,0.08724655210971832,-0.012328601442277431,-0.007864459417760372,0.002471920335665345,-0.019892839714884758,-0.016485996544361115,-0.037749405950307846,-0.04007284715771675,-0.011845638044178486,0.006709265056997538,0.04383212700486183,0.028090152889490128,0.020441068336367607,0.020910978317260742,0.005257113836705685,0.07304483652114868,-0.007250966504216194,-0.00828215666115284,0.008438793942332268,-0.06740592420101166,0.04263124614953995,-0.016107456758618355,0.05497942492365837,-0.06510858982801437,-0.026941485702991486,-0.022881988435983658,-0.04362327605485916,-0.02294725365936756,-0.0014309401158243418,-0.035687025636434555,-0.005475752521306276,-0.00037466318462975323,0.01358821988105774,-0.010559913702309132,-0.02832510694861412,-0.024474458768963814,-0.020924031734466553,-0.020388856530189514,-0.03375517576932907,0.03503437340259552,-0.032893672585487366,-0.009078392758965492,-0.0242133978754282,-0.02923882007598877,-0.026562945917248726,0.022973358631134033,-0.025022687390446663,-0.03986399993300438,-0.002197806490585208,-0.010931924916803837,0.00713348900899291,-0.004036654252558947,0.01920102909207344,-0.015063214115798473,0.005798815283924341,-0.019879788160324097,0.019305452704429626,0.03928966447710991,-0.040882136672735214,0.03516490384936333,-0.02905607782304287,0.0240306556224823,0.007675190456211567,0.0008508953615091741,0.003703801427036524,-0.021707214415073395,0.030857399106025696,-0.06798025965690613,-0.02607998438179493,-0.01863974891602993,0.03545207157731056,-0.020166954025626183,0.0387936495244503,0.014201712794601917,0.04456309601664543,0.017843512818217278,0.018143732100725174,0.048922814428806305,0.022633980959653854,0.0018127416260540485,0.006258935201913118,0.008960915729403496,-0.015167638659477234,-0.0017523713177070022,0.015428699553012848,-0.03712286055088043,0.011786899529397488,0.053935181349515915,0.05247323960065842,0.016472943127155304,-0.017373602837324142,-0.023913178592920303,-0.02161584235727787,0.04542459547519684,-0.01163678988814354,0.03704454004764557,0.0320843830704689,-0.030309170484542847,-0.0368356928229332,0.027672454714775085,0.020271379500627518,0.032110489904880524,-0.03785382956266403,-0.04680822044610977,0.02572755143046379,0.019240187481045723,-0.03054412454366684,-0.044406458735466,-0.05064581334590912,-0.012047960422933102,0.00034794522798620164,-0.009019654244184494,-0.004989526234567165,0.029108289629220963,-0.022594820708036423,0.021720267832279205,-0.018887756392359734,-0.00931334774941206,-0.004428245592862368,-0.013105257414281368,0.02645852230489254,0.026589052751660347,-0.03874143585562706,0.021158985793590546,0.009548302739858627,-0.022477343678474426,-0.042970623821020126,-0.05858206748962402,0.013549060560762882,-0.0038049626164138317,-0.0032371552661061287,-0.00809941440820694,-0.009985579177737236,0.012733245268464088,0.004532669670879841,0.05466615408658981,0.006562418304383755,-0.011538892053067684,0.02460498921573162,-0.03620914742350578,-0.03579144924879074,0.017034223303198814,0.001827426254749298,0.03816710412502289,-0.037906043231487274,-0.03315473347902298,0.01332063227891922,0.03182332217693329,0.024122027680277824,0.020023370161652565,0.006304620765149593,0.029134396463632584,0.04422371834516525,0.009071866050362587,-0.0164076779037714,-0.05899976193904877,-0.005694390740245581,0.05722454935312271,-0.006275251507759094,-0.04114319756627083,0.025101006031036377,-0.06448204070329666,0.02084571309387684,0.014240872114896774,0.01360127329826355,0.051846694201231,0.030883504077792168,0.03506048023700714,0.005576913710683584,0.025492597371339798,0.02161584235727787,-0.007557712960988283,0.0054888054728507996,0.02033664472401142,-0.02942156419157982,-0.007538133300840855,-0.04764361307024956,0.016538208350539207,-0.02087181806564331,-0.002126014791429043,0.00656894501298666,0.020219165831804276,0.01937071792781353,0.009169763885438442,0.007936251349747181,0.013509901240468025,-0.011055929586291313,0.028064046055078506,-0.03151005133986473,-0.03453835844993591,-0.012433025054633617,0.024122027680277824,0.0259233471006155,-0.003971389029175043,-0.016812322661280632,-0.005808604881167412,-0.0852624922990799,0.01639462448656559,-0.06604840606451035,0.055240485817193985,0.011414888314902782,-0.033781278878450394,-0.01716475374996662,-0.0010809552622959018,-0.0009797941893339157,0.01602913998067379,0.03443393111228943,-0.024722468107938766,0.0025257642846554518,-0.053569696843624115,-0.03221491351723671,0.014971842989325523,-0.008960915729403496,-0.010370643809437752,0.0203105378895998,-0.06040949374437332,-0.04007284715771675,-0.009593987837433815,-0.05946967378258705,0.014214766211807728,0.03795825317502022,0.005749866366386414,0.002357706194743514,-0.06056612730026245,-0.07393244653940201,0.022973358631134033,0.025192376226186752,0.017099488526582718,0.018208997324109077,0.016538208350539207,0.010279272682964802,-0.05795551836490631,-0.0009822415886446834,-0.07628199458122253,-0.04075160622596741,-0.019083552062511444,0.014880470931529999,-0.03315473347902298,-0.07748287171125412,0.02235986664891243,0.024552777409553528,0.06181922182440758,0.030883504077792168,0.018195943906903267,0.06839795410633087,4.137713403906673e-05,0.03892417997121811,0.018391739577054977,0.011982695199549198,-0.015063214115798473,0.037749405950307846,0.013203155249357224,-0.005035212263464928,0.009065339341759682,0.014371402561664581,0.003990968689322472,-0.02759413793683052,0.031771112233400345,-0.0034590568393468857,-0.03707064688205719,-0.04639052227139473,-0.05398739501833916,-0.026562945917248726,-0.01617272198200226,-0.02647157572209835,-0.02777688018977642,-0.028612274676561356,-0.013000832870602608,0.016668738797307014,-0.018117627128958702,0.031745005398988724,0.044249821454286575,0.10291020572185516,-0.03545207157731056,0.0019106394611299038,-0.03338968753814697,0.04338832199573517,-0.025845028460025787,0.045163534581661224,0.050619710236787796,-0.014815205708146095,-0.0011804847745224833,-0.03057023137807846,0.005629125516861677,0.01508932001888752,-0.010657811537384987,0.04949714615941048,0.012224176898598671,-0.004408665932714939,-0.030413594096899033,0.04289230704307556,0.026184407994151115,-0.002104803454130888,-0.014136447571218014,-0.0026138722896575928,0.021929115056991577,0.004950367379933596,0.004783940967172384,-0.03785382956266403,0.018339527770876884,-0.04041222855448723,0.01407118234783411,0.019462089985609055,-0.034564461559057236,0.012511343695223331,0.001721370266750455,0.005713970400393009,-0.013640431687235832,0.033441901206970215,-0.0013958599884063005,0.02345632202923298,0.013379370793700218,-0.021172039210796356,-0.018900809809565544,-0.06714486330747604,-0.03443393111228943,-0.03730560094118118,0.004281398840248585,-0.021733319386839867,-0.03132730722427368,-0.05372633412480354,0.0009757151128724217,-0.02198132872581482,0.0180523619055748,-0.029134396463632584,-0.005126583389937878,-0.03234544396400452,0.0013420161558315158,0.037749405950307846,-0.009574408642947674,-0.010918872430920601,-0.03338968753814697,0.0011470363242551684,-0.021889956668019295,0.016316305845975876,-0.0010458752512931824,-0.029029972851276398,0.02406981587409973,-0.03367685526609421,-0.006023980211466551,0.01312483660876751,0.021132880821824074,0.005446383263915777,-0.005531227681785822,-0.002824352588504553,-0.04328389838337898,-0.022451236844062805,0.025858081877231598,-0.012034907937049866,0.03670516237616539,0.006559154950082302,0.007459815125912428,0.05497942492365837,0.005567123647779226,-0.042605139315128326,-0.039785679429769516,-0.027072016149759293,-0.013744856230914593,-0.004284662194550037,-0.0346427820622921,-0.01417560689151287,0.0007460631313733757,-0.009502616710960865,-0.011375728994607925,-0.060670554637908936,0.004382560029625893,-0.023051677271723747,0.012485237792134285,-0.01880943775177002,-0.012968200258910656,-0.005387644283473492,-0.012217650189995766,-0.019644832238554955,0.016303252428770065,0.015011001378297806,0.014945736154913902,-0.05409181863069534,0.03166668862104416,0.007211807183921337,-0.007002958562225103,-0.003860438009724021,0.019331559538841248,-0.00689853448420763,0.0331808403134346,0.015063214115798473,-0.00012094461999367923,-0.016694843769073486,0.03453835844993591,0.014528038911521435,-0.003609166946262121,-0.04111709073185921,0.015415646135807037,-0.010161795653402805,-0.003449267242103815,-0.013314105570316315,-0.032710932195186615,-0.032162703573703766,-0.022412078455090523,0.033259157091379166,-0.029369350522756577,-0.023143049329519272,0.033468008041381836,0.017791301012039185,0.03372906893491745,0.017843512818217278,-0.016525154933333397,-0.02144615352153778,-0.014971842989325523,-0.021498365327715874,-0.03265871852636337,0.02051938697695732,0.035321541130542755,0.0015223113587126136,0.016133563593029976,-0.0022924409713596106,-0.002434392925351858,0.024644149467349052,-0.02587113529443741,0.02237292006611824,0.036600738763809204,-0.016720950603485107,-0.04978431388735771,-0.007616451941430569,0.04675600677728653,0.018391739577054977,0.015976926311850548,-0.011799952946603298,0.02589724212884903,0.0004935682518407702,-0.010383697226643562,-0.023143049329519272,0.029552094638347626,0.025440385565161705,0.006591787561774254,0.031745005398988724,0.04299673065543175,0.0034035814460366964,-0.03237155079841614,0.010599073022603989,0.00950914341956377,-0.009058813564479351,0.03814099729061127,0.026040824130177498,-0.03892417997121811,-0.030805185437202454,0.00829521007835865,0.012485237792134285,-0.015598388388752937,0.013705696910619736,-0.006252408493310213,0.005381118040531874,-0.025022687390446663,0.0029026709962636232,-0.040542759001255035,0.014619410037994385,-0.04150868207216263,-0.02161584235727787,-0.044093187898397446,0.03975957632064819,-0.001545154256746173,0.012504816986620426,0.02216407097876072,0.0011731424601748586,0.020819606259465218,-0.02853395603597164,-0.00978325679898262,0.030413594096899033,0.036966223269701004,0.013307579793035984,-0.03255429491400719,0.013157469220459461,0.020950136706233025,0.0001971213787328452,0.040699392557144165,0.00961356796324253,-0.01332063227891922,-0.017451921477913857,0.03090961091220379,-0.05252545326948166,0.030831292271614075,0.004597934894263744,0.01750413328409195,0.008641115389764309,0.014919630251824856,-0.016525154933333397,0.02683706022799015,-0.01843089982867241,-0.00988768134266138,-0.0034916894510388374,0.05419624224305153,-0.013731803745031357,-0.012733245268464088,-0.003393791615962982,-0.027437500655651093,-0.02326052635908127,0.029134396463632584,0.005798815283924341,-0.0167731624096632,-0.0342772975564003,-0.039942316710948944,-0.03448614478111267,-0.03461667522788048,-0.021720267832279205,0.03853258863091469,-9.432864317204803e-06,0.0317188985645771,-0.020950136706233025,-0.02386096678674221,0.018678907305002213,-0.002912460593506694,0.027306970208883286,0.011852164752781391,0.017177807167172432,0.025818923488259315,-0.06683158874511719,-0.02126341126859188,0.0031474155839532614,0.022320706397294998,0.011088562197983265,0.03349411487579346,0.044824156910181046,0.017190860584378242,0.00811246782541275,-0.04174363613128662,-0.011584577150642872,-0.014841312542557716,-0.010103057138621807,0.012746298685669899,0.009182817302644253,-0.021341728046536446,0.044824156910181046,-0.00011482600530143827,-0.030987929552793503,0.035504281520843506,0.04607724770903587,0.03965514898300171,0.0038473850581794977,-0.009633147157728672,0.010553386993706226,-0.004056233912706375,0.0036613792181015015,-0.0036287466064095497,-0.03803657367825508,-0.032867565751075745,-0.006768004037439823,-0.02178553305566311,0.039890106767416,0.013562113977968693,0.028951654210686684,-0.03002200275659561,-0.008595430292189121,0.001998747466132045,0.026132196187973022,0.0008198943687602878,-0.007120436057448387,-0.0038049626164138317,0.016472943127155304,0.0032192072831094265,-0.01749107986688614,0.020754341036081314,-0.00031572053558193147,0.02848174422979355,0.0060272435657680035,-0.06202806904911995,0.006180617026984692,-0.04022948443889618,0.03166668862104416,-0.02722865156829357,-0.042944516986608505,-0.005074371118098497,0.0121066989377141,-0.004128025379031897,0.01696895807981491,0.03054412454366684,0.029943685978651047,-0.03036138229072094,-0.028403425589203835,-0.012243756093084812,0.0035308487713336945,-0.0346427820622921,0.03388570621609688,0.027646349743008614,0.00423571327701211,-0.019814522936940193,-0.06035728007555008,0.013268420472741127,-0.03675737604498863,-0.0043140314519405365,0.015154585242271423,0.02754192426800728,0.012537449598312378,0.007955830544233322,0.02216407097876072,0.015063214115798473,-0.0024572357069700956,-0.0048818388022482395,-0.024696361273527145,0.01052728109061718,0.0020819606725126505,-0.05701570212841034,-0.0043205576948821545,-0.009822416119277477,-0.013757909648120403,0.027646349743008614,-0.018718065693974495,-0.013614325784146786,0.020454121753573418,0.06056612730026245,-0.023900125175714493,0.0037331709172576666,0.0060729291290044785,-0.00512984674423933,-0.01920102909207344,0.009548302739858627,-0.019683992490172386,-0.005240797530859709,-0.0391591340303421,0.022660085931420326,-0.011395308189094067,0.032162703573703766,-0.03221491351723671,0.0039289663545787334,0.006262198556214571,-0.015206797048449516,-0.022594820708036423,0.0032436817418783903,0.009652726352214813,0.01750413328409195,-0.017073383554816246,0.016264094039797783,-0.00940471887588501,0.023730436339974403,-0.05255156010389328,0.03657463192939758,-0.0013322264421731234,0.010292326100170612,-0.009104498662054539,0.0017964253202080727,0.039968423545360565,0.06375107169151306,-0.017960989847779274,-0.007433709222823381,0.005694390740245581,0.025048794224858284,0.004131288733333349,-0.01193701010197401,-0.011773847043514252,-0.001962851732969284,-0.030335277318954468,0.017203914001584053,-0.005952188745141029,-0.005762919317930937,-0.061767008155584335,-0.008027622476220131,-0.0040138112381100655,0.017974043264985085,0.01099066436290741,-0.022633980959653854,-0.014841312542557716,-0.015389540232717991,-0.030648550018668175,0.019109657034277916,0.001650394406169653,-0.0491577684879303,-0.01807846687734127,0.002909197472035885,-0.016551261767745018,-0.01621188223361969,0.01883554458618164,-0.00903270673006773,-0.0020999086555093527,-0.025610074400901794,0.022464290261268616,0.04132594168186188,-0.003684221999719739,-0.007368443999439478,-0.011042876169085503,0.03576534241437912,0.011806479655206203,0.021093720570206642,-0.05043696612119675,-0.009176290594041348,-0.006386202294379473,0.000301035848679021,0.024148132652044296,0.009469984099268913,0.018691960722208023,-0.0008121441351249814,0.009626620449125767,0.04344053566455841,0.034381721168756485,-0.058790914714336395,-0.026119142770767212,-0.007890565320849419,-0.0012310652527958155,-0.003703801427036524,-0.01294209435582161,0.015885556116700172,-0.015728918835520744,0.035321541130542755,-0.03654852509498596,0.021681107580661774,0.019631778821349144,-0.011878270655870438,0.036783479154109955,0.007727402728050947,-0.02848174422979355,-0.035504281520843506,-0.030491912737488747,-0.017177807167172432,-0.005769446026533842,-0.0016707897884771228,-0.01972315087914467,-0.023626010864973068,0.025427332147955894,0.031222883611917496,-0.012126279063522816,0.02084571309387684,-0.001321620773524046,0.016355466097593307,0.04216133430600166,0.033807385712862015,-0.03767108917236328,-0.0028374057728797197,0.018483111634850502,0.034773312509059906,0.017373602837324142,0.008269104175269604,-0.009372086264193058,-0.002432761248201132,-0.0013844385975971818,0.049653783440589905,0.015911661088466644,0.03730560094118118,0.03151005133986473,-0.04751308262348175,-0.006059876177459955,-0.006653789896517992,0.0037233810871839523,-0.006559154950082302,-0.03338968753814697,-0.009104498662054539,0.023717382922768593,-0.014789099805057049,-0.03190164268016815,0.005629125516861677,-0.012726718559861183,0.009339453652501106,-0.011316990479826927,0.00188616500236094,0.003010358428582549,-0.0039289663545787334,0.01974925771355629,-0.031014034524559975,0.020924031734466553,0.01489352434873581,-0.04176974296569824,-0.026040824130177498,-0.0136926444247365,0.04043833166360855,0.0056650214828550816,-0.006082719191908836,-0.0162249356508255,-0.018496165052056313,0.006350306328386068,0.015363434329628944,-0.00783182680606842,0.02494436874985695,0.04960156977176666,-0.010037791915237904,0.020414961501955986,-0.0063731493428349495,0.019971158355474472,0.03184942901134491,-0.029578199610114098,0.010853607207536697,0.021955221891403198,-0.0035602180287241936,-0.02871669828891754,-0.004026864189654589,-0.003671168815344572,0.008843437768518925,0.03187553584575653,-0.031405627727508545,-0.011845638044178486,0.0016454994911327958,0.027855198830366135,0.012902935035526752,0.02010168880224228,0.0038963339757174253,-0.010742655955255032,0.005707444157451391,0.013562113977968693,0.016355466097593307,-0.021746372804045677,0.027124227955937386,-0.012341653928160667,0.01788267120718956,-0.01499794889241457,0.044432565569877625,0.025009633973240852,-0.0024523409083485603,-0.014371402561664581,-0.008497532457113266,-0.03615693375468254,-0.01995810493826866,0.03414676710963249,-0.004382560029625893,-0.021955221891403198,0.01117340661585331,0.0075185541063547134,-0.03015253320336342,0.016707897186279297,0.02683706022799015,-0.025205429643392563,0.02068907581269741,0.036052510142326355,0.005240797530859709,-0.0015345485880970955,-0.026223568245768547,-0.009176290594041348,0.022020487114787102,0.018013201653957367,0.015050160698592663,0.044093187898397446,0.008928283117711544,0.009822416119277477,-0.01266145333647728,-0.016107456758618355,0.020388856530189514,0.015037108212709427,-0.04328389838337898,0.01146057341247797,-0.0033742121886461973,-0.005107003729790449,-0.0027509292121976614,0.017060330137610435,-0.012171964161098003,-0.010129163041710854,0.022425131872296333,0.01694285310804844,0.030779080465435982,-0.00684632221236825,-0.02033664472401142,-0.0032893673051148653,-0.018574483692646027,-0.0082038389518857,0.007583819329738617,-0.0012734876945614815,-0.013379370793700218,0.00017580820713192225,0.03190164268016815,0.03352021798491478,0.002320178784430027,0.02326052635908127,0.020623810589313507,-0.022986412048339844,0.00811246782541275,0.0010801394237205386,0.03294588625431061,0.027515819296240807,-0.034407828003168106,-0.011597630567848682,-0.007642557844519615,-0.007153068669140339,-0.02008863538503647,0.0017996885580942035,0.010383697226643562,0.01313789002597332,-0.026040824130177498,-0.016485996544361115,0.005266903899610043,-0.019605673849582672,0.022085752338171005,-0.00865416880697012,0.01661652699112892,6.755970389349386e-05,0.019292401149868965,0.0022630717139691114,0.013392424210906029,-0.005968505050987005,-0.01378401555120945,-0.013927599415183067,-0.02217712439596653,0.004568565636873245,-0.00828215666115284,-0.010559913702309132,0.0036776955239474773,-0.014045076444745064,-0.0004089274152647704,0.002781930146738887,-0.014045076444745064,-0.01938377134501934,-0.005417013540863991,-0.03490384295582771,0.014045076444745064,0.0027639823965728283,0.0018796385265886784,0.0015092582907527685,-0.03725339099764824,0.0022500185295939445,0.05088077113032341,-0.023038623854517937,-0.012021854519844055,-0.03057023137807846,-0.023404110223054886,-0.020584652200341225,-0.006709265056997538,-0.02105456218123436,-0.004728465341031551,-0.03432950749993324,-0.0012751193717122078,0.01507626660168171,-0.02624967321753502,0.014475827105343342,0.0035765343345701694,-0.014580251649022102,-0.020049476996064186,-0.021367834880948067,-0.010390223935246468,-0.028768911957740784,-0.02494436874985695,-0.011042876169085503,0.03816710412502289,0.002139067742973566,-0.014567198231816292,0.004056233912706375,-0.0076490845531225204,0.01117340661585331,0.0024099184665828943,-0.007792667951434851,0.022855881601572037,0.03388570621609688,0.006630946882069111,0.07283598929643631,0.001368122291751206,-0.02741139382123947,-0.0059880842454731464,0.008882597088813782,-0.0044054025784134865,0.009287241846323013,0.021093720570206642,-0.0029499882366508245,0.05565818399190903,-0.03284146264195442,-0.01715170033276081,0.00763603113591671,-0.0042683458887040615,0.007009485270828009,-0.02835121378302574,-0.025087952613830566,-0.024461407214403152,0.03430340066552162,-0.009065339341759682,0.020362749695777893,0.0036450629122555256,-0.004597934894263744,0.022829776629805565,0.010899292305111885,0.006650526542216539,-0.010279272682964802,-0.018966075032949448,-0.0038473850581794977,-0.010775288566946983,-0.034381721168756485,0.01698201149702072,0.02832510694861412,-0.006852848455309868,0.01566365361213684,-0.018561430275440216,-0.02383485995233059,0.008066781796514988,-0.004046443849802017,0.006356833036988974,-0.008484479039907455,0.018926914781332016,0.002693822141736746,-0.03545207157731056,-0.03294588625431061,-0.0029483565595000982,0.017386656254529953,0.02477467991411686,-0.023508533835411072,0.033441901206970215,0.032110489904880524,0.008001516573131084,0.02623661980032921,0.02798572927713394,-0.0010360854212194681,-0.017282230779528618,-0.009306821040809155,0.0119239566847682,0.06077497825026512,-0.0011331674177199602,-0.05174227058887482,-0.0764908418059349,-0.02420034632086754,0.04579008370637894,-0.021850798279047012,0.02461804263293743,0.0016373413382098079,0.018887756392359734,-0.02272535115480423,0.0020819606725126505,0.01730833761394024,-0.000682021607644856,0.007374970242381096,-0.004506563767790794,0.0030756236519664526,0.015728918835520744,0.059887371957302094,-0.017582451924681664,-0.0004327900242060423,-0.004428245592862368,-0.012446078471839428,-0.00986810214817524,-0.009535249322652817,-0.029186608269810677,0.017256125807762146,0.029395457357168198,0.01828731596469879,-0.020937085151672363,0.07643862813711166,-0.0015794184291735291,-0.022020487114787102,-0.031066246330738068,-0.0014872313477098942,0.02648462913930416,-0.009711465798318386,-0.028951654210686684,-0.012204596772789955,0.030648550018668175,0.00379843614064157,0.009091446176171303,0.004607724957168102,-0.00866722222417593,-0.0006151247653178871,0.029317138716578484,-0.044406458735466,-0.04174363613128662,0.04411929100751877,-0.00261060893535614,-0.03276314213871956,0.0223859716206789,0.01730833761394024,-0.014005917124450207,0.03145783767104149,-0.017360549420118332,0.01519374456256628,-0.000695074675604701,-0.02741139382123947,-0.014514986425638199,0.01221112348139286,0.012446078471839428,-0.009058813564479351,0.008588903583586216,-0.01565060019493103,-0.026589052751660347,-0.035086583346128464,0.031953852623701096,0.01507626660168171,-0.009698412381112576,0.01340547762811184,0.0013036729069426656,0.026171354576945305,0.01480215322226286,0.001524758874438703,0.021681107580661774,-0.00554101774469018,-0.026171354576945305,0.015128479339182377,-0.014410561881959438,-0.003140888875350356,-0.02363906428217888,-0.021942168474197388,0.02420034632086754,0.00961356796324253,-0.010364118032157421,0.012602714821696281,-0.012818090617656708,-0.0008647642098367214,-0.0051690055988729,0.0409865602850914,0.015611441805958748,-0.016851481050252914,-0.014684675261378288,-0.013927599415183067,-0.0015598388854414225,0.000412802561186254,-0.013431583531200886,-0.027280863374471664,-0.02383485995233059,0.0038375952281057835,-0.024552777409553528,-0.023991497233510017,0.004679516423493624,0.018509218469262123,-0.014789099805057049,0.011330042965710163,-0.006301357410848141,0.03038748912513256,0.0015924714971333742,-0.018378688022494316,0.001023032353259623,0.020349696278572083,-0.00033224080107174814,0.00485246954485774,-0.001191906165331602,-0.006781056988984346,-0.016159670427441597,-0.008380054496228695,0.00147744151763618,-0.0016724213492125273,-0.0012628821423277259,-0.015454805456101894,-0.004960156977176666,-0.003423161106184125,-0.01397981122136116,0.015533123165369034,0.020806552842259407,0.0017164754681289196,-0.013953705318272114,-0.008719434030354023,0.006236092187464237,0.00950914341956377,-0.03928966447710991,-0.006800636649131775,0.004066023509949446,-0.002517606131732464,0.017451921477913857,0.018691960722208023,-0.014554144814610481,-0.022581767290830612,-0.024709414690732956,-0.011206039227545261,-0.025792816653847694,0.012922514230012894,0.02645852230489254,-0.02494436874985695,-0.026301885023713112,-0.03075297363102436,0.005162479355931282,0.011982695199549198,0.010214007459580898,0.020193060860037804,-0.010429383255541325,-0.022111859172582626,-0.017269179224967957,0.025296801701188087,-0.012798510491847992,0.0279335156083107,0.004219396971166134,0.0020183271262794733,-0.025218483060598373,-0.01598997972905636,-0.018757225945591927,0.03151005133986473,-0.0034753731451928616,-0.013098730705678463,-0.01399286463856697,0.008432267233729362,-0.001650394406169653,-0.01749107986688614,0.02554480917751789,0.0037755933590233326,0.002520869253203273,0.02835121378302574,-0.009039233438670635,-0.009685358963906765,0.01843089982867241,-0.01507626660168171,-0.018208997324109077,-0.015063214115798473,0.016459889709949493,0.01498489547520876,-0.018587535247206688,-0.011388781480491161,-0.0368356928229332,0.005064581520855427,0.013522954657673836,0.02981315553188324,-0.011421414092183113,-0.00970493908971548,-0.01193701010197401,0.012452605180442333,0.009182817302644253,0.02866448648273945,-0.011336569674313068,-0.03151005133986473,-0.014201712794601917,-0.0028374057728797197,-0.020910978317260742,4.5379725634120405e-05,0.01276587788015604,0.026314938440918922,0.016146617010235786,0.04067328944802284,-0.0006677448400296271,-0.007061697542667389,-0.01510237343609333,0.00867374800145626,-0.0006134930881671607,-0.01396675780415535,0.037018436938524246,-0.019945053383708,0.013085677288472652,-0.020375803112983704,-0.01990589313209057,-0.025636181235313416,0.03145783767104149,0.022477343678474426,0.024370035156607628,0.007237913552671671,-0.020963190123438835,0.02089792490005493,0.0026595578528940678,-0.0361047238111496,-0.05826879292726517,-0.024278663098812103,-0.003276314353570342,-0.011525838635861874,0.01792183145880699,0.01730833761394024,-0.00432708440348506,-0.011780372820794582,-0.008236471563577652,-0.012805037200450897,0.01974925771355629,0.03665294870734215,-0.007538133300840855,0.016277147457003593,0.030048109591007233,0.025936400517821312,-0.025440385565161705,0.012185017578303814,-0.006643999833613634,0.01201532781124115,0.021158985793590546,0.015128479339182377,-0.017934883013367653,0.05017590522766113,-0.027306970208883286,-0.05064581334590912,-0.006696212105453014,0.005782498978078365,-0.00979631021618843,0.010768761858344078,0.007309705018997192,-0.035086583346128464,-0.005945662036538124,-0.007981937378644943,0.02425255812704563,-0.001894323155283928,-0.0016112352022901177,0.011839112266898155,0.012067540548741817,0.009235029108822346,0.012524397112429142,0.053204212337732315,-0.022033540531992912,-0.004079076461493969,-0.009567881934344769,0.053178105503320694,-0.022242389619350433,0.029734836891293526,0.024996580556035042,0.03409455344080925,-0.02983926050364971,0.030700761824846268,-0.06808467954397202,-0.00033652380807325244,0.038114890456199646,0.011571524664759636,0.002998937154188752,0.033807385712862015,0.0090261809527874,0.003739697393029928,0.007074750494211912,0.0187833309173584,-0.009287241846323013,0.030883504077792168,0.007342337630689144,0.006395991891622543,-0.00884996447712183,-0.04495468735694885,-0.0540396049618721,-0.02996979095041752,-0.013026938773691654,-0.050202012062072754,-0.01016832236200571,0.013849280774593353,-0.018574483692646027,-0.007257493212819099,0.006363359279930592,-0.009920313954353333,0.013823174871504307,-0.005527964793145657,0.001806215150281787,0.007237913552671671,0.02665431797504425,-0.011649842374026775,-0.035713132470846176,0.01747802644968033,-0.013353264890611172,-0.008504059165716171,0.025309855118393898,0.027333077043294907,0.016720950603485107,-0.03017864003777504,-0.03263261169195175,-0.002943461760878563,0.04620777815580368,-0.018195943906903267,0.021498365327715874,0.006950746290385723,-0.011068982072174549,0.0020640126895159483,-0.013196628540754318,-0.014697728678584099,-0.017399709671735764,-0.03618304058909416,-0.03592197969555855,0.0049666836857795715]' AS cosine_distance \n",
      "FROM chunks ORDER BY chunks.embedding <=> '[-0.031588368117809296,-0.04046443849802017,0.09309431910514832,-0.008797752670943737,0.01267450675368309,-0.038271527737379074,0.09251998364925385,0.03453835844993591,-0.019657885655760765,-0.009678833186626434,0.032893672585487366,0.03187553584575653,-0.010266220197081566,-0.030283063650131226,0.05503163859248161,0.017765194177627563,0.004999316297471523,0.034198977053165436,-0.040699392557144165,0.0761253610253334,0.04383212700486183,-0.010305378586053848,-0.02327357977628708,0.0058118682354688644,-0.0006110456888563931,-0.005609546322375536,-0.03727949783205986,0.05518827587366104,0.016655685380101204,0.007890565320849419,-0.050384752452373505,-0.02425255812704563,0.01557228248566389,0.0540396049618721,0.03294588625431061,-0.007218333892524242,0.024004550650715828,-0.013757909648120403,-0.012791983783245087,0.03148394450545311,-0.019475143402814865,-0.04017727077007294,-0.00237402250058949,-0.006409045308828354,-0.016433782875537872,-0.02929103374481201,-0.021850798279047012,0.02591029368340969,0.003313841763883829,-0.03881975635886192,0.0034623201936483383,-0.026406310498714447,0.026602106168866158,0.010279272682964802,-0.011865218169987202,-0.02572755143046379,-0.024082867428660393,0.01304651889950037,-0.004862259142100811,0.024174239486455917,0.06761477142572403,-0.008706380613148212,-0.03526932746171951,0.040699392557144165,0.014123395085334778,-0.00828215666115284,-0.009502616710960865,-0.026393257081508636,-0.004832889884710312,0.04320557788014412,-0.04453698918223381,-0.013562113977968693,0.025649232789874077,-0.01219154428690672,0.01657736673951149,-0.014332243241369724,-0.005599756259471178,0.04941882938146591,0.011088562197983265,-0.012635347433388233,-0.0006073744734749198,-0.027124227955937386,-0.0025502387434244156,0.005756393074989319,0.014528038911521435,-0.023808754980564117,-0.05372633412480354,0.02798572927713394,-0.029395457357168198,0.00949609000235796,-0.0026171356439590454,-0.0034133712761104107,0.02029748447239399,0.0555015467107296,0.05983515828847885,0.02367822453379631,-0.002576344646513462,0.017556345090270042,0.007329284679144621,0.009411245584487915,0.00452288007363677,-0.0313534140586853,-0.010448962450027466,-0.032475974410772324,0.008915229700505733,-0.019449036568403244,0.030648550018668175,0.004761097952723503,-0.017974043264985085,0.026314938440918922,-0.088812917470932,-0.029734836891293526,0.01546785794198513,0.013901492580771446,-0.004186764359474182,-0.015337327495217323,0.0368356928229332,-0.062497980892658234,-0.007962357252836227,-0.04059496894478798,0.018561430275440216,0.0380626805126667,0.012733245268464088,-0.019971158355474472,0.032475974410772324,-0.024265611544251442,-0.025492597371339798,-0.012915988452732563,0.0038539115339517593,-0.03200606629252434,0.05576260760426521,0.016120510175824165,0.05255156010389328,-0.01939682476222515,-0.02162889577448368,-0.016251040622591972,-0.028220683336257935,0.024826891720294952,-0.05276040732860565,0.019853681325912476,0.005482278764247894,0.03448614478111267,0.009058813564479351,0.01995810493826866,0.03357243165373802,7.123087561922148e-05,-0.03834984451532364,-0.01621188223361969,-0.0424485020339489,0.03709675371646881,0.002126014791429043,-0.01806541346013546,-0.03628746420145035,-0.01035106461495161,0.026706529781222343,-0.01674705743789673,0.01674705743789673,-0.02929103374481201,-0.02536206692457199,-0.013327158987522125,-0.0020020108204334974,-0.007264019455760717,0.025845028460025787,-0.0018421109998598695,0.017765194177627563,0.02589724212884903,0.011976168490946293,0.0180523619055748,-0.026536840945482254,-0.012256809510290623,-0.018861649557948112,-0.014110341668128967,0.018026255071163177,-0.007962357252836227,0.06677937507629395,-0.036600738763809204,0.043884336948394775,-0.03814099729061127,-0.019605673849582672,-0.022790616378188133,-0.006324200425297022,-0.005697654094547033,-0.023769594728946686,-0.0065036797896027565,-0.027254758402705193,0.026758741587400436,0.03448614478111267,-0.005475752521306276,0.00958746112883091,-0.016264094039797783,-0.025479543954133987,-0.03302420303225517,-0.036052510142326355,0.033050309866666794,0.04169142618775368,-0.022647032514214516,-0.003583060810342431,0.005847764201462269,-0.014541092328727245,-0.009124078787863255,-0.005038475152105093,0.010031265206634998,-0.03521711379289627,-0.034721098840236664,-0.06317673623561859,0.0024131815880537033,-0.012589662335813046,0.007388023659586906,0.04038612172007561,-0.015924714505672455,-0.001982431160286069,-0.021380888298153877,-0.04565955325961113,0.08724655210971832,-0.012328601442277431,-0.007864459417760372,0.002471920335665345,-0.019892839714884758,-0.016485996544361115,-0.037749405950307846,-0.04007284715771675,-0.011845638044178486,0.006709265056997538,0.04383212700486183,0.028090152889490128,0.020441068336367607,0.020910978317260742,0.005257113836705685,0.07304483652114868,-0.007250966504216194,-0.00828215666115284,0.008438793942332268,-0.06740592420101166,0.04263124614953995,-0.016107456758618355,0.05497942492365837,-0.06510858982801437,-0.026941485702991486,-0.022881988435983658,-0.04362327605485916,-0.02294725365936756,-0.0014309401158243418,-0.035687025636434555,-0.005475752521306276,-0.00037466318462975323,0.01358821988105774,-0.010559913702309132,-0.02832510694861412,-0.024474458768963814,-0.020924031734466553,-0.020388856530189514,-0.03375517576932907,0.03503437340259552,-0.032893672585487366,-0.009078392758965492,-0.0242133978754282,-0.02923882007598877,-0.026562945917248726,0.022973358631134033,-0.025022687390446663,-0.03986399993300438,-0.002197806490585208,-0.010931924916803837,0.00713348900899291,-0.004036654252558947,0.01920102909207344,-0.015063214115798473,0.005798815283924341,-0.019879788160324097,0.019305452704429626,0.03928966447710991,-0.040882136672735214,0.03516490384936333,-0.02905607782304287,0.0240306556224823,0.007675190456211567,0.0008508953615091741,0.003703801427036524,-0.021707214415073395,0.030857399106025696,-0.06798025965690613,-0.02607998438179493,-0.01863974891602993,0.03545207157731056,-0.020166954025626183,0.0387936495244503,0.014201712794601917,0.04456309601664543,0.017843512818217278,0.018143732100725174,0.048922814428806305,0.022633980959653854,0.0018127416260540485,0.006258935201913118,0.008960915729403496,-0.015167638659477234,-0.0017523713177070022,0.015428699553012848,-0.03712286055088043,0.011786899529397488,0.053935181349515915,0.05247323960065842,0.016472943127155304,-0.017373602837324142,-0.023913178592920303,-0.02161584235727787,0.04542459547519684,-0.01163678988814354,0.03704454004764557,0.0320843830704689,-0.030309170484542847,-0.0368356928229332,0.027672454714775085,0.020271379500627518,0.032110489904880524,-0.03785382956266403,-0.04680822044610977,0.02572755143046379,0.019240187481045723,-0.03054412454366684,-0.044406458735466,-0.05064581334590912,-0.012047960422933102,0.00034794522798620164,-0.009019654244184494,-0.004989526234567165,0.029108289629220963,-0.022594820708036423,0.021720267832279205,-0.018887756392359734,-0.00931334774941206,-0.004428245592862368,-0.013105257414281368,0.02645852230489254,0.026589052751660347,-0.03874143585562706,0.021158985793590546,0.009548302739858627,-0.022477343678474426,-0.042970623821020126,-0.05858206748962402,0.013549060560762882,-0.0038049626164138317,-0.0032371552661061287,-0.00809941440820694,-0.009985579177737236,0.012733245268464088,0.004532669670879841,0.05466615408658981,0.006562418304383755,-0.011538892053067684,0.02460498921573162,-0.03620914742350578,-0.03579144924879074,0.017034223303198814,0.001827426254749298,0.03816710412502289,-0.037906043231487274,-0.03315473347902298,0.01332063227891922,0.03182332217693329,0.024122027680277824,0.020023370161652565,0.006304620765149593,0.029134396463632584,0.04422371834516525,0.009071866050362587,-0.0164076779037714,-0.05899976193904877,-0.005694390740245581,0.05722454935312271,-0.006275251507759094,-0.04114319756627083,0.025101006031036377,-0.06448204070329666,0.02084571309387684,0.014240872114896774,0.01360127329826355,0.051846694201231,0.030883504077792168,0.03506048023700714,0.005576913710683584,0.025492597371339798,0.02161584235727787,-0.007557712960988283,0.0054888054728507996,0.02033664472401142,-0.02942156419157982,-0.007538133300840855,-0.04764361307024956,0.016538208350539207,-0.02087181806564331,-0.002126014791429043,0.00656894501298666,0.020219165831804276,0.01937071792781353,0.009169763885438442,0.007936251349747181,0.013509901240468025,-0.011055929586291313,0.028064046055078506,-0.03151005133986473,-0.03453835844993591,-0.012433025054633617,0.024122027680277824,0.0259233471006155,-0.003971389029175043,-0.016812322661280632,-0.005808604881167412,-0.0852624922990799,0.01639462448656559,-0.06604840606451035,0.055240485817193985,0.011414888314902782,-0.033781278878450394,-0.01716475374996662,-0.0010809552622959018,-0.0009797941893339157,0.01602913998067379,0.03443393111228943,-0.024722468107938766,0.0025257642846554518,-0.053569696843624115,-0.03221491351723671,0.014971842989325523,-0.008960915729403496,-0.010370643809437752,0.0203105378895998,-0.06040949374437332,-0.04007284715771675,-0.009593987837433815,-0.05946967378258705,0.014214766211807728,0.03795825317502022,0.005749866366386414,0.002357706194743514,-0.06056612730026245,-0.07393244653940201,0.022973358631134033,0.025192376226186752,0.017099488526582718,0.018208997324109077,0.016538208350539207,0.010279272682964802,-0.05795551836490631,-0.0009822415886446834,-0.07628199458122253,-0.04075160622596741,-0.019083552062511444,0.014880470931529999,-0.03315473347902298,-0.07748287171125412,0.02235986664891243,0.024552777409553528,0.06181922182440758,0.030883504077792168,0.018195943906903267,0.06839795410633087,4.137713403906673e-05,0.03892417997121811,0.018391739577054977,0.011982695199549198,-0.015063214115798473,0.037749405950307846,0.013203155249357224,-0.005035212263464928,0.009065339341759682,0.014371402561664581,0.003990968689322472,-0.02759413793683052,0.031771112233400345,-0.0034590568393468857,-0.03707064688205719,-0.04639052227139473,-0.05398739501833916,-0.026562945917248726,-0.01617272198200226,-0.02647157572209835,-0.02777688018977642,-0.028612274676561356,-0.013000832870602608,0.016668738797307014,-0.018117627128958702,0.031745005398988724,0.044249821454286575,0.10291020572185516,-0.03545207157731056,0.0019106394611299038,-0.03338968753814697,0.04338832199573517,-0.025845028460025787,0.045163534581661224,0.050619710236787796,-0.014815205708146095,-0.0011804847745224833,-0.03057023137807846,0.005629125516861677,0.01508932001888752,-0.010657811537384987,0.04949714615941048,0.012224176898598671,-0.004408665932714939,-0.030413594096899033,0.04289230704307556,0.026184407994151115,-0.002104803454130888,-0.014136447571218014,-0.0026138722896575928,0.021929115056991577,0.004950367379933596,0.004783940967172384,-0.03785382956266403,0.018339527770876884,-0.04041222855448723,0.01407118234783411,0.019462089985609055,-0.034564461559057236,0.012511343695223331,0.001721370266750455,0.005713970400393009,-0.013640431687235832,0.033441901206970215,-0.0013958599884063005,0.02345632202923298,0.013379370793700218,-0.021172039210796356,-0.018900809809565544,-0.06714486330747604,-0.03443393111228943,-0.03730560094118118,0.004281398840248585,-0.021733319386839867,-0.03132730722427368,-0.05372633412480354,0.0009757151128724217,-0.02198132872581482,0.0180523619055748,-0.029134396463632584,-0.005126583389937878,-0.03234544396400452,0.0013420161558315158,0.037749405950307846,-0.009574408642947674,-0.010918872430920601,-0.03338968753814697,0.0011470363242551684,-0.021889956668019295,0.016316305845975876,-0.0010458752512931824,-0.029029972851276398,0.02406981587409973,-0.03367685526609421,-0.006023980211466551,0.01312483660876751,0.021132880821824074,0.005446383263915777,-0.005531227681785822,-0.002824352588504553,-0.04328389838337898,-0.022451236844062805,0.025858081877231598,-0.012034907937049866,0.03670516237616539,0.006559154950082302,0.007459815125912428,0.05497942492365837,0.005567123647779226,-0.042605139315128326,-0.039785679429769516,-0.027072016149759293,-0.013744856230914593,-0.004284662194550037,-0.0346427820622921,-0.01417560689151287,0.0007460631313733757,-0.009502616710960865,-0.011375728994607925,-0.060670554637908936,0.004382560029625893,-0.023051677271723747,0.012485237792134285,-0.01880943775177002,-0.012968200258910656,-0.005387644283473492,-0.012217650189995766,-0.019644832238554955,0.016303252428770065,0.015011001378297806,0.014945736154913902,-0.05409181863069534,0.03166668862104416,0.007211807183921337,-0.007002958562225103,-0.003860438009724021,0.019331559538841248,-0.00689853448420763,0.0331808403134346,0.015063214115798473,-0.00012094461999367923,-0.016694843769073486,0.03453835844993591,0.014528038911521435,-0.003609166946262121,-0.04111709073185921,0.015415646135807037,-0.010161795653402805,-0.003449267242103815,-0.013314105570316315,-0.032710932195186615,-0.032162703573703766,-0.022412078455090523,0.033259157091379166,-0.029369350522756577,-0.023143049329519272,0.033468008041381836,0.017791301012039185,0.03372906893491745,0.017843512818217278,-0.016525154933333397,-0.02144615352153778,-0.014971842989325523,-0.021498365327715874,-0.03265871852636337,0.02051938697695732,0.035321541130542755,0.0015223113587126136,0.016133563593029976,-0.0022924409713596106,-0.002434392925351858,0.024644149467349052,-0.02587113529443741,0.02237292006611824,0.036600738763809204,-0.016720950603485107,-0.04978431388735771,-0.007616451941430569,0.04675600677728653,0.018391739577054977,0.015976926311850548,-0.011799952946603298,0.02589724212884903,0.0004935682518407702,-0.010383697226643562,-0.023143049329519272,0.029552094638347626,0.025440385565161705,0.006591787561774254,0.031745005398988724,0.04299673065543175,0.0034035814460366964,-0.03237155079841614,0.010599073022603989,0.00950914341956377,-0.009058813564479351,0.03814099729061127,0.026040824130177498,-0.03892417997121811,-0.030805185437202454,0.00829521007835865,0.012485237792134285,-0.015598388388752937,0.013705696910619736,-0.006252408493310213,0.005381118040531874,-0.025022687390446663,0.0029026709962636232,-0.040542759001255035,0.014619410037994385,-0.04150868207216263,-0.02161584235727787,-0.044093187898397446,0.03975957632064819,-0.001545154256746173,0.012504816986620426,0.02216407097876072,0.0011731424601748586,0.020819606259465218,-0.02853395603597164,-0.00978325679898262,0.030413594096899033,0.036966223269701004,0.013307579793035984,-0.03255429491400719,0.013157469220459461,0.020950136706233025,0.0001971213787328452,0.040699392557144165,0.00961356796324253,-0.01332063227891922,-0.017451921477913857,0.03090961091220379,-0.05252545326948166,0.030831292271614075,0.004597934894263744,0.01750413328409195,0.008641115389764309,0.014919630251824856,-0.016525154933333397,0.02683706022799015,-0.01843089982867241,-0.00988768134266138,-0.0034916894510388374,0.05419624224305153,-0.013731803745031357,-0.012733245268464088,-0.003393791615962982,-0.027437500655651093,-0.02326052635908127,0.029134396463632584,0.005798815283924341,-0.0167731624096632,-0.0342772975564003,-0.039942316710948944,-0.03448614478111267,-0.03461667522788048,-0.021720267832279205,0.03853258863091469,-9.432864317204803e-06,0.0317188985645771,-0.020950136706233025,-0.02386096678674221,0.018678907305002213,-0.002912460593506694,0.027306970208883286,0.011852164752781391,0.017177807167172432,0.025818923488259315,-0.06683158874511719,-0.02126341126859188,0.0031474155839532614,0.022320706397294998,0.011088562197983265,0.03349411487579346,0.044824156910181046,0.017190860584378242,0.00811246782541275,-0.04174363613128662,-0.011584577150642872,-0.014841312542557716,-0.010103057138621807,0.012746298685669899,0.009182817302644253,-0.021341728046536446,0.044824156910181046,-0.00011482600530143827,-0.030987929552793503,0.035504281520843506,0.04607724770903587,0.03965514898300171,0.0038473850581794977,-0.009633147157728672,0.010553386993706226,-0.004056233912706375,0.0036613792181015015,-0.0036287466064095497,-0.03803657367825508,-0.032867565751075745,-0.006768004037439823,-0.02178553305566311,0.039890106767416,0.013562113977968693,0.028951654210686684,-0.03002200275659561,-0.008595430292189121,0.001998747466132045,0.026132196187973022,0.0008198943687602878,-0.007120436057448387,-0.0038049626164138317,0.016472943127155304,0.0032192072831094265,-0.01749107986688614,0.020754341036081314,-0.00031572053558193147,0.02848174422979355,0.0060272435657680035,-0.06202806904911995,0.006180617026984692,-0.04022948443889618,0.03166668862104416,-0.02722865156829357,-0.042944516986608505,-0.005074371118098497,0.0121066989377141,-0.004128025379031897,0.01696895807981491,0.03054412454366684,0.029943685978651047,-0.03036138229072094,-0.028403425589203835,-0.012243756093084812,0.0035308487713336945,-0.0346427820622921,0.03388570621609688,0.027646349743008614,0.00423571327701211,-0.019814522936940193,-0.06035728007555008,0.013268420472741127,-0.03675737604498863,-0.0043140314519405365,0.015154585242271423,0.02754192426800728,0.012537449598312378,0.007955830544233322,0.02216407097876072,0.015063214115798473,-0.0024572357069700956,-0.0048818388022482395,-0.024696361273527145,0.01052728109061718,0.0020819606725126505,-0.05701570212841034,-0.0043205576948821545,-0.009822416119277477,-0.013757909648120403,0.027646349743008614,-0.018718065693974495,-0.013614325784146786,0.020454121753573418,0.06056612730026245,-0.023900125175714493,0.0037331709172576666,0.0060729291290044785,-0.00512984674423933,-0.01920102909207344,0.009548302739858627,-0.019683992490172386,-0.005240797530859709,-0.0391591340303421,0.022660085931420326,-0.011395308189094067,0.032162703573703766,-0.03221491351723671,0.0039289663545787334,0.006262198556214571,-0.015206797048449516,-0.022594820708036423,0.0032436817418783903,0.009652726352214813,0.01750413328409195,-0.017073383554816246,0.016264094039797783,-0.00940471887588501,0.023730436339974403,-0.05255156010389328,0.03657463192939758,-0.0013322264421731234,0.010292326100170612,-0.009104498662054539,0.0017964253202080727,0.039968423545360565,0.06375107169151306,-0.017960989847779274,-0.007433709222823381,0.005694390740245581,0.025048794224858284,0.004131288733333349,-0.01193701010197401,-0.011773847043514252,-0.001962851732969284,-0.030335277318954468,0.017203914001584053,-0.005952188745141029,-0.005762919317930937,-0.061767008155584335,-0.008027622476220131,-0.0040138112381100655,0.017974043264985085,0.01099066436290741,-0.022633980959653854,-0.014841312542557716,-0.015389540232717991,-0.030648550018668175,0.019109657034277916,0.001650394406169653,-0.0491577684879303,-0.01807846687734127,0.002909197472035885,-0.016551261767745018,-0.01621188223361969,0.01883554458618164,-0.00903270673006773,-0.0020999086555093527,-0.025610074400901794,0.022464290261268616,0.04132594168186188,-0.003684221999719739,-0.007368443999439478,-0.011042876169085503,0.03576534241437912,0.011806479655206203,0.021093720570206642,-0.05043696612119675,-0.009176290594041348,-0.006386202294379473,0.000301035848679021,0.024148132652044296,0.009469984099268913,0.018691960722208023,-0.0008121441351249814,0.009626620449125767,0.04344053566455841,0.034381721168756485,-0.058790914714336395,-0.026119142770767212,-0.007890565320849419,-0.0012310652527958155,-0.003703801427036524,-0.01294209435582161,0.015885556116700172,-0.015728918835520744,0.035321541130542755,-0.03654852509498596,0.021681107580661774,0.019631778821349144,-0.011878270655870438,0.036783479154109955,0.007727402728050947,-0.02848174422979355,-0.035504281520843506,-0.030491912737488747,-0.017177807167172432,-0.005769446026533842,-0.0016707897884771228,-0.01972315087914467,-0.023626010864973068,0.025427332147955894,0.031222883611917496,-0.012126279063522816,0.02084571309387684,-0.001321620773524046,0.016355466097593307,0.04216133430600166,0.033807385712862015,-0.03767108917236328,-0.0028374057728797197,0.018483111634850502,0.034773312509059906,0.017373602837324142,0.008269104175269604,-0.009372086264193058,-0.002432761248201132,-0.0013844385975971818,0.049653783440589905,0.015911661088466644,0.03730560094118118,0.03151005133986473,-0.04751308262348175,-0.006059876177459955,-0.006653789896517992,0.0037233810871839523,-0.006559154950082302,-0.03338968753814697,-0.009104498662054539,0.023717382922768593,-0.014789099805057049,-0.03190164268016815,0.005629125516861677,-0.012726718559861183,0.009339453652501106,-0.011316990479826927,0.00188616500236094,0.003010358428582549,-0.0039289663545787334,0.01974925771355629,-0.031014034524559975,0.020924031734466553,0.01489352434873581,-0.04176974296569824,-0.026040824130177498,-0.0136926444247365,0.04043833166360855,0.0056650214828550816,-0.006082719191908836,-0.0162249356508255,-0.018496165052056313,0.006350306328386068,0.015363434329628944,-0.00783182680606842,0.02494436874985695,0.04960156977176666,-0.010037791915237904,0.020414961501955986,-0.0063731493428349495,0.019971158355474472,0.03184942901134491,-0.029578199610114098,0.010853607207536697,0.021955221891403198,-0.0035602180287241936,-0.02871669828891754,-0.004026864189654589,-0.003671168815344572,0.008843437768518925,0.03187553584575653,-0.031405627727508545,-0.011845638044178486,0.0016454994911327958,0.027855198830366135,0.012902935035526752,0.02010168880224228,0.0038963339757174253,-0.010742655955255032,0.005707444157451391,0.013562113977968693,0.016355466097593307,-0.021746372804045677,0.027124227955937386,-0.012341653928160667,0.01788267120718956,-0.01499794889241457,0.044432565569877625,0.025009633973240852,-0.0024523409083485603,-0.014371402561664581,-0.008497532457113266,-0.03615693375468254,-0.01995810493826866,0.03414676710963249,-0.004382560029625893,-0.021955221891403198,0.01117340661585331,0.0075185541063547134,-0.03015253320336342,0.016707897186279297,0.02683706022799015,-0.025205429643392563,0.02068907581269741,0.036052510142326355,0.005240797530859709,-0.0015345485880970955,-0.026223568245768547,-0.009176290594041348,0.022020487114787102,0.018013201653957367,0.015050160698592663,0.044093187898397446,0.008928283117711544,0.009822416119277477,-0.01266145333647728,-0.016107456758618355,0.020388856530189514,0.015037108212709427,-0.04328389838337898,0.01146057341247797,-0.0033742121886461973,-0.005107003729790449,-0.0027509292121976614,0.017060330137610435,-0.012171964161098003,-0.010129163041710854,0.022425131872296333,0.01694285310804844,0.030779080465435982,-0.00684632221236825,-0.02033664472401142,-0.0032893673051148653,-0.018574483692646027,-0.0082038389518857,0.007583819329738617,-0.0012734876945614815,-0.013379370793700218,0.00017580820713192225,0.03190164268016815,0.03352021798491478,0.002320178784430027,0.02326052635908127,0.020623810589313507,-0.022986412048339844,0.00811246782541275,0.0010801394237205386,0.03294588625431061,0.027515819296240807,-0.034407828003168106,-0.011597630567848682,-0.007642557844519615,-0.007153068669140339,-0.02008863538503647,0.0017996885580942035,0.010383697226643562,0.01313789002597332,-0.026040824130177498,-0.016485996544361115,0.005266903899610043,-0.019605673849582672,0.022085752338171005,-0.00865416880697012,0.01661652699112892,6.755970389349386e-05,0.019292401149868965,0.0022630717139691114,0.013392424210906029,-0.005968505050987005,-0.01378401555120945,-0.013927599415183067,-0.02217712439596653,0.004568565636873245,-0.00828215666115284,-0.010559913702309132,0.0036776955239474773,-0.014045076444745064,-0.0004089274152647704,0.002781930146738887,-0.014045076444745064,-0.01938377134501934,-0.005417013540863991,-0.03490384295582771,0.014045076444745064,0.0027639823965728283,0.0018796385265886784,0.0015092582907527685,-0.03725339099764824,0.0022500185295939445,0.05088077113032341,-0.023038623854517937,-0.012021854519844055,-0.03057023137807846,-0.023404110223054886,-0.020584652200341225,-0.006709265056997538,-0.02105456218123436,-0.004728465341031551,-0.03432950749993324,-0.0012751193717122078,0.01507626660168171,-0.02624967321753502,0.014475827105343342,0.0035765343345701694,-0.014580251649022102,-0.020049476996064186,-0.021367834880948067,-0.010390223935246468,-0.028768911957740784,-0.02494436874985695,-0.011042876169085503,0.03816710412502289,0.002139067742973566,-0.014567198231816292,0.004056233912706375,-0.0076490845531225204,0.01117340661585331,0.0024099184665828943,-0.007792667951434851,0.022855881601572037,0.03388570621609688,0.006630946882069111,0.07283598929643631,0.001368122291751206,-0.02741139382123947,-0.0059880842454731464,0.008882597088813782,-0.0044054025784134865,0.009287241846323013,0.021093720570206642,-0.0029499882366508245,0.05565818399190903,-0.03284146264195442,-0.01715170033276081,0.00763603113591671,-0.0042683458887040615,0.007009485270828009,-0.02835121378302574,-0.025087952613830566,-0.024461407214403152,0.03430340066552162,-0.009065339341759682,0.020362749695777893,0.0036450629122555256,-0.004597934894263744,0.022829776629805565,0.010899292305111885,0.006650526542216539,-0.010279272682964802,-0.018966075032949448,-0.0038473850581794977,-0.010775288566946983,-0.034381721168756485,0.01698201149702072,0.02832510694861412,-0.006852848455309868,0.01566365361213684,-0.018561430275440216,-0.02383485995233059,0.008066781796514988,-0.004046443849802017,0.006356833036988974,-0.008484479039907455,0.018926914781332016,0.002693822141736746,-0.03545207157731056,-0.03294588625431061,-0.0029483565595000982,0.017386656254529953,0.02477467991411686,-0.023508533835411072,0.033441901206970215,0.032110489904880524,0.008001516573131084,0.02623661980032921,0.02798572927713394,-0.0010360854212194681,-0.017282230779528618,-0.009306821040809155,0.0119239566847682,0.06077497825026512,-0.0011331674177199602,-0.05174227058887482,-0.0764908418059349,-0.02420034632086754,0.04579008370637894,-0.021850798279047012,0.02461804263293743,0.0016373413382098079,0.018887756392359734,-0.02272535115480423,0.0020819606725126505,0.01730833761394024,-0.000682021607644856,0.007374970242381096,-0.004506563767790794,0.0030756236519664526,0.015728918835520744,0.059887371957302094,-0.017582451924681664,-0.0004327900242060423,-0.004428245592862368,-0.012446078471839428,-0.00986810214817524,-0.009535249322652817,-0.029186608269810677,0.017256125807762146,0.029395457357168198,0.01828731596469879,-0.020937085151672363,0.07643862813711166,-0.0015794184291735291,-0.022020487114787102,-0.031066246330738068,-0.0014872313477098942,0.02648462913930416,-0.009711465798318386,-0.028951654210686684,-0.012204596772789955,0.030648550018668175,0.00379843614064157,0.009091446176171303,0.004607724957168102,-0.00866722222417593,-0.0006151247653178871,0.029317138716578484,-0.044406458735466,-0.04174363613128662,0.04411929100751877,-0.00261060893535614,-0.03276314213871956,0.0223859716206789,0.01730833761394024,-0.014005917124450207,0.03145783767104149,-0.017360549420118332,0.01519374456256628,-0.000695074675604701,-0.02741139382123947,-0.014514986425638199,0.01221112348139286,0.012446078471839428,-0.009058813564479351,0.008588903583586216,-0.01565060019493103,-0.026589052751660347,-0.035086583346128464,0.031953852623701096,0.01507626660168171,-0.009698412381112576,0.01340547762811184,0.0013036729069426656,0.026171354576945305,0.01480215322226286,0.001524758874438703,0.021681107580661774,-0.00554101774469018,-0.026171354576945305,0.015128479339182377,-0.014410561881959438,-0.003140888875350356,-0.02363906428217888,-0.021942168474197388,0.02420034632086754,0.00961356796324253,-0.010364118032157421,0.012602714821696281,-0.012818090617656708,-0.0008647642098367214,-0.0051690055988729,0.0409865602850914,0.015611441805958748,-0.016851481050252914,-0.014684675261378288,-0.013927599415183067,-0.0015598388854414225,0.000412802561186254,-0.013431583531200886,-0.027280863374471664,-0.02383485995233059,0.0038375952281057835,-0.024552777409553528,-0.023991497233510017,0.004679516423493624,0.018509218469262123,-0.014789099805057049,0.011330042965710163,-0.006301357410848141,0.03038748912513256,0.0015924714971333742,-0.018378688022494316,0.001023032353259623,0.020349696278572083,-0.00033224080107174814,0.00485246954485774,-0.001191906165331602,-0.006781056988984346,-0.016159670427441597,-0.008380054496228695,0.00147744151763618,-0.0016724213492125273,-0.0012628821423277259,-0.015454805456101894,-0.004960156977176666,-0.003423161106184125,-0.01397981122136116,0.015533123165369034,0.020806552842259407,0.0017164754681289196,-0.013953705318272114,-0.008719434030354023,0.006236092187464237,0.00950914341956377,-0.03928966447710991,-0.006800636649131775,0.004066023509949446,-0.002517606131732464,0.017451921477913857,0.018691960722208023,-0.014554144814610481,-0.022581767290830612,-0.024709414690732956,-0.011206039227545261,-0.025792816653847694,0.012922514230012894,0.02645852230489254,-0.02494436874985695,-0.026301885023713112,-0.03075297363102436,0.005162479355931282,0.011982695199549198,0.010214007459580898,0.020193060860037804,-0.010429383255541325,-0.022111859172582626,-0.017269179224967957,0.025296801701188087,-0.012798510491847992,0.0279335156083107,0.004219396971166134,0.0020183271262794733,-0.025218483060598373,-0.01598997972905636,-0.018757225945591927,0.03151005133986473,-0.0034753731451928616,-0.013098730705678463,-0.01399286463856697,0.008432267233729362,-0.001650394406169653,-0.01749107986688614,0.02554480917751789,0.0037755933590233326,0.002520869253203273,0.02835121378302574,-0.009039233438670635,-0.009685358963906765,0.01843089982867241,-0.01507626660168171,-0.018208997324109077,-0.015063214115798473,0.016459889709949493,0.01498489547520876,-0.018587535247206688,-0.011388781480491161,-0.0368356928229332,0.005064581520855427,0.013522954657673836,0.02981315553188324,-0.011421414092183113,-0.00970493908971548,-0.01193701010197401,0.012452605180442333,0.009182817302644253,0.02866448648273945,-0.011336569674313068,-0.03151005133986473,-0.014201712794601917,-0.0028374057728797197,-0.020910978317260742,4.5379725634120405e-05,0.01276587788015604,0.026314938440918922,0.016146617010235786,0.04067328944802284,-0.0006677448400296271,-0.007061697542667389,-0.01510237343609333,0.00867374800145626,-0.0006134930881671607,-0.01396675780415535,0.037018436938524246,-0.019945053383708,0.013085677288472652,-0.020375803112983704,-0.01990589313209057,-0.025636181235313416,0.03145783767104149,0.022477343678474426,0.024370035156607628,0.007237913552671671,-0.020963190123438835,0.02089792490005493,0.0026595578528940678,-0.0361047238111496,-0.05826879292726517,-0.024278663098812103,-0.003276314353570342,-0.011525838635861874,0.01792183145880699,0.01730833761394024,-0.00432708440348506,-0.011780372820794582,-0.008236471563577652,-0.012805037200450897,0.01974925771355629,0.03665294870734215,-0.007538133300840855,0.016277147457003593,0.030048109591007233,0.025936400517821312,-0.025440385565161705,0.012185017578303814,-0.006643999833613634,0.01201532781124115,0.021158985793590546,0.015128479339182377,-0.017934883013367653,0.05017590522766113,-0.027306970208883286,-0.05064581334590912,-0.006696212105453014,0.005782498978078365,-0.00979631021618843,0.010768761858344078,0.007309705018997192,-0.035086583346128464,-0.005945662036538124,-0.007981937378644943,0.02425255812704563,-0.001894323155283928,-0.0016112352022901177,0.011839112266898155,0.012067540548741817,0.009235029108822346,0.012524397112429142,0.053204212337732315,-0.022033540531992912,-0.004079076461493969,-0.009567881934344769,0.053178105503320694,-0.022242389619350433,0.029734836891293526,0.024996580556035042,0.03409455344080925,-0.02983926050364971,0.030700761824846268,-0.06808467954397202,-0.00033652380807325244,0.038114890456199646,0.011571524664759636,0.002998937154188752,0.033807385712862015,0.0090261809527874,0.003739697393029928,0.007074750494211912,0.0187833309173584,-0.009287241846323013,0.030883504077792168,0.007342337630689144,0.006395991891622543,-0.00884996447712183,-0.04495468735694885,-0.0540396049618721,-0.02996979095041752,-0.013026938773691654,-0.050202012062072754,-0.01016832236200571,0.013849280774593353,-0.018574483692646027,-0.007257493212819099,0.006363359279930592,-0.009920313954353333,0.013823174871504307,-0.005527964793145657,0.001806215150281787,0.007237913552671671,0.02665431797504425,-0.011649842374026775,-0.035713132470846176,0.01747802644968033,-0.013353264890611172,-0.008504059165716171,0.025309855118393898,0.027333077043294907,0.016720950603485107,-0.03017864003777504,-0.03263261169195175,-0.002943461760878563,0.04620777815580368,-0.018195943906903267,0.021498365327715874,0.006950746290385723,-0.011068982072174549,0.0020640126895159483,-0.013196628540754318,-0.014697728678584099,-0.017399709671735764,-0.03618304058909416,-0.03592197969555855,0.0049666836857795715]' \n",
      " LIMIT 10\n",
      "Parameters: {}\n",
      "\n",
      "--- EXPLAIN ANALYZE Output: ---\n",
      "Limit  (cost=10000008424.19..10000008424.21 rows=10 width=1362) (actual time=250.484..250.487 rows=10 loops=1)\n",
      "  ->  Sort  (cost=10000008424.19..10000008479.32 rows=22052 width=1362) (actual time=250.482..250.484 rows=10 loops=1)\n",
      "        Sort Key: ((embedding <=> '[-0.031588368,-0.04046444,0.09309432,-0.008797753,0.012674507,-0.038271528,0.09251998,0.03453836,-0.019657886,-0.009678833,0.032893673,0.031875536,-0.01026622,-0.030283064,0.05503164,0.017765194,0.0049993163,0.034198977,-0.040699393,0.07612536,0.043832127,-0.010305379,-0.02327358,0.0058118682,-0.0006110457,-0.0056095463,-0.037279498,0.055188276,0.016655685,0.007890565,-0.050384752,-0.024252558,0.0155722825,0.054039605,0.032945886,-0.007218334,0.02400455,-0.01375791,-0.012791984,0.031483945,-0.019475143,-0.04017727,-0.0023740225,-0.0064090453,-0.016433783,-0.029291034,-0.021850798,0.025910294,0.0033138418,-0.038819756,0.0034623202,-0.02640631,0.026602106,0.010279273,-0.011865218,-0.025727551,-0.024082867,0.013046519,-0.004862259,0.02417424,0.06761477,-0.008706381,-0.035269327,0.040699393,0.014123395,-0.008282157,-0.009502617,-0.026393257,-0.00483289,0.043205578,-0.04453699,-0.013562114,0.025649233,-0.012191544,0.016577367,-0.014332243,-0.0055997563,0.04941883,0.011088562,-0.012635347,-0.0006073745,-0.027124228,-0.0025502387,0.005756393,0.014528039,-0.023808755,-0.053726334,0.02798573,-0.029395457,0.00949609,-0.0026171356,-0.0034133713,0.020297484,0.055501547,0.05983516,0.023678225,-0.0025763446,0.017556345,0.0073292847,0.009411246,0.00452288,-0.031353414,-0.010448962,-0.032475974,0.00891523,-0.019449037,0.03064855,0.004761098,-0.017974043,0.026314938,-0.08881292,-0.029734837,0.015467858,0.013901493,-0.0041867644,-0.0153373275,0.036835693,-0.06249798,-0.007962357,-0.04059497,0.01856143,0.03806268,0.012733245,-0.019971158,0.032475974,-0.024265612,-0.025492597,-0.012915988,0.0038539115,-0.032006066,0.055762608,0.01612051,0.05255156,-0.019396825,-0.021628896,-0.01625104,-0.028220683,0.024826892,-0.052760407,0.019853681,0.0054822788,0.034486145,0.009058814,0.019958105,0.03357243,7.1230876e-05,-0.038349845,-0.016211882,-0.042448502,0.037096754,0.0021260148,-0.018065413,-0.036287464,-0.010351065,0.02670653,-0.016747057,0.016747057,-0.029291034,-0.025362067,-0.013327159,-0.0020020108,-0.0072640195,0.025845028,-0.001842111,0.017765194,0.025897242,0.0119761685,0.018052362,-0.026536841,-0.0122568095,-0.01886165,-0.014110342,0.018026255,-0.007962357,0.066779375,-0.03660074,0.043884337,-0.038140997,-0.019605674,-0.022790616,-0.0063242004,-0.005697654,-0.023769595,-0.00650368,-0.027254758,0.026758742,0.034486145,-0.0054757525,0.009587461,-0.016264094,-0.025479544,-0.033024203,-0.03605251,0.03305031,0.041691426,-0.022647033,-0.0035830608,0.005847764,-0.014541092,-0.009124079,-0.005038475,0.010031265,-0.035217114,-0.0347211,-0.063176736,0.0024131816,-0.012589662,0.0073880237,0.04038612,-0.015924715,-0.0019824312,-0.021380888,-0.045659553,0.08724655,-0.012328601,-0.007864459,0.0024719203,-0.01989284,-0.016485997,-0.037749406,-0.040072847,-0.011845638,0.006709265,0.043832127,0.028090153,0.020441068,0.020910978,0.005257114,0.07304484,-0.0072509665,-0.008282157,0.008438794,-0.067405924,0.042631246,-0.016107457,0.054979425,-0.06510859,-0.026941486,-0.022881988,-0.043623276,-0.022947254,-0.0014309401,-0.035687026,-0.0054757525,-0.00037466318,0.01358822,-0.010559914,-0.028325107,-0.024474459,-0.020924032,-0.020388857,-0.033755176,0.035034373,-0.032893673,-0.009078393,-0.024213398,-0.02923882,-0.026562946,0.022973359,-0.025022687,-0.039864,-0.0021978065,-0.010931925,0.007133489,-0.0040366543,0.01920103,-0.015063214,0.0057988153,-0.019879788,0.019305453,0.039289664,-0.040882137,0.035164904,-0.029056078,0.024030656,0.0076751905,0.00085089536,0.0037038014,-0.021707214,0.0308574,-0.06798026,-0.026079984,-0.018639749,0.03545207,-0.020166954,0.03879365,0.014201713,0.044563096,0.017843513,0.018143732,0.048922814,0.022633981,0.0018127416,0.006258935,0.008960916,-0.015167639,-0.0017523713,0.0154287,-0.03712286,0.0117868995,0.05393518,0.05247324,0.016472943,-0.017373603,-0.023913179,-0.021615842,0.045424595,-0.01163679,0.03704454,0.032084383,-0.03030917,-0.036835693,0.027672455,0.02027138,0.03211049,-0.03785383,-0.04680822,0.025727551,0.019240187,-0.030544125,-0.04440646,-0.050645813,-0.01204796,0.00034794523,-0.009019654,-0.0049895262,0.02910829,-0.02259482,0.021720268,-0.018887756,-0.009313348,-0.0044282456,-0.013105257,0.026458522,0.026589053,-0.038741436,0.021158986,0.009548303,-0.022477344,-0.042970624,-0.058582067,0.013549061,-0.0038049626,-0.0032371553,-0.008099414,-0.009985579,0.012733245,0.0045326697,0.054666154,0.0065624183,-0.011538892,0.02460499,-0.036209147,-0.03579145,0.017034223,0.0018274263,0.038167104,-0.037906043,-0.033154733,0.013320632,0.031823322,0.024122028,0.02002337,0.0063046208,0.029134396,0.04422372,0.009071866,-0.016407678,-0.058999762,-0.0056943907,0.05722455,-0.0062752515,-0.041143198,0.025101006,-0.06448204,0.020845713,0.014240872,0.013601273,0.051846694,0.030883504,0.03506048,0.0055769137,0.025492597,0.021615842,-0.007557713,0.0054888055,0.020336645,-0.029421564,-0.0075381333,-0.047643613,0.016538208,-0.020871818,-0.0021260148,0.006568945,0.020219166,0.019370718,0.009169764,0.007936251,0.013509901,-0.01105593,0.028064046,-0.03151005,-0.03453836,-0.012433025,0.024122028,0.025923347,-0.003971389,-0.016812323,-0.005808605,-0.08526249,0.016394624,-0.066048406,0.055240486,0.011414888,-0.03378128,-0.017164754,-0.0010809553,-0.0009797942,0.01602914,0.03443393,-0.024722468,0.0025257643,-0.053569697,-0.032214914,0.014971843,-0.008960916,-0.010370644,0.020310538,-0.060409494,-0.040072847,-0.009593988,-0.059469674,0.014214766,0.037958253,0.0057498664,0.0023577062,-0.060566127,-0.07393245,0.022973359,0.025192376,0.017099489,0.018208997,0.016538208,0.010279273,-0.05795552,-0.0009822416,-0.076281995,-0.040751606,-0.019083552,0.014880471,-0.033154733,-0.07748287,0.022359867,0.024552777,0.06181922,0.030883504,0.018195944,0.068397954,4.1377134e-05,0.03892418,0.01839174,0.011982695,-0.015063214,0.037749406,0.013203155,-0.0050352123,0.009065339,0.014371403,0.0039909687,-0.027594138,0.031771112,-0.0034590568,-0.037070647,-0.046390522,-0.053987395,-0.026562946,-0.016172722,-0.026471576,-0.02777688,-0.028612275,-0.013000833,0.016668739,-0.018117627,0.031745005,0.04424982,0.102910206,-0.03545207,0.0019106395,-0.033389688,0.043388322,-0.025845028,0.045163535,0.05061971,-0.014815206,-0.0011804848,-0.030570231,0.0056291255,0.01508932,-0.010657812,0.049497146,0.012224177,-0.004408666,-0.030413594,0.042892307,0.026184408,-0.0021048035,-0.014136448,-0.0026138723,0.021929115,0.0049503674,0.004783941,-0.03785383,0.018339528,-0.04041223,0.014071182,0.01946209,-0.03456446,0.012511344,0.0017213703,0.0057139704,-0.013640432,0.0334419,-0.00139586,0.023456322,0.013379371,-0.02117204,-0.01890081,-0.06714486,-0.03443393,-0.0373056,0.004281399,-0.02173332,-0.031327307,-0.053726334,0.0009757151,-0.021981329,0.018052362,-0.029134396,-0.0051265834,-0.032345444,0.0013420162,0.037749406,-0.009574409,-0.010918872,-0.033389688,0.0011470363,-0.021889957,0.016316306,-0.0010458753,-0.029029973,0.024069816,-0.033676855,-0.00602398,0.013124837,0.02113288,0.0054463833,-0.0055312277,-0.0028243526,-0.0432839,-0.022451237,0.025858082,-0.012034908,0.036705162,0.006559155,0.007459815,0.054979425,0.0055671236,-0.04260514,-0.03978568,-0.027072016,-0.013744856,-0.004284662,-0.034642782,-0.014175607,0.00074606313,-0.009502617,-0.011375729,-0.060670555,0.00438256,-0.023051677,0.012485238,-0.018809438,-0.0129682,-0.0053876443,-0.01221765,-0.019644832,0.016303252,0.015011001,0.014945736,-0.05409182,0.03166669,0.007211807,-0.0070029586,-0.003860438,0.01933156,-0.0068985345,0.03318084,0.015063214,-0.00012094462,-0.016694844,0.03453836,0.014528039,-0.003609167,-0.04111709,0.015415646,-0.010161796,-0.0034492672,-0.013314106,-0.032710932,-0.032162704,-0.022412078,0.033259157,-0.02936935,-0.02314305,0.033468008,0.017791301,0.03372907,0.017843513,-0.016525155,-0.021446154,-0.014971843,-0.021498365,-0.03265872,0.020519387,0.03532154,0.0015223114,0.016133564,-0.002292441,-0.002434393,0.02464415,-0.025871135,0.02237292,0.03660074,-0.01672095,-0.049784314,-0.007616452,0.046756007,0.01839174,0.015976926,-0.011799953,0.025897242,0.00049356825,-0.010383697,-0.02314305,0.029552095,0.025440386,0.0065917876,0.031745005,0.04299673,0.0034035814,-0.03237155,0.010599073,0.009509143,-0.009058814,0.038140997,0.026040824,-0.03892418,-0.030805185,0.00829521,0.012485238,-0.015598388,0.013705697,-0.0062524085,0.005381118,-0.025022687,0.002902671,-0.04054276,0.01461941,-0.041508682,-0.021615842,-0.044093188,0.039759576,-0.0015451543,0.012504817,0.022164071,0.0011731425,0.020819606,-0.028533956,-0.009783257,0.030413594,0.036966223,0.01330758,-0.032554295,0.013157469,0.020950137,0.00019712138,0.040699393,0.009613568,-0.013320632,-0.017451921,0.03090961,-0.052525453,0.030831292,0.004597935,0.017504133,0.008641115,0.01491963,-0.016525155,0.02683706,-0.0184309,-0.009887681,-0.0034916895,0.054196242,-0.013731804,-0.012733245,-0.0033937916,-0.0274375,-0.023260526,0.029134396,0.0057988153,-0.016773162,-0.034277298,-0.039942317,-0.034486145,-0.034616675,-0.021720268,0.03853259,-9.432864e-06,0.0317189,-0.020950137,-0.023860967,0.018678907,-0.0029124606,0.02730697,0.011852165,0.017177807,0.025818923,-0.06683159,-0.021263411,0.0031474156,0.022320706,0.011088562,0.033494115,0.044824157,0.01719086,0.008112468,-0.041743636,-0.011584577,-0.014841313,-0.010103057,0.012746299,0.009182817,-0.021341728,0.044824157,-0.000114826005,-0.03098793,0.03550428,0.046077248,0.03965515,0.003847385,-0.009633147,0.010553387,-0.004056234,0.0036613792,-0.0036287466,-0.038036574,-0.032867566,-0.006768004,-0.021785533,0.039890107,0.013562114,0.028951654,-0.030022003,-0.00859543,0.0019987475,0.026132196,0.00081989437,-0.007120436,-0.0038049626,0.016472943,0.0032192073,-0.01749108,0.020754341,-0.00031572054,0.028481744,0.0060272436,-0.06202807,0.006180617,-0.040229484,0.03166669,-0.027228652,-0.042944517,-0.005074371,0.012106699,-0.0041280254,0.016968958,0.030544125,0.029943686,-0.030361382,-0.028403426,-0.012243756,0.0035308488,-0.034642782,0.033885706,0.02764635,0.0042357133,-0.019814523,-0.06035728,0.0132684205,-0.036757376,-0.0043140315,0.015154585,0.027541924,0.01253745,0.007955831,0.022164071,0.015063214,-0.0024572357,-0.004881839,-0.024696361,0.010527281,0.0020819607,-0.057015702,-0.0043205577,-0.009822416,-0.01375791,0.02764635,-0.018718066,-0.013614326,0.020454122,0.060566127,-0.023900125,0.003733171,0.006072929,-0.0051298467,-0.01920103,0.009548303,-0.019683992,-0.0052407975,-0.039159134,0.022660086,-0.011395308,0.032162704,-0.032214914,0.0039289664,0.0062621986,-0.015206797,-0.02259482,0.0032436817,0.009652726,0.017504133,-0.017073384,0.016264094,-0.009404719,0.023730436,-0.05255156,0.036574632,-0.0013322264,0.010292326,-0.009104499,0.0017964253,0.039968424,0.06375107,-0.01796099,-0.007433709,0.0056943907,0.025048794,0.0041312887,-0.01193701,-0.011773847,-0.0019628517,-0.030335277,0.017203914,-0.0059521887,-0.0057629193,-0.06176701,-0.0080276225,-0.0040138112,0.017974043,0.010990664,-0.022633981,-0.014841313,-0.01538954,-0.03064855,0.019109657,0.0016503944,-0.04915777,-0.018078467,0.0029091975,-0.016551262,-0.016211882,0.018835545,-0.009032707,-0.0020999087,-0.025610074,0.02246429,0.04132594,-0.003684222,-0.007368444,-0.011042876,0.035765342,0.01180648,0.02109372,-0.050436966,-0.009176291,-0.0063862023,0.00030103585,0.024148133,0.009469984,0.01869196,-0.00081214414,0.00962662,0.043440536,0.03438172,-0.058790915,-0.026119143,-0.007890565,-0.0012310653,-0.0037038014,-0.012942094,0.015885556,-0.015728919,0.03532154,-0.036548525,0.021681108,0.019631779,-0.011878271,0.03678348,0.0077274027,-0.028481744,-0.03550428,-0.030491913,-0.017177807,-0.005769446,-0.0016707898,-0.01972315,-0.02362601,0.025427332,0.031222884,-0.012126279,0.020845713,-0.0013216208,0.016355466,0.042161334,0.033807386,-0.03767109,-0.0028374058,0.018483112,0.034773313,0.017373603,0.008269104,-0.009372086,-0.0024327612,-0.0013844386,0.049653783,0.015911661,0.0373056,0.03151005,-0.047513083,-0.006059876,-0.00665379,0.003723381,-0.006559155,-0.033389688,-0.009104499,0.023717383,-0.0147891,-0.031901643,0.0056291255,-0.012726719,0.009339454,-0.0113169905,0.001886165,0.0030103584,-0.0039289664,0.019749258,-0.031014035,0.020924032,0.014893524,-0.041769743,-0.026040824,-0.013692644,0.04043833,0.0056650215,-0.006082719,-0.016224936,-0.018496165,0.0063503063,0.015363434,-0.007831827,0.024944369,0.04960157,-0.010037792,0.020414962,-0.0063731493,0.019971158,0.03184943,-0.0295782,0.010853607,0.021955222,-0.003560218,-0.028716698,-0.004026864,-0.0036711688,0.008843438,0.031875536,-0.031405628,-0.011845638,0.0016454995,0.027855199,0.012902935,0.020101689,0.003896334,-0.010742656,0.005707444,0.013562114,0.016355466,-0.021746373,0.027124228,-0.012341654,0.017882671,-0.014997949,0.044432566,0.025009634,-0.002452341,-0.014371403,-0.008497532,-0.036156934,-0.019958105,0.034146767,-0.00438256,-0.021955222,0.011173407,0.007518554,-0.030152533,0.016707897,0.02683706,-0.02520543,0.020689076,0.03605251,0.0052407975,-0.0015345486,-0.026223568,-0.009176291,0.022020487,0.018013202,0.015050161,0.044093188,0.008928283,0.009822416,-0.012661453,-0.016107457,0.020388857,0.015037108,-0.0432839,0.011460573,-0.0033742122,-0.0051070037,-0.0027509292,0.01706033,-0.012171964,-0.010129163,0.022425132,0.016942853,0.03077908,-0.006846322,-0.020336645,-0.0032893673,-0.018574484,-0.008203839,0.0075838193,-0.0012734877,-0.013379371,0.0001758082,0.031901643,0.033520218,0.0023201788,0.023260526,0.02062381,-0.022986412,0.008112468,0.0010801394,0.032945886,0.02751582,-0.034407828,-0.011597631,-0.007642558,-0.0071530687,-0.020088635,0.0017996886,0.010383697,0.01313789,-0.026040824,-0.016485997,0.005266904,-0.019605674,0.022085752,-0.008654169,0.016616527,6.7559704e-05,0.019292401,0.0022630717,0.013392424,-0.005968505,-0.013784016,-0.013927599,-0.022177124,0.0045685656,-0.008282157,-0.010559914,0.0036776955,-0.014045076,-0.00040892742,0.0027819301,-0.014045076,-0.019383771,-0.0054170135,-0.034903843,0.014045076,0.0027639824,0.0018796385,0.0015092583,-0.03725339,0.0022500185,0.05088077,-0.023038624,-0.0120218545,-0.030570231,-0.02340411,-0.020584652,-0.006709265,-0.021054562,-0.0047284653,-0.034329507,-0.0012751194,0.015076267,-0.026249673,0.014475827,0.0035765343,-0.014580252,-0.020049477,-0.021367835,-0.010390224,-0.028768912,-0.024944369,-0.011042876,0.038167104,0.0021390677,-0.014567198,0.004056234,-0.0076490846,0.011173407,0.0024099185,-0.007792668,0.022855882,0.033885706,0.006630947,0.07283599,0.0013681223,-0.027411394,-0.0059880842,0.008882597,-0.0044054026,0.009287242,0.02109372,-0.0029499882,0.055658184,-0.032841463,-0.0171517,0.007636031,-0.004268346,0.0070094853,-0.028351214,-0.025087953,-0.024461407,0.0343034,-0.009065339,0.02036275,0.003645063,-0.004597935,0.022829777,0.010899292,0.0066505265,-0.010279273,-0.018966075,-0.003847385,-0.010775289,-0.03438172,0.016982011,0.028325107,-0.0068528485,0.015663654,-0.01856143,-0.02383486,0.008066782,-0.004046444,0.006356833,-0.008484479,0.018926915,0.0026938221,-0.03545207,-0.032945886,-0.0029483566,0.017386656,0.02477468,-0.023508534,0.0334419,0.03211049,0.008001517,0.02623662,0.02798573,-0.0010360854,-0.01728223,-0.009306821,0.011923957,0.06077498,-0.0011331674,-0.05174227,-0.07649084,-0.024200346,0.045790084,-0.021850798,0.024618043,0.0016373413,0.018887756,-0.022725351,0.0020819607,0.017308338,-0.0006820216,0.0073749702,-0.004506564,0.0030756237,0.015728919,0.059887372,-0.017582452,-0.00043279002,-0.0044282456,-0.0124460785,-0.009868102,-0.009535249,-0.029186608,0.017256126,0.029395457,0.018287316,-0.020937085,0.07643863,-0.0015794184,-0.022020487,-0.031066246,-0.0014872313,0.02648463,-0.009711466,-0.028951654,-0.012204597,0.03064855,0.0037984361,0.009091446,0.004607725,-0.008667222,-0.00061512477,0.029317139,-0.04440646,-0.041743636,0.04411929,-0.002610609,-0.032763142,0.022385972,0.017308338,-0.014005917,0.031457838,-0.01736055,0.015193745,-0.0006950747,-0.027411394,-0.014514986,0.0122111235,0.0124460785,-0.009058814,0.008588904,-0.0156506,-0.026589053,-0.035086583,0.031953853,0.015076267,-0.009698412,0.013405478,0.0013036729,0.026171355,0.014802153,0.0015247589,0.021681108,-0.0055410177,-0.026171355,0.015128479,-0.014410562,-0.0031408889,-0.023639064,-0.021942168,0.024200346,0.009613568,-0.010364118,0.012602715,-0.012818091,-0.0008647642,-0.0051690056,0.04098656,0.015611442,-0.016851481,-0.014684675,-0.013927599,-0.0015598389,0.00041280256,-0.0134315835,-0.027280863,-0.02383486,0.0038375952,-0.024552777,-0.023991497,0.0046795164,0.018509218,-0.0147891,0.011330043,-0.0063013574,0.03038749,0.0015924715,-0.018378688,0.0010230324,0.020349696,-0.0003322408,0.0048524695,-0.0011919062,-0.006781057,-0.01615967,-0.0083800545,0.0014774415,-0.0016724213,-0.0012628821,-0.015454805,-0.004960157,-0.003423161,-0.013979811,0.015533123,0.020806553,0.0017164755,-0.013953705,-0.008719434,0.006236092,0.009509143,-0.039289664,-0.0068006366,0.0040660235,-0.0025176061,0.017451921,0.01869196,-0.014554145,-0.022581767,-0.024709415,-0.011206039,-0.025792817,0.012922514,0.026458522,-0.024944369,-0.026301885,-0.030752974,0.0051624794,0.011982695,0.010214007,0.02019306,-0.010429383,-0.02211186,-0.01726918,0.025296802,-0.0127985105,0.027933516,0.004219397,0.0020183271,-0.025218483,-0.01598998,-0.018757226,0.03151005,-0.0034753731,-0.013098731,-0.013992865,0.008432267,-0.0016503944,-0.01749108,0.02554481,0.0037755934,0.0025208693,0.028351214,-0.009039233,-0.009685359,0.0184309,-0.015076267,-0.018208997,-0.015063214,0.01645989,0.0149848955,-0.018587535,-0.0113887815,-0.036835693,0.0050645815,0.013522955,0.029813156,-0.011421414,-0.009704939,-0.01193701,0.012452605,0.009182817,0.028664486,-0.01133657,-0.03151005,-0.014201713,-0.0028374058,-0.020910978,4.5379726e-05,0.012765878,0.026314938,0.016146617,0.04067329,-0.00066774484,-0.0070616975,-0.015102373,0.008673748,-0.0006134931,-0.013966758,0.037018437,-0.019945053,0.013085677,-0.020375803,-0.019905893,-0.025636181,0.031457838,0.022477344,0.024370035,0.0072379136,-0.02096319,0.020897925,0.0026595579,-0.036104724,-0.058268793,-0.024278663,-0.0032763144,-0.011525839,0.017921831,0.017308338,-0.0043270844,-0.011780373,-0.008236472,-0.012805037,0.019749258,0.03665295,-0.0075381333,0.016277147,0.03004811,0.0259364,-0.025440386,0.012185018,-0.006644,0.012015328,0.021158986,0.015128479,-0.017934883,0.050175905,-0.02730697,-0.050645813,-0.006696212,0.005782499,-0.00979631,0.010768762,0.007309705,-0.035086583,-0.005945662,-0.007981937,0.024252558,-0.0018943232,-0.0016112352,0.011839112,0.012067541,0.009235029,0.012524397,0.053204212,-0.02203354,-0.0040790765,-0.009567882,0.053178106,-0.02224239,0.029734837,0.02499658,0.034094553,-0.02983926,0.030700762,-0.06808468,-0.0003365238,0.03811489,0.011571525,0.0029989372,0.033807386,0.009026181,0.0037396974,0.0070747505,0.01878333,-0.009287242,0.030883504,0.0073423376,0.006395992,-0.0088499645,-0.044954687,-0.054039605,-0.029969791,-0.013026939,-0.050202012,-0.010168322,0.013849281,-0.018574484,-0.007257493,0.0063633593,-0.009920314,0.013823175,-0.005527965,0.0018062152,0.0072379136,0.026654318,-0.011649842,-0.035713132,0.017478026,-0.013353265,-0.008504059,0.025309855,0.027333077,0.01672095,-0.03017864,-0.03263261,-0.0029434618,0.04620778,-0.018195944,0.021498365,0.0069507463,-0.011068982,0.0020640127,-0.013196629,-0.014697729,-0.01739971,-0.03618304,-0.03592198,0.0049666837]'::vector))\n",
      "        Sort Method: top-N heapsort  Memory: 51kB\n",
      "        ->  Seq Scan on chunks  (cost=10000000000.00..10000007947.65 rows=22052 width=1362) (actual time=0.055..242.523 rows=22052 loops=1)\n",
      "Planning Time: 0.211 ms\n",
      "Execution Time: 250.504 ms\n",
      "-------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m query_method = \u001b[33m\"\u001b[39m\u001b[33mcosine_distance\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#results_df_dense = test_pipeline(df_dense, create_db=False, trim_chunks=True, retrieval= \"dense\", dense_comparator=\"cosine_distance\")\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#results_df_dense.to_csv(f\"{RESULT_PATH}dense.csv\")\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#results_df_sparse = test_pipeline(df_sparse, create_db=False, trim_chunks=True, retrieval= \"sparse\")\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#results_df_sparse.to_csv(f\"{RESULT_PATH}sparse.csv\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m results_df_hybrid = \u001b[43mtest_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_hybrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieval\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhybrid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#results_df_hybrid.to_csv(f\"{RESULT_PATH}hybrid.csv\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mtest_pipeline\u001b[39m\u001b[34m(df, create_db, data_path, embed_model, min_tokens, max_tokens, trim_chunks, retrieval, dense_comparator)\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShowing results from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretrieval\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m search:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m test_queries:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m         metrics = \u001b[43mevaluate_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_comparator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m         new_row = {\n\u001b[32m     34\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mretrieval\u001b[39m\u001b[33m\"\u001b[39m: retrieval,\n\u001b[32m     35\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33membedding_model\u001b[39m\u001b[33m\"\u001b[39m: embed_model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMean Reciprocal Rank (MRR)\u001b[39m\u001b[33m\"\u001b[39m: metrics[\u001b[33m\"\u001b[39m\u001b[33mMean Reciprocal Rank (MRR)\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     42\u001b[39m }       \n\u001b[32m     43\u001b[39m         df.loc[\u001b[38;5;28mlen\u001b[39m(df)] = new_row\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mevaluate_search\u001b[39m\u001b[34m(query, retrieval, dense_comparator, n)\u001b[39m\n\u001b[32m     12\u001b[39m     query_results = sparse_search(query)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m retrieval.lower() == \u001b[33m\"\u001b[39m\u001b[33mhybrid\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     query_results = \u001b[43mhybrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_comparator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid retrieval method. Choose \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdense\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33msparse\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhybrid\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36mhybrid_search\u001b[39m\u001b[34m(query, dense_comparator, alpha, n)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# Normalize Dense Scores\u001b[39;00m\n\u001b[32m    136\u001b[39m max_dense_score = \u001b[38;5;28mmax\u001b[39m(item[\u001b[33m\"\u001b[39m\u001b[33msimilarity_score\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m dense_results) \u001b[38;5;28;01mif\u001b[39;00m dense_results \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdense_results\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnorm_dense_score\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msimilarity_score\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_dense_score\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresult_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchunk\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchunk\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchunk\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdense_score\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_dense_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msparse_score\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m}\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "query_method = \"cosine_distance\"\n",
    "#results_df_dense = test_pipeline(df_dense, create_db=False, trim_chunks=True, retrieval= \"dense\", dense_comparator=\"cosine_distance\")\n",
    "#results_df_dense.to_csv(f\"{RESULT_PATH}dense.csv\")\n",
    "#results_df_sparse = test_pipeline(df_sparse, create_db=False, trim_chunks=True, retrieval= \"sparse\")\n",
    "#results_df_sparse.to_csv(f\"{RESULT_PATH}sparse.csv\")\n",
    "results_df_hybrid = test_pipeline(df_hybrid, retrieval='hybrid')\n",
    "#results_df_hybrid.to_csv(f\"{RESULT_PATH}hybrid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrieval</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>query</th>\n",
       "      <th>average_relevance_score</th>\n",
       "      <th>normalised_relevance_score</th>\n",
       "      <th>Spearman Rank Correlation</th>\n",
       "      <th>Mean Reciprocal Rank (MRR)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>setting up development environment?</td>\n",
       "      <td>0.306525</td>\n",
       "      <td>0.306525</td>\n",
       "      <td>0.299248</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>necessary software?</td>\n",
       "      <td>0.150516</td>\n",
       "      <td>0.150516</td>\n",
       "      <td>0.621053</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>Paid Time Off (PTO)</td>\n",
       "      <td>0.558035</td>\n",
       "      <td>0.558035</td>\n",
       "      <td>0.457172</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>How do I request time off</td>\n",
       "      <td>0.252645</td>\n",
       "      <td>0.252645</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>Sick leave</td>\n",
       "      <td>0.529405</td>\n",
       "      <td>0.529405</td>\n",
       "      <td>0.296569</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>meal expanses limit</td>\n",
       "      <td>0.019019</td>\n",
       "      <td>0.019019</td>\n",
       "      <td>0.290909</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>set up meeting</td>\n",
       "      <td>0.311485</td>\n",
       "      <td>0.311485</td>\n",
       "      <td>0.384211</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>gitlabs coding standards</td>\n",
       "      <td>0.613979</td>\n",
       "      <td>0.613979</td>\n",
       "      <td>0.128070</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>setting up development environment?</td>\n",
       "      <td>0.306525</td>\n",
       "      <td>0.306525</td>\n",
       "      <td>0.299248</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>necessary software?</td>\n",
       "      <td>0.150516</td>\n",
       "      <td>0.150516</td>\n",
       "      <td>0.621053</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>Paid Time Off (PTO)</td>\n",
       "      <td>0.558035</td>\n",
       "      <td>0.558035</td>\n",
       "      <td>0.457172</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>How do I request time off</td>\n",
       "      <td>0.252645</td>\n",
       "      <td>0.252645</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>Sick leave</td>\n",
       "      <td>0.529405</td>\n",
       "      <td>0.529405</td>\n",
       "      <td>0.296569</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>meal expanses limit</td>\n",
       "      <td>0.019019</td>\n",
       "      <td>0.019019</td>\n",
       "      <td>0.290909</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>set up meeting</td>\n",
       "      <td>0.311485</td>\n",
       "      <td>0.311485</td>\n",
       "      <td>0.384211</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>gitlabs coding standards</td>\n",
       "      <td>0.613979</td>\n",
       "      <td>0.613979</td>\n",
       "      <td>0.128070</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>setting up development environment?</td>\n",
       "      <td>0.306521</td>\n",
       "      <td>0.306521</td>\n",
       "      <td>0.299248</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>setting up development environment?</td>\n",
       "      <td>0.306525</td>\n",
       "      <td>0.306525</td>\n",
       "      <td>0.299248</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>necessary software?</td>\n",
       "      <td>0.150516</td>\n",
       "      <td>0.150516</td>\n",
       "      <td>0.621053</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>Paid Time Off (PTO)</td>\n",
       "      <td>0.558035</td>\n",
       "      <td>0.558035</td>\n",
       "      <td>0.457172</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>How do I request time off</td>\n",
       "      <td>0.252645</td>\n",
       "      <td>0.252645</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>Sick leave</td>\n",
       "      <td>0.529405</td>\n",
       "      <td>0.529405</td>\n",
       "      <td>0.296569</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>meal expanses limit</td>\n",
       "      <td>0.019019</td>\n",
       "      <td>0.019019</td>\n",
       "      <td>0.290909</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>set up meeting</td>\n",
       "      <td>0.311485</td>\n",
       "      <td>0.311485</td>\n",
       "      <td>0.384211</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>gitlabs coding standards</td>\n",
       "      <td>0.613979</td>\n",
       "      <td>0.613979</td>\n",
       "      <td>0.128070</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   retrieval         embedding_model  chunk_size  \\\n",
       "0     hybrid  text-embedding-3-small  200 - 1000   \n",
       "1     hybrid  text-embedding-3-small  200 - 1000   \n",
       "2     hybrid  text-embedding-3-small  200 - 1000   \n",
       "3     hybrid  text-embedding-3-small  200 - 1000   \n",
       "4     hybrid  text-embedding-3-small  200 - 1000   \n",
       "5     hybrid  text-embedding-3-small  200 - 1000   \n",
       "6     hybrid  text-embedding-3-small  200 - 1000   \n",
       "7     hybrid  text-embedding-3-small  200 - 1000   \n",
       "8     hybrid  text-embedding-3-small  200 - 1000   \n",
       "9     hybrid  text-embedding-3-small  200 - 1000   \n",
       "10    hybrid  text-embedding-3-small  200 - 1000   \n",
       "11    hybrid  text-embedding-3-small  200 - 1000   \n",
       "12    hybrid  text-embedding-3-small  200 - 1000   \n",
       "13    hybrid  text-embedding-3-small  200 - 1000   \n",
       "14    hybrid  text-embedding-3-small  200 - 1000   \n",
       "15    hybrid  text-embedding-3-small  200 - 1000   \n",
       "16    hybrid  text-embedding-3-small  200 - 1000   \n",
       "17    hybrid  text-embedding-3-small  200 - 1000   \n",
       "18    hybrid  text-embedding-3-small  200 - 1000   \n",
       "19    hybrid  text-embedding-3-small  200 - 1000   \n",
       "20    hybrid  text-embedding-3-small  200 - 1000   \n",
       "21    hybrid  text-embedding-3-small  200 - 1000   \n",
       "22    hybrid  text-embedding-3-small  200 - 1000   \n",
       "23    hybrid  text-embedding-3-small  200 - 1000   \n",
       "24    hybrid  text-embedding-3-small  200 - 1000   \n",
       "\n",
       "                                  query  average_relevance_score  \\\n",
       "0   setting up development environment?                 0.306525   \n",
       "1                   necessary software?                 0.150516   \n",
       "2                   Paid Time Off (PTO)                 0.558035   \n",
       "3             How do I request time off                 0.252645   \n",
       "4                            Sick leave                 0.529405   \n",
       "5                   meal expanses limit                 0.019019   \n",
       "6                        set up meeting                 0.311485   \n",
       "7              gitlabs coding standards                 0.613979   \n",
       "8   setting up development environment?                 0.306525   \n",
       "9                   necessary software?                 0.150516   \n",
       "10                  Paid Time Off (PTO)                 0.558035   \n",
       "11            How do I request time off                 0.252645   \n",
       "12                           Sick leave                 0.529405   \n",
       "13                  meal expanses limit                 0.019019   \n",
       "14                       set up meeting                 0.311485   \n",
       "15             gitlabs coding standards                 0.613979   \n",
       "16  setting up development environment?                 0.306521   \n",
       "17  setting up development environment?                 0.306525   \n",
       "18                  necessary software?                 0.150516   \n",
       "19                  Paid Time Off (PTO)                 0.558035   \n",
       "20            How do I request time off                 0.252645   \n",
       "21                           Sick leave                 0.529405   \n",
       "22                  meal expanses limit                 0.019019   \n",
       "23                       set up meeting                 0.311485   \n",
       "24             gitlabs coding standards                 0.613979   \n",
       "\n",
       "    normalised_relevance_score  Spearman Rank Correlation  \\\n",
       "0                     0.306525                   0.299248   \n",
       "1                     0.150516                   0.621053   \n",
       "2                     0.558035                   0.457172   \n",
       "3                     0.252645                   0.605263   \n",
       "4                     0.529405                   0.296569   \n",
       "5                     0.019019                   0.290909   \n",
       "6                     0.311485                   0.384211   \n",
       "7                     0.613979                   0.128070   \n",
       "8                     0.306525                   0.299248   \n",
       "9                     0.150516                   0.621053   \n",
       "10                    0.558035                   0.457172   \n",
       "11                    0.252645                   0.605263   \n",
       "12                    0.529405                   0.296569   \n",
       "13                    0.019019                   0.290909   \n",
       "14                    0.311485                   0.384211   \n",
       "15                    0.613979                   0.128070   \n",
       "16                    0.306521                   0.299248   \n",
       "17                    0.306525                   0.299248   \n",
       "18                    0.150516                   0.621053   \n",
       "19                    0.558035                   0.457172   \n",
       "20                    0.252645                   0.605263   \n",
       "21                    0.529405                   0.296569   \n",
       "22                    0.019019                   0.290909   \n",
       "23                    0.311485                   0.384211   \n",
       "24                    0.613979                   0.128070   \n",
       "\n",
       "    Mean Reciprocal Rank (MRR)  \n",
       "0                     0.083333  \n",
       "1                     0.500000  \n",
       "2                     1.000000  \n",
       "3                     0.250000  \n",
       "4                     0.166667  \n",
       "5                     0.500000  \n",
       "6                     1.000000  \n",
       "7                     1.000000  \n",
       "8                     0.083333  \n",
       "9                     0.500000  \n",
       "10                    1.000000  \n",
       "11                    0.250000  \n",
       "12                    0.166667  \n",
       "13                    0.500000  \n",
       "14                    1.000000  \n",
       "15                    1.000000  \n",
       "16                    0.083333  \n",
       "17                    0.083333  \n",
       "18                    0.500000  \n",
       "19                    1.000000  \n",
       "20                    0.250000  \n",
       "21                    0.166667  \n",
       "22                    0.500000  \n",
       "23                    1.000000  \n",
       "24                    1.000000  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing results from hybrid search:\n",
      "Query: setting up development environment?\n",
      "Chunk 8110 - Relevance Score: 0.40797067 - dense_score: 1.0 - sparse_score: 0 - hybrid_score: 0.5\n",
      "Chunk 3042 - Relevance Score: 0.7570233 - dense_score: 0 - sparse_score: 1.0 - hybrid_score: 0.5\n",
      "Chunk 3324 - Relevance Score: 0.24588585 - dense_score: 0.9964826952650555 - sparse_score: 0 - hybrid_score: 0.49824134763252775\n",
      "Chunk 3699 - Relevance Score: 0.30703637 - dense_score: 0.9848164048052285 - sparse_score: 0 - hybrid_score: 0.49240820240261424\n",
      "Chunk 19473 - Relevance Score: 0.11931599 - dense_score: 0.9589768148543789 - sparse_score: 0 - hybrid_score: 0.47948840742718946\n",
      "Chunk 14662 - Relevance Score: 0.3258213 - dense_score: 0.9337536748549158 - sparse_score: 0 - hybrid_score: 0.4668768374274579\n",
      "Chunk 19607 - Relevance Score: 0.18866788 - dense_score: 0.9229720641365494 - sparse_score: 0 - hybrid_score: 0.4614860320682747\n",
      "Chunk 18369 - Relevance Score: 0.22164577 - dense_score: 0.9199170150017324 - sparse_score: 0 - hybrid_score: 0.4599585075008662\n",
      "Chunk 6365 - Relevance Score: 0.13252604 - dense_score: 0.9082556828087506 - sparse_score: 0 - hybrid_score: 0.4541278414043753\n",
      "Chunk 6619 - Relevance Score: 0.3640543 - dense_score: 0.9062602516106915 - sparse_score: 0 - hybrid_score: 0.45313012580534573\n",
      "Chunk 15969 - Relevance Score: 0.35736617 - dense_score: 0.9058719493988029 - sparse_score: 0 - hybrid_score: 0.45293597469940144\n",
      "Chunk 3073 - Relevance Score: 0.80625874 - dense_score: 0 - sparse_score: 0.5993191889245819 - hybrid_score: 0.29965959446229096\n",
      "Chunk 11679 - Relevance Score: 0.49626526 - dense_score: 0 - sparse_score: 0.4468085182435492 - hybrid_score: 0.2234042591217746\n",
      "Chunk 7106 - Relevance Score: 0.42391166 - dense_score: 0 - sparse_score: 0.2234042591217746 - hybrid_score: 0.1117021295608873\n",
      "Chunk 5996 - Relevance Score: 0.11815108 - dense_score: 0 - sparse_score: 0.16340425554730648 - hybrid_score: 0.08170212777365324\n",
      "Chunk 8243 - Relevance Score: 0.1476169 - dense_score: 0 - sparse_score: 0.12127660163875066 - hybrid_score: 0.06063830081937533\n",
      "Chunk 10699 - Relevance Score: 0.050410144 - dense_score: 0 - sparse_score: 0.09989197063645906 - hybrid_score: 0.04994598531822953\n",
      "Chunk 19621 - Relevance Score: 0.12945801 - dense_score: 0 - sparse_score: 0.09931392552023703 - hybrid_score: 0.049656962760118516\n",
      "Chunk 8398 - Relevance Score: 0.30969188 - dense_score: 0 - sparse_score: 0.09929078083899201 - hybrid_score: 0.049645390419496005\n",
      "Chunk 11955 - Relevance Score: 0.22134264 - dense_score: 0 - sparse_score: 0.09929078083899201 - hybrid_score: 0.049645390419496005\n",
      "\n",
      "üîç Evaluation Metrics:\n",
      "Spearman Rank Correlation: 0.2992\n",
      "Mean Reciprocal Rank (MRR): 0.0833\n",
      "Average Relevance Score: 0.3065\n",
      "Average Normalised Score: 0.3065\n"
     ]
    }
   ],
   "source": [
    "results_df_hybrid = test_pipeline(df_hybrid, retrieval='hybrid', dense_comparator=\"cosine_distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrieval</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>query</th>\n",
       "      <th>average_relevance_score</th>\n",
       "      <th>normalised_relevance_score</th>\n",
       "      <th>Spearman Rank Correlation</th>\n",
       "      <th>Mean Reciprocal Rank (MRR)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>setting up development environment?</td>\n",
       "      <td>0.306525</td>\n",
       "      <td>0.306525</td>\n",
       "      <td>0.299248</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>necessary software?</td>\n",
       "      <td>0.150516</td>\n",
       "      <td>0.150516</td>\n",
       "      <td>0.621053</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>Paid Time Off (PTO)</td>\n",
       "      <td>0.558035</td>\n",
       "      <td>0.558035</td>\n",
       "      <td>0.457172</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>How do I request time off</td>\n",
       "      <td>0.252645</td>\n",
       "      <td>0.252645</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>Sick leave</td>\n",
       "      <td>0.529405</td>\n",
       "      <td>0.529405</td>\n",
       "      <td>0.296569</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>meal expanses limit</td>\n",
       "      <td>0.019019</td>\n",
       "      <td>0.019019</td>\n",
       "      <td>0.290909</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>set up meeting</td>\n",
       "      <td>0.311485</td>\n",
       "      <td>0.311485</td>\n",
       "      <td>0.384211</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>gitlabs coding standards</td>\n",
       "      <td>0.613979</td>\n",
       "      <td>0.613979</td>\n",
       "      <td>0.128070</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>setting up development environment?</td>\n",
       "      <td>0.306525</td>\n",
       "      <td>0.306525</td>\n",
       "      <td>0.299248</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>necessary software?</td>\n",
       "      <td>0.150516</td>\n",
       "      <td>0.150516</td>\n",
       "      <td>0.621053</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>Paid Time Off (PTO)</td>\n",
       "      <td>0.558035</td>\n",
       "      <td>0.558035</td>\n",
       "      <td>0.457172</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>How do I request time off</td>\n",
       "      <td>0.252645</td>\n",
       "      <td>0.252645</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>Sick leave</td>\n",
       "      <td>0.529405</td>\n",
       "      <td>0.529405</td>\n",
       "      <td>0.296569</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>meal expanses limit</td>\n",
       "      <td>0.019019</td>\n",
       "      <td>0.019019</td>\n",
       "      <td>0.290909</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>set up meeting</td>\n",
       "      <td>0.311485</td>\n",
       "      <td>0.311485</td>\n",
       "      <td>0.384211</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>200 - 1000</td>\n",
       "      <td>gitlabs coding standards</td>\n",
       "      <td>0.613979</td>\n",
       "      <td>0.613979</td>\n",
       "      <td>0.128070</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   retrieval         embedding_model  chunk_size  \\\n",
       "0     hybrid  text-embedding-3-small  200 - 1000   \n",
       "1     hybrid  text-embedding-3-small  200 - 1000   \n",
       "2     hybrid  text-embedding-3-small  200 - 1000   \n",
       "3     hybrid  text-embedding-3-small  200 - 1000   \n",
       "4     hybrid  text-embedding-3-small  200 - 1000   \n",
       "5     hybrid  text-embedding-3-small  200 - 1000   \n",
       "6     hybrid  text-embedding-3-small  200 - 1000   \n",
       "7     hybrid  text-embedding-3-small  200 - 1000   \n",
       "8     hybrid  text-embedding-3-small  200 - 1000   \n",
       "9     hybrid  text-embedding-3-small  200 - 1000   \n",
       "10    hybrid  text-embedding-3-small  200 - 1000   \n",
       "11    hybrid  text-embedding-3-small  200 - 1000   \n",
       "12    hybrid  text-embedding-3-small  200 - 1000   \n",
       "13    hybrid  text-embedding-3-small  200 - 1000   \n",
       "14    hybrid  text-embedding-3-small  200 - 1000   \n",
       "15    hybrid  text-embedding-3-small  200 - 1000   \n",
       "\n",
       "                                  query  average_relevance_score  \\\n",
       "0   setting up development environment?                 0.306525   \n",
       "1                   necessary software?                 0.150516   \n",
       "2                   Paid Time Off (PTO)                 0.558035   \n",
       "3             How do I request time off                 0.252645   \n",
       "4                            Sick leave                 0.529405   \n",
       "5                   meal expanses limit                 0.019019   \n",
       "6                        set up meeting                 0.311485   \n",
       "7              gitlabs coding standards                 0.613979   \n",
       "8   setting up development environment?                 0.306525   \n",
       "9                   necessary software?                 0.150516   \n",
       "10                  Paid Time Off (PTO)                 0.558035   \n",
       "11            How do I request time off                 0.252645   \n",
       "12                           Sick leave                 0.529405   \n",
       "13                  meal expanses limit                 0.019019   \n",
       "14                       set up meeting                 0.311485   \n",
       "15             gitlabs coding standards                 0.613979   \n",
       "\n",
       "    normalised_relevance_score  Spearman Rank Correlation  \\\n",
       "0                     0.306525                   0.299248   \n",
       "1                     0.150516                   0.621053   \n",
       "2                     0.558035                   0.457172   \n",
       "3                     0.252645                   0.605263   \n",
       "4                     0.529405                   0.296569   \n",
       "5                     0.019019                   0.290909   \n",
       "6                     0.311485                   0.384211   \n",
       "7                     0.613979                   0.128070   \n",
       "8                     0.306525                   0.299248   \n",
       "9                     0.150516                   0.621053   \n",
       "10                    0.558035                   0.457172   \n",
       "11                    0.252645                   0.605263   \n",
       "12                    0.529405                   0.296569   \n",
       "13                    0.019019                   0.290909   \n",
       "14                    0.311485                   0.384211   \n",
       "15                    0.613979                   0.128070   \n",
       "\n",
       "    Mean Reciprocal Rank (MRR)  \n",
       "0                     0.083333  \n",
       "1                     0.500000  \n",
       "2                     1.000000  \n",
       "3                     0.250000  \n",
       "4                     0.166667  \n",
       "5                     0.500000  \n",
       "6                     1.000000  \n",
       "7                     1.000000  \n",
       "8                     0.083333  \n",
       "9                     0.500000  \n",
       "10                    1.000000  \n",
       "11                    0.250000  \n",
       "12                    0.166667  \n",
       "13                    0.500000  \n",
       "14                    1.000000  \n",
       "15                    1.000000  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with Golden Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating question 1: What is GitLab's approach to compensation?...\n",
      "Evaluating question 2: How does GitLab handle remote work?...\n",
      "Evaluating question 3: What are GitLab's core values?...\n",
      "Evaluating question 4: What is GitLab's approach to feature flags?...\n",
      "Evaluating question 5: How does GitLab handle database changes and migrat...\n",
      "Evaluating question 6: What is GitLab's Merge Request workflow?...\n",
      "Evaluating question 7: How does GitLab's stock option program work?...\n",
      "Evaluating question 8: What health benefits does GitLab offer to employee...\n",
      "Evaluating question 9: What is GitLab's parental leave policy?...\n",
      "Evaluating question 10: How does GitLab handle incident management?...\n",
      "Evaluating question 11: What is GitLab's approach to product development?...\n",
      "Evaluating question 12: How does GitLab handle security vulnerabilities?...\n",
      "Evaluating question 13: What is GitLab's mission and vision?...\n",
      "Evaluating question 14: How does GitLab approach pricing for its products?...\n",
      "Evaluating question 15: What is GitLab's approach to OKRs (Objectives and ...\n",
      "Evaluating question 16: How does GitLab's customer success team operate?...\n",
      "Evaluating question 17: What is GitLab's approach to the open source commu...\n",
      "Evaluating question 18: How does GitLab handle customer feedback?...\n",
      "Evaluating question 19: What is GitLab's approach to GDPR compliance?...\n",
      "Evaluating question 20: What is GitLab's code of business conduct and ethi...\n",
      "Evaluating question 21: I'm interviewing for a role at GitLab. What are th...\n",
      "Evaluating question 22: My team wants to improve our workflow efficiency. ...\n",
      "Evaluating question 23: I need to take a few days off next month. What's t...\n",
      "Evaluating question 24: How does GitLab actively promote Diversity, Inclus...\n",
      "Evaluating question 25: How often are performance reviews conducted at Git...\n",
      "Evaluating question 26: I need a license for Figma for my design work. Wha...\n",
      "Evaluating question 27: My CI/CD pipeline failed with a 'script timeout' e...\n",
      "Evaluating question 28: I'm based in the Netherlands and expecting a child...\n",
      "Evaluating question 29: I think I've discovered a security vulnerability i...\n",
      "Evaluating question 30: As a remote employee, I need a better ergonomic ch...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 307\u001b[39m\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[38;5;66;03m# Run the evaluation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m evaluation_results = \u001b[43mevaluate_rag_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGOLDEN_TEST_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_rag_search\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmy_llm_answer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 272\u001b[39m, in \u001b[36mevaluate_rag_system\u001b[39m\u001b[34m(golden_test_set_path, rag_function, llm_function)\u001b[39m\n\u001b[32m    265\u001b[39m     rag_results.append({\n\u001b[32m    266\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquestion_id\u001b[39m\u001b[33m\"\u001b[39m: question_id,\n\u001b[32m    267\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mretrieved_sources\u001b[39m\u001b[33m\"\u001b[39m: retrieved_sources,\n\u001b[32m    268\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgenerated_answer\u001b[39m\u001b[33m\"\u001b[39m: generated_answer\n\u001b[32m    269\u001b[39m     })\n\u001b[32m    271\u001b[39m \u001b[38;5;66;03m# Run the evaluation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m evaluation_results = \u001b[43mevaluator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrag_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;66;03m# Save the results\u001b[39;00m\n\u001b[32m    275\u001b[39m evaluator.save_results(evaluation_results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 171\u001b[39m, in \u001b[36mRAGEvaluator.run_evaluation\u001b[39m\u001b[34m(self, rag_results)\u001b[39m\n\u001b[32m    168\u001b[39m     retrieval_eval = \u001b[38;5;28mself\u001b[39m.evaluate_retrieval(question_id, retrieved_sources)\n\u001b[32m    169\u001b[39m     retrieval_results.append(retrieval_eval)\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     answer_eval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_answer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m     answer_results.append(answer_eval)\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# Calculate aggregate metrics\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 129\u001b[39m, in \u001b[36mRAGEvaluator.evaluate_answer\u001b[39m\u001b[34m(self, question_id, generated_answer)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# Calculate semantic similarity\u001b[39;00m\n\u001b[32m    128\u001b[39m golden_embedding = \u001b[38;5;28mself\u001b[39m.encoder.encode([golden_answer])[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m generated_embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgenerated_answer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    130\u001b[39m semantic_similarity = cosine_similarity([golden_embedding], [generated_embedding])[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Simple lexical overlap (Jaccard similarity)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Saxion\\Jaar4\\Afstudeerstage\\chatbot_git\\ChatbotExperiment\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:653\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m start_index \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), batch_size, desc=\u001b[33m\"\u001b[39m\u001b[33mBatches\u001b[39m\u001b[33m\"\u001b[39m, disable=\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[32m    652\u001b[39m     sentences_batch = sentences_sorted[start_index : start_index + batch_size]\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    654\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    655\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Saxion\\Jaar4\\Afstudeerstage\\chatbot_git\\ChatbotExperiment\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1118\u001b[39m, in \u001b[36mSentenceTransformer.tokenize\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m   1107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[32m   1108\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1109\u001b[39m \u001b[33;03m    Tokenizes the texts.\u001b[39;00m\n\u001b[32m   1110\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1116\u001b[39m \u001b[33;03m            \"attention_mask\", and \"token_type_ids\".\u001b[39;00m\n\u001b[32m   1117\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_first_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Saxion\\Jaar4\\Afstudeerstage\\chatbot_git\\ChatbotExperiment\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:494\u001b[39m, in \u001b[36mTransformer.tokenize\u001b[39m\u001b[34m(self, texts, padding)\u001b[39m\n\u001b[32m    492\u001b[39m batch1, batch2 = [], []\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text_tuple \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     batch1.append(\u001b[43mtext_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m    495\u001b[39m     batch2.append(text_tuple[\u001b[32m1\u001b[39m])\n\u001b[32m    496\u001b[39m to_tokenize = [batch1, batch2]\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "import os\n",
    "\n",
    "class RAGEvaluator:\n",
    "    def __init__(self, golden_test_set_path: str, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the RAG evaluation system\n",
    "        \n",
    "        Args:\n",
    "            golden_test_set_path: Path to the CSV file containing the golden test set\n",
    "            model_name: The sentence transformer model to use for semantic similarity\n",
    "        \"\"\"\n",
    "        self.golden_test_set_path = golden_test_set_path\n",
    "        self.golden_df = self._load_golden_test_set()\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "        \n",
    "    def _load_golden_test_set(self) -> pd.DataFrame:\n",
    "        \"\"\"Load the golden test set from CSV\"\"\"\n",
    "        try:\n",
    "            # Try reading with common encodings\n",
    "            for encoding in ['utf-8', 'latin-1', 'ISO-8859-1']:\n",
    "                try:\n",
    "                    df = pd.read_csv(self.golden_test_set_path, encoding=encoding, delimiter=';')\n",
    "                    return df\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "                    \n",
    "            # If all encodings fail, try excel format\n",
    "            return pd.read_excel(self.golden_test_set_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading golden test set: {e}\")\n",
    "            # Create empty DataFrame with expected columns if loading fails\n",
    "            return pd.DataFrame(columns=[\"ID\", \"Question\", \"Answer\", \"Source File\", \"Relevant Section\"])\n",
    "    \n",
    "    def _normalize_path(self, path: str) -> str:\n",
    "        \"\"\"Normalize file paths for comparison\"\"\"\n",
    "        # Remove leading/trailing whitespace, normalize slashes\n",
    "        path = path.strip()\n",
    "        path = path.replace('\\\\', '/')\n",
    "        \n",
    "        # Remove leading \"/\" if present\n",
    "        if path.startswith('/'):\n",
    "            path = path[1:]\n",
    "            \n",
    "        # Handle cases where the path might be in a different format\n",
    "        path = re.sub(r'^.*?content/', 'content/', path)\n",
    "        \n",
    "        return path.lower()\n",
    "    \n",
    "    def evaluate_retrieval(self, question_id: int, retrieved_sources: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate if the retrieved sources match the expected sources for a question\n",
    "        \n",
    "        Args:\n",
    "            question_id: The ID of the question to evaluate\n",
    "            retrieved_sources: List of source paths retrieved by the RAG system\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        # Get the golden source(s) for this question\n",
    "        question_row = self.golden_df[self.golden_df[\"ID\"] == question_id]\n",
    "        if question_row.empty:\n",
    "            return {\"error\": f\"Question ID {question_id} not found in golden test set\"}\n",
    "        \n",
    "        golden_sources_raw = question_row[\"Source File\"].iloc[0]\n",
    "        golden_sources = [self._normalize_path(src.strip()) for src in golden_sources_raw.split(',')]\n",
    "        \n",
    "        # Normalize retrieved sources\n",
    "        normalized_retrieved = [self._normalize_path(src) for src in retrieved_sources]\n",
    "        \n",
    "        # Check exact matches\n",
    "        exact_matches = set(golden_sources).intersection(set(normalized_retrieved))\n",
    "        \n",
    "        # Calculate partial matches (if a retrieved source contains or is contained in a golden source)\n",
    "        partial_matches = []\n",
    "        for g_src in golden_sources:\n",
    "            for r_src in normalized_retrieved:\n",
    "                if g_src in r_src or r_src in g_src:\n",
    "                    if (g_src, r_src) not in partial_matches and (r_src, g_src) not in partial_matches:\n",
    "                        partial_matches.append((g_src, r_src))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = len(exact_matches) / len(normalized_retrieved) if normalized_retrieved else 0\n",
    "        recall = len(exact_matches) / len(golden_sources) if golden_sources else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"question_id\": question_id,\n",
    "            \"golden_sources\": golden_sources,\n",
    "            \"retrieved_sources\": normalized_retrieved,\n",
    "            \"exact_matches\": list(exact_matches),\n",
    "            \"partial_matches\": partial_matches,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "            \"retrieval_success\": len(exact_matches) > 0 or len(partial_matches) > 0\n",
    "        }\n",
    "    \n",
    "    def evaluate_answer(self, question_id: int, generated_answer: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate the quality of a generated answer against the golden answer\n",
    "        \n",
    "        Args:\n",
    "            question_id: The ID of the question to evaluate\n",
    "            generated_answer: The answer generated by the RAG system\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        # Get the golden answer for this question\n",
    "        question_row = self.golden_df[self.golden_df[\"ID\"] == question_id]\n",
    "        if question_row.empty:\n",
    "            return {\"error\": f\"Question ID {question_id} not found in golden test set\"}\n",
    "        \n",
    "        golden_answer = question_row[\"Answer\"].iloc[0]\n",
    "        question = question_row[\"Question\"].iloc[0]\n",
    "        \n",
    "        # Calculate semantic similarity\n",
    "        golden_embedding = self.encoder.encode([golden_answer])[0]\n",
    "        generated_embedding = self.encoder.encode([generated_answer])[0]\n",
    "        semantic_similarity = cosine_similarity([golden_embedding], [generated_embedding])[0][0]\n",
    "        \n",
    "        # Simple lexical overlap (Jaccard similarity)\n",
    "        golden_tokens = set(re.findall(r'\\b\\w+\\b', golden_answer.lower()))\n",
    "        generated_tokens = set(re.findall(r'\\b\\w+\\b', generated_answer.lower()))\n",
    "        \n",
    "        jaccard = len(golden_tokens.intersection(generated_tokens)) / len(golden_tokens.union(generated_tokens)) if golden_tokens or generated_tokens else 0\n",
    "        \n",
    "        return {\n",
    "            \"question_id\": question_id,\n",
    "            \"question\": question,\n",
    "            \"golden_answer\": golden_answer,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"semantic_similarity\": float(semantic_similarity),\n",
    "            \"lexical_overlap\": jaccard\n",
    "        }\n",
    "    \n",
    "    def run_evaluation(self, rag_results: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run a full evaluation on a batch of RAG results\n",
    "        \n",
    "        Args:\n",
    "            rag_results: List of dictionaries, each containing:\n",
    "                - question_id: The ID of the question\n",
    "                - retrieved_sources: List of source paths retrieved by the RAG\n",
    "                - generated_answer: The answer generated by the system\n",
    "                \n",
    "        Returns:\n",
    "            Dictionary with aggregated evaluation metrics\n",
    "        \"\"\"\n",
    "        retrieval_results = []\n",
    "        answer_results = []\n",
    "        \n",
    "        for result in rag_results:\n",
    "            question_id = result[\"question_id\"]\n",
    "            retrieved_sources = result.get(\"retrieved_sources\", [])\n",
    "            generated_answer = result.get(\"generated_answer\", \"\")\n",
    "            \n",
    "            retrieval_eval = self.evaluate_retrieval(question_id, retrieved_sources)\n",
    "            retrieval_results.append(retrieval_eval)\n",
    "            \n",
    "            answer_eval = self.evaluate_answer(question_id, generated_answer)\n",
    "            answer_results.append(answer_eval)\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        retrieval_success_rate = sum(1 for r in retrieval_results if r.get(\"retrieval_success\", False)) / len(retrieval_results) if retrieval_results else 0\n",
    "        avg_retrieval_precision = sum(r.get(\"precision\", 0) for r in retrieval_results) / len(retrieval_results) if retrieval_results else 0\n",
    "        avg_retrieval_recall = sum(r.get(\"recall\", 0) for r in retrieval_results) / len(retrieval_results) if retrieval_results else 0\n",
    "        avg_retrieval_f1 = sum(r.get(\"f1_score\", 0) for r in retrieval_results) / len(retrieval_results) if retrieval_results else 0\n",
    "        \n",
    "        avg_semantic_similarity = sum(a.get(\"semantic_similarity\", 0) for a in answer_results) / len(answer_results) if answer_results else 0\n",
    "        avg_lexical_overlap = sum(a.get(\"lexical_overlap\", 0) for a in answer_results) / len(answer_results) if answer_results else 0\n",
    "        \n",
    "        return {\n",
    "            \"num_questions_evaluated\": len(rag_results),\n",
    "            \"retrieval_metrics\": {\n",
    "                \"success_rate\": retrieval_success_rate,\n",
    "                \"avg_precision\": avg_retrieval_precision,\n",
    "                \"avg_recall\": avg_retrieval_recall,\n",
    "                \"avg_f1\": avg_retrieval_f1\n",
    "            },\n",
    "            \"answer_metrics\": {\n",
    "                \"avg_semantic_similarity\": float(avg_semantic_similarity),\n",
    "                \"avg_lexical_overlap\": avg_lexical_overlap\n",
    "            },\n",
    "            \"detailed_results\": {\n",
    "                \"retrieval\": retrieval_results,\n",
    "                \"answers\": answer_results\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def save_results(self, evaluation_results: Dict[str, Any], output_path: str = \"rag_evaluation_results.json\"):\n",
    "        \"\"\"Save evaluation results to a JSON file\"\"\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(evaluation_results, f, indent=2)\n",
    "        print(f\"Evaluation results saved to {output_path}\")\n",
    "        \n",
    "        # Also save a CSV summary of per-question results\n",
    "        with open(output_path.replace('.json', '_summary.csv'), 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                \"Question ID\", \"Question\", \"Retrieval Success\", \"Precision\", \"Recall\", \n",
    "                \"F1 Score\", \"Semantic Similarity\", \"Lexical Overlap\"\n",
    "            ])\n",
    "            \n",
    "            retrieval_results = evaluation_results[\"detailed_results\"][\"retrieval\"]\n",
    "            answer_results = evaluation_results[\"detailed_results\"][\"answers\"]\n",
    "            \n",
    "            for i in range(len(retrieval_results)):\n",
    "                r = retrieval_results[i]\n",
    "                a = answer_results[i]\n",
    "                writer.writerow([\n",
    "                    r.get(\"question_id\", \"N/A\"),\n",
    "                    a.get(\"question\", \"N/A\"),\n",
    "                    r.get(\"retrieval_success\", False),\n",
    "                    r.get(\"precision\", 0),\n",
    "                    r.get(\"recall\", 0),\n",
    "                    r.get(\"f1_score\", 0),\n",
    "                    a.get(\"semantic_similarity\", 0),\n",
    "                    a.get(\"lexical_overlap\", 0)\n",
    "                ])\n",
    "        print(f\"Summary CSV saved to {output_path.replace('.json', '_summary.csv')}\")\n",
    "\n",
    "# Example usage function to demonstrate how to use the evaluator with an existing RAG system\n",
    "def evaluate_rag_system(golden_test_set_path: str, rag_function, llm_function):\n",
    "    \"\"\"\n",
    "    Evaluate a RAG system against the golden test set\n",
    "    \n",
    "    Args:\n",
    "        golden_test_set_path: Path to the CSV file containing the golden test set\n",
    "        rag_function: Function that takes a question and returns retrieved sources\n",
    "        llm_function: Function that takes a question and retrieved chunks and returns an answer\n",
    "    \"\"\"\n",
    "    evaluator = RAGEvaluator(golden_test_set_path)\n",
    "    golden_df = evaluator.golden_df\n",
    "    \n",
    "    rag_results = []\n",
    "    \n",
    "    # Process each question in the golden test set\n",
    "    for _, row in golden_df.iterrows():\n",
    "        question_id = row[\"ID\"]\n",
    "        question = row[\"Question\"]\n",
    "        \n",
    "        print(f\"Evaluating question {question_id}: {question[:50]}...\")\n",
    "        \n",
    "        # Call your existing RAG retrieval function\n",
    "        retrieved_chunks = rag_function(question)\n",
    "        \n",
    "        # Extract just the source paths from the retrieved chunks\n",
    "        # Adjust this based on how your system returns sources\n",
    "        retrieved_sources = [chunk[\"source\"] for chunk in retrieved_chunks]\n",
    "        \n",
    "        # Call your existing LLM answer generation function\n",
    "        generated_answer = llm_function(question, retrieved_chunks)\n",
    "        \n",
    "        rag_results.append({\n",
    "            \"question_id\": question_id,\n",
    "            \"retrieved_sources\": retrieved_sources,\n",
    "            \"generated_answer\": generated_answer\n",
    "        })\n",
    "    \n",
    "    # Run the evaluation\n",
    "    evaluation_results = evaluator.run_evaluation(rag_results)\n",
    "    \n",
    "    # Save the results\n",
    "    evaluator.save_results(evaluation_results)\n",
    "    \n",
    "    # Print summary metrics\n",
    "    print(\"\\nEvaluation Summary:\")\n",
    "    print(f\"Number of questions evaluated: {evaluation_results['num_questions_evaluated']}\")\n",
    "    print(f\"Retrieval success rate: {evaluation_results['retrieval_metrics']['success_rate']:.2f}\")\n",
    "    print(f\"Average retrieval F1 score: {evaluation_results['retrieval_metrics']['avg_f1']:.2f}\")\n",
    "    print(f\"Average answer semantic similarity: {evaluation_results['answer_metrics']['avg_semantic_similarity']:.2f}\")\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "# Example of how to integrate with your existing RAG system\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these functions with your actual implementations\n",
    "    def my_rag_search(question):\n",
    "        # This should call your existing RAG search function\n",
    "        # Return format should be a list of dictionaries, each with at least a \"source\" field\n",
    "        # Example: [{\"source\": \"/content/handbook/values/index.md\", \"content\": \"...\"}]\n",
    "        db = sessionLocal()\n",
    "        results = hybrid_search(question, db)\n",
    "        chunks = [result['chunk'] for result in results]\n",
    "        chunks_dict = [{\"source\": chunk.document.location, \"content\": chunk.chunk} for chunk in chunks]\n",
    "        return chunks_dict\n",
    "        \n",
    "    \n",
    "    def my_llm_answer(question, retrieved_chunks):\n",
    "        # This should call your existing LLM answer generation function\n",
    "        # It should return the generated answer as a string\n",
    "        pass\n",
    "\n",
    "    \n",
    "    # Run the evaluation\n",
    "    evaluation_results = evaluate_rag_system(GOLDEN_TEST_PATH, my_rag_search, my_llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Source File</th>\n",
       "      <th>Relevant Section</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is GitLab's approach to compensation?</td>\n",
       "      <td>GitLab uses a competitive, market-based compen...</td>\n",
       "      <td>/content/handbook/total-rewards/compensation/i...</td>\n",
       "      <td>Under \"Compensation Principles\" section: \"We u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does GitLab handle remote work?</td>\n",
       "      <td>GitLab is an all-remote company where everyone...</td>\n",
       "      <td>/content/handbook/company/culture/all-remote/i...</td>\n",
       "      <td>Introduction section: \"GitLab is an all-remote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are GitLab's core values?</td>\n",
       "      <td>GitLab's six core values are: Collaboration, R...</td>\n",
       "      <td>/content/handbook/values/index.md</td>\n",
       "      <td>In the opening section: \"Our six values are Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is GitLab's approach to feature flags?</td>\n",
       "      <td>GitLab uses feature flags to separately deploy...</td>\n",
       "      <td>/content/handbook/product-development/feature-...</td>\n",
       "      <td>In the introduction: \"Feature flags are a powe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does GitLab handle database changes and mi...</td>\n",
       "      <td>GitLab uses migrations to handle database chan...</td>\n",
       "      <td>/content/handbook/engineering/infrastructure/d...</td>\n",
       "      <td>In the \"Types of Migrations\" section: \"In GitL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Question  \\\n",
       "ID                                                      \n",
       "1          What is GitLab's approach to compensation?   \n",
       "2                 How does GitLab handle remote work?   \n",
       "3                      What are GitLab's core values?   \n",
       "4         What is GitLab's approach to feature flags?   \n",
       "5   How does GitLab handle database changes and mi...   \n",
       "\n",
       "                                               Answer  \\\n",
       "ID                                                      \n",
       "1   GitLab uses a competitive, market-based compen...   \n",
       "2   GitLab is an all-remote company where everyone...   \n",
       "3   GitLab's six core values are: Collaboration, R...   \n",
       "4   GitLab uses feature flags to separately deploy...   \n",
       "5   GitLab uses migrations to handle database chan...   \n",
       "\n",
       "                                          Source File  \\\n",
       "ID                                                      \n",
       "1   /content/handbook/total-rewards/compensation/i...   \n",
       "2   /content/handbook/company/culture/all-remote/i...   \n",
       "3                   /content/handbook/values/index.md   \n",
       "4   /content/handbook/product-development/feature-...   \n",
       "5   /content/handbook/engineering/infrastructure/d...   \n",
       "\n",
       "                                     Relevant Section  \n",
       "ID                                                     \n",
       "1   Under \"Compensation Principles\" section: \"We u...  \n",
       "2   Introduction section: \"GitLab is an all-remote...  \n",
       "3   In the opening section: \"Our six values are Co...  \n",
       "4   In the introduction: \"Feature flags are a powe...  \n",
       "5   In the \"Types of Migrations\" section: \"In GitL...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(GOLDEN_TEST_PATH, delimiter=';', index_col=\"ID\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Evaluation criteria models\n",
    "class CriterionScore(BaseModel):\n",
    "    score: float = Field(..., ge=0, le=10, description=\"Score from 0-10\")\n",
    "    explanation: str = Field(..., description=\"Brief explanation for the score\")\n",
    "\n",
    "class ResponseEvaluation(BaseModel):\n",
    "    factual_accuracy: CriterionScore = Field(..., description=\"Measures correctness of information\")\n",
    "    completeness: CriterionScore = Field(..., description=\"Measures if all aspects of the question are addressed\")\n",
    "    relevance: CriterionScore = Field(..., description=\"Measures if information is directly relevant\")\n",
    "    clarity: CriterionScore = Field(..., description=\"Measures if the response is clear and understandable\")\n",
    "    conciseness: CriterionScore = Field(..., description=\"Measures if the response is appropriately concise\")\n",
    "    total_score: float = Field(..., ge=0, le=50, description=\"Sum of all scores\")\n",
    "    evaluation_summary: str = Field(..., description=\"Brief overall assessment\")\n",
    "\n",
    "class LLMJudge:\n",
    "    def __init__(self, model=\"gpt-4\"):\n",
    "        self.model = model\n",
    "    \n",
    "    def evaluate_response(self, question: str, rag_response: str, golden_answer: str) -> ResponseEvaluation:\n",
    "        prompt = f\"\"\"You are an expert evaluator of RAG system responses.\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        RAG Response: {rag_response}\n",
    "        \n",
    "        Golden Answer: {golden_answer}\n",
    "        \n",
    "        Evaluate the RAG response against the golden answer based on factual accuracy, completeness, relevance, clarity, and conciseness.\n",
    "        \n",
    "        Each criterion should be scored from 0-10.\n",
    "        \n",
    "        Return a structured evaluation with scores and brief explanations for each category.\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert evaluator of AI assistant responses.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        result = ResponseEvaluation.model_validate_json(response.choices[0].message.content)\n",
    "        return result\n",
    "\n",
    "    def batch_evaluate(self, evaluation_data: List[Dict]) -> List[ResponseEvaluation]:\n",
    "        results = []\n",
    "        for item in evaluation_data:\n",
    "            result = self.evaluate_response(\n",
    "                item[\"question\"],\n",
    "                item[\"rag_response\"],\n",
    "                item[\"golden_answer\"]\n",
    "            )\n",
    "            results.append(result)\n",
    "        return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
