{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This journal contains the pipeline to download data from the GitLab handbook and store it into a LanceDB Database with embeddings to be used for RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Saxion\\Jaar4\\Afstudeerstage\\chatbot_git\\ChatbotExperiment\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from lancedb.embeddings import get_registry\n",
    "from typing import List\n",
    "from pydantic_ai import Agent\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai.models.gemini import GeminiModel\n",
    "import os\n",
    "import datetime\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "import re\n",
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download gitlab handbook contents from gitlab and store it in Data folder\n",
    "This code currently stops halfway while retrieving the data, so in the mean time we will continue using a predownloaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote\n",
    "\n",
    "def download_files_in_folder(project_id, file, save_path):\n",
    "    \"\"\"\n",
    "    Download current markdown file from gitlab project into local folder.\n",
    "\n",
    "    args:\n",
    "    project\n",
    "    \"\"\"\n",
    "    file_path_encoded = quote(file[\"path\"], safe='')\n",
    "    file_url = f'https://gitlab.com/api/v4/projects/{project_id}/repository/files/{file_path_encoded}/raw'\n",
    "    file_params = {'ref': 'main'}\n",
    "    file_response = requests.get(file_url, params=file_params)\n",
    "    file_response.raise_for_status()\n",
    "\n",
    "    print(file_url)\n",
    "    local_file_path = os.path.join(save_path, file['path'])\n",
    "    os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "\n",
    "    with open(local_file_path, 'wb') as f:\n",
    "        f.write(file_response.content)\n",
    "    print(f'Downloaded: {file[\"path\"]} to {local_file_path}')\n",
    "\n",
    "def download_gitlab_data(project_id, gitlab_path, save_path):\n",
    "    api_url = f'https://gitlab.com/api/v4/projects/{project_id}/repository/tree'\n",
    "    params = {\n",
    "        'path': gitlab_path,\n",
    "        'recursive': True\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(api_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        files = response.json()\n",
    "\n",
    "        for file in files:\n",
    "            if file['type'] == 'blob' and file['name'].endswith('.md'):\n",
    "                download_files_in_folder(project_id, file, save_path)\n",
    "            else:\n",
    "                download_gitlab_data(project_id, f'{file[\"path\"]}', save_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occured at {gitlab_path}: {e}\")\n",
    "\n",
    "PROJECT_ID = \"gitlab-com%2Fcontent-sites%2Fhandbook\"\n",
    "FOLDER_PATH = \"content\"\n",
    "SAVE_PATH = \"../data/gitlab_handbook\"\n",
    "\n",
    "download_gitlab_data(PROJECT_ID, FOLDER_PATH, SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets focus on 1 MD file first to test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"../data/handbook-main-content/content/handbook/communication/_index.md\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "\n",
    "def split_markdown_text(file_path: str, embedding_model: str, min_tokens: int = 200, max_tokens: int = 1000, overlap: int = 30):\n",
    "    md_regex = r\"(^#+\\s*.*)\" #regex which captures all levels of headers in markdown.\n",
    "    tokenizer = tiktoken.encoding_for_model(embedding_model)\n",
    "    text = \"\"\n",
    "    #read text\n",
    "    try:\n",
    "        file = open(file_path, 'r', encoding=\"utf-8\")\n",
    "        text = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"error while reading file: {e}\")\n",
    "\n",
    "    #split text by headers\n",
    "    sections = re.split(md_regex, text, flags=re.MULTILINE)\n",
    "    chunks = []\n",
    "    temp_chunk = \"\"\n",
    "    \n",
    "\n",
    "    for i in range(1, len(sections), 2): # loop through headers and text in sections\n",
    "        \n",
    "        header = sections[i].strip()\n",
    "        content = sections[i+1].strip() if i + 1 <= len(sections) else \"\"\n",
    "        chunk = header + '\\n' + content\n",
    "        token_count = len(tokenizer.encode(chunk))\n",
    "\n",
    "        # add chunk to chunk list or to temporary chunk to combine with other chunks\n",
    "        if token_count < min_tokens:\n",
    "            temp_chunk += chunk + \"\\n\"          \n",
    "        else:\n",
    "            if temp_chunk:\n",
    "                chunks.append(temp_chunk)\n",
    "                temp_chunk = \"\"\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    # add remaining temp chunk if it exists\n",
    "    if temp_chunk:\n",
    "        chunks.append(temp_chunk)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size = max_tokens, chunk_overlap = overlap)\n",
    "    split_chunks = []\n",
    "    for chunk in chunks:\n",
    "        split_chunks.extend(splitter.split_text(chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "chunks = split_markdown_text(FILE_PATH, EMBEDDING_MODEL)\n",
    "tokenizer = tiktoken.encoding_for_model(EMBEDDING_MODEL)\n",
    "print(len(chunks))\n",
    "print(len(tokenizer.encode(chunks[2])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split all markdown files in the handbook folder up into chunks.\n",
    "Chunks are split up by headings and large sections of text will be split up further into chunks of 1000 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67102\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def split_markdown_text(text: str, embedding_model: str, min_tokens: int = 200, max_tokens: int = 1000, overlap: int = 30):\n",
    "    md_regex = r\"(^#+\\s*.*)\" #regex which captures all levels of headers in markdown.\n",
    "    tokenizer = tiktoken.encoding_for_model(embedding_model)\n",
    "\n",
    "    #split text by headers\n",
    "    sections = re.split(md_regex, text, flags=re.MULTILINE)\n",
    "    chunks = []\n",
    "    temp_chunk = \"\"\n",
    "    \n",
    "    #capture first text which often does not start with a header\n",
    "    \n",
    "    if len(tokenizer.encode(sections[0])) < min_tokens:\n",
    "        temp_chunk = sections[0]\n",
    "    else:\n",
    "        chunks.append(sections[0])\n",
    "    \n",
    "\n",
    "    for i in range(1, len(sections), 2): # loop through headers and text in sections\n",
    "        header = sections[i].strip()\n",
    "        content = sections[i+1].strip() if i + 1 <= len(sections) else \"\"\n",
    "        chunk = header + '\\n' + content\n",
    "        token_count = len(tokenizer.encode(chunk))\n",
    "\n",
    "        # add chunk to chunk list or to temporary chunk to combine with other chunks\n",
    "        if token_count < min_tokens:\n",
    "            temp_chunk += chunk + \"\\n\"          \n",
    "        else:\n",
    "            if temp_chunk:\n",
    "                chunks.append(temp_chunk)\n",
    "                temp_chunk = \"\"\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    # add remaining temp chunk if it exists\n",
    "    if temp_chunk:\n",
    "        chunks.append(temp_chunk)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size = max_tokens, chunk_overlap = overlap)\n",
    "    split_chunks = []\n",
    "    for chunk in chunks:\n",
    "        split_chunks.extend(splitter.split_text(chunk))\n",
    "    \n",
    "    return split_chunks\n",
    "\n",
    "def extract_text_from_data(folder_path):\n",
    "    file_chunks = []\n",
    "    id = 0\n",
    "    HANDBOOK_ROOT_URL = \"https://gitlab.com/gitlab-com/content-sites/handbook/-/tree/main/content\"\n",
    "\n",
    "    # walk through all folders and subfolders\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.md'):                  #only extract text from markdown files\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_url = file_path.replace(\"../data/handbook-main-content\\\\content\", HANDBOOK_ROOT_URL)\n",
    "                file_url = file_url.replace('\\\\', '/')\n",
    "               \n",
    "                with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                    chunks = split_markdown_text(content, EMBEDDING_MODEL, min_tokens=200)\n",
    "                    for chunk in chunks:\n",
    "\n",
    "                        file_chunks.append((id, file_url, chunk))\n",
    "                        id += 1\n",
    "                    \n",
    "    return file_chunks\n",
    "\n",
    "DATA_PATH = \"../data/handbook-main-content\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file_chunks = extract_text_from_data(DATA_PATH)\n",
    "# def create_chunks_df(file_chunks: List[(str, List[str])]):\n",
    "#     id = 0\n",
    "print(len(file_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---\\ntitle: The Handbook\\nno_list: true\\nmenu:\\n  main:\\n    name: Handbook\\n    pre: \\'<i class=\"fa-solid fa-book\"></i>\\'\\ncascade:\\n      type: docs\\n---\\n\\n## Introduction\\nThe GitLab team handbook is the central repository for how we run the company. Printed, it consists of over\\n[2,000 pages of text](/handbook/about/#count-handbook-pages). As part of our value of being\\ntransparent the handbook is [open to the world](https://gitlab.com/gitlab-com/content-sites/handbook/), and we welcome\\nfeedback. Please make a [merge request](https://gitlab.com/gitlab-com/content-sites/handbook/merge_requests) to suggest\\nimprovements or add clarifications. Please use [issues](https://gitlab.com/gitlab-com/content-sites/handbook/issues) to\\nask questions.\\n\\nFor a very specific set of [internal](/handbook/communication/confidentiality-levels/#internal)\\ninformation we also maintain an [Internal Handbook](https://internal.gitlab.com/handbook/)\\n## Handbook Contents\\n{{< cardpane >}}'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=file_chunks, columns=['chunk_id','source_url', 'chunk'])\n",
    "df.iloc[0]['chunk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count total tokens and estimate embedding cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(EMBEDDING_MODEL)\n",
    "total_count = 0\n",
    "for id, file, chunk in file_chunks:\n",
    "    token_count = len(tokenizer.encode(chunk))\n",
    "    total_count += token_count\n",
    "print(f\"total token count = {total_count}\")\n",
    "print(f\"API costs at ~0.1 dollar per million tokens equals {total_count/1000000 * 0.1} dollar\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove small chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14056\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(EMBEDDING_MODEL)\n",
    "total_count = 0\n",
    "trimmed_file_chunks = []\n",
    "for id, file, chunk in file_chunks:\n",
    "    \n",
    "    count = len(tokenizer.encode(chunk))\n",
    "    if count > 200:\n",
    "        trimmed_file_chunks.append((id, file, chunk))\n",
    "\n",
    "print(len(trimmed_file_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = OpenAI()\n",
    "\n",
    "#FIX SHOULD RETURN A LIST OF FLOATS INSTEAD OF STRING\n",
    "def get_embeddings(chunks, model=EMBEDDING_MODEL):\n",
    "    try:\n",
    "        response = client.embeddings.create(input = chunks, model=model)\n",
    "        print(\"batch done\")\n",
    "        return [item.embedding for item in response.data]\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding failed with error: {e}\")\n",
    "    return [None] * len(chunks)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"## Where we are Headed\\nAt GitLab, we encourage everyone to work [handbook first](/handbook/about/handbook-usage/#why-handbook-first) in order to promote asynchronous collaboration and documentation. Working this way has its challenges, not the least of which is the time and effort involved in making a change. While this extra investment can encourage contributors to be more considered and deliberate with their changes, at a certain point it discourages meaningful collaboration and works against our goals.\\n\\nOur hope is that the GitLab Handbook is something that others want to emulate. To facilitate that, we want to ensure that any user can easily use and update the handbook. Ideally, the handbook has:\\n\\n- Organized, and up-to-date content\\n- Fast, predictable deployments\\n- A clean, scalable information architecture and modern codebase\\n### What's Next & Why\\nWe are focused on maintaining the handbook's fast (pipeline under 10 minutes) and stable (minimal pipeline failures) state.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=trimmed_file_chunks, columns=['chunk_id','source_url', 'chunk'])\n",
    "df.iloc[1]['chunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandbookChunk2(LanceModel):\n",
    "    chunk_id: str\n",
    "    source_url: str\n",
    "    chunk: str = func.SourceField()\n",
    "    vector: Vector(func.ndims()) = func.VectorField()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "func = get_registry().get(\"openai\").create(name=\"text-embedding-3-large\")\n",
    "\n",
    "class HandbookChunk2(LanceModel):\n",
    "    chunk_id: str\n",
    "    source_url: str\n",
    "    chunk: str = func.SourceField()\n",
    "    vector: Vector(func.ndims()) = func.VectorField()\n",
    "\n",
    "    \n",
    "db = lancedb.connect(\"../data/lancedb\")\n",
    "\n",
    "\n",
    "table = db.create_table(\"embedded_handbook_with_urls\", schema = HandbookChunk2)\n",
    "\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    table.add(df.iloc[i:i + batch_size])\n",
    "    break\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "\n",
    "for i in range(0, len(df), batch_size):\n",
    "    table.add(df.iloc[i:i + batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "chunk_id: string not null\n",
      "source_url: string not null\n",
      "chunk: string not null\n",
      "vector: fixed_size_list<item: float>[3072]\n",
      "  child 0, item: float\n",
      "----\n",
      "chunk_id: [[\"1\",\"9\",\"12\",\"16\",\"25\"]]\n",
      "source_url: [[\"https://gitlab.com/gitlab-com/content-sites/handbook/-/tree/main/content/handbook/_index.md\",\"https://gitlab.com/gitlab-com/content-sites/handbook/-/tree/main/content/handbook/about/direction.md\",\"https://gitlab.com/gitlab-com/content-sites/handbook/-/tree/main/content/handbook/about/escalation.md\",\"https://gitlab.com/gitlab-com/content-sites/handbook/-/tree/main/content/handbook/about/escalation.md\",\"https://gitlab.com/gitlab-com/content-sites/handbook/-/tree/main/content/handbook/about/handbook-usage.md\"]]\n",
      "chunk: [[\"---\n",
      "title: The Handbook\n",
      "no_list: true\n",
      "menu:\n",
      "  main:\n",
      "    name: Handbook\n",
      "    pre: '<i class=\"fa-solid fa-book\"></i>'\n",
      "cascade:\n",
      "      type: docs\n",
      "---\n",
      "\n",
      "## Introduction\n",
      "The GitLab team handbook is the central repository for how we run the company. Printed, it consists of over\n",
      "[2,000 pages of text](/handbook/about/#count-handbook-pages). As part of our value of being\n",
      "transparent the handbook is [open to the world](https://gitlab.com/gitlab-com/content-sites/handbook/), and we welcome\n",
      "feedback. Please make a [merge request](https://gitlab.com/gitlab-com/content-sites/handbook/merge_requests) to suggest\n",
      "improvements or add clarifications. Please use [issues](https://gitlab.com/gitlab-com/content-sites/handbook/issues) to\n",
      "ask questions.\n",
      "\n",
      "For a very specific set of [internal](/handbook/communication/confidentiality-levels/#internal)\n",
      "information we also maintain an [Internal Handbook](https://internal.gitlab.com/handbook/)\n",
      "## Handbook Contents\n",
      "{{< cardpane >}}\",\"## Where we are Headed\n",
      "At GitLab, we encourage everyone to work [handbook first](/handbook/about/handbook-usage/#why-handbook-first) in order to promote asynchronous collaboration and documentation. Working this way has its challenges, not the least of which is the time and effort involved in making a change. While this extra investment can encourage contributors to be more considered and deliberate with their changes, at a certain point it discourages meaningful collaboration and works against our goals.\n",
      "\n",
      "Our hope is that the GitLab Handbook is something that others want to emulate. To facilitate that, we want to ensure that any user can easily use and update the handbook. Ideally, the handbook has:\n",
      "\n",
      "- Organized, and up-to-date content\n",
      "- Fast, predictable deployments\n",
      "- A clean, scalable information architecture and modern codebase\n",
      "### What's Next & Why\n",
      "We are focused on maintaining the handbook's fast (pipeline under 10 minutes) and stable (minimal pipeline failures) state.\",\"---\n",
      "title: Handbook Escalation\n",
      "---\n",
      "\n",
      "For information on team members' roles and responsibilities, see [Content Websites page](maintenance.md).\n",
      "\n",
      "## Introduction\n",
      "The Handbook is a critical part of empowering team members to do their jobs effectively. As such, we have a group of team members who assist in resolving issues affecting all team members.\n",
      "## Reporting an issue\n",
      "If you're looking for general help, please see the [editing handbook page](editing-handbook/_index.md#need-help).\n",
      "\n",
      "Any work stopping issues should be reported in the [#handbook-escalation](https://gitlab.slack.com/archives/CVDP3HG5V) channel in Slack.\n",
      "Otherwise, consider creating an issue in the relevant [content sites repository](https://gitlab.com/gitlab-com/content-sites/) and posting in the [#handbook Slack channel](https://gitlab.enterprise.slack.com/archives/C81PT2ALD).\n",
      "### When to escalate an issue\n",
      "Issues should only be escalated if it relates to:\",\"### Expectations for the group\n",
      "1. Make sure you are in and do not mute the [#handbook-escalation](https://gitlab.slack.com/archives/CVDP3HG5V) channel.\n",
      "1. When an issue is reported:\n",
      "   1. Acknowledge the team member and let them know you are looking into it.\n",
      "   1. You can check on `#production`, `#incident-management`, and `#is-this-known` to see if it's a known issue with infrastructure or other problems.\n",
      "   1. Provide an update as soon as you are able to confirm their problem.\n",
      "   1. You can also post updates in `#mr-buddies` and/or `#handbook` as appropriate.\n",
      "   1. Offer to have a Zoom call to help replicate or resolve the issue if it is not straight forward.\n",
      "   1. Resolve the problem, or provide feedback to the team member on how they can resolve it.\n",
      "   1. If you do not believe you can resolve it and need further assistance, consider any or all of the following:\n",
      "      1. Ping another member of the [Keep main green group](#keep-main-green-group).\",\"> The biggest problem is GitLab not working handbook first. We have an amazing handbook that allows us to collaborate, onboard new people, and think collectively.\n",
      ">\n",
      "> However, it is counterintuitive to communicate changes to the handbook. The default of people, when they wish to communicate a change, is to send a Slack message, send an email, give a presentation, or tell people in a meeting ‚Äî *anything* but make a change in the handbook.\n",
      ">\n",
      "> It's slower for them. It's quicker to use any other form. If they make a change in the handbook, they first have to find the relevant part of the handbook, they sometimes have to adjust the handbook to make sure their change will fit in, they have to go through a technical process and a review or two, and they have to wait a bit before it's deployed.\n",
      ">\n",
      "> It's slower than any other option. However, it allows people that commit a change after that to build upon a change. When they take that extra time, it's building a foundation for the next thing.\"]]\n",
      "vector: [[[0.0024760664,-0.017002448,-0.01682141,0.0036245238,0.006287285,...,0.011013121,-0.002106448,-0.0026250454,-0.010130563,-0.0038263053],[-0.020737102,-0.019688683,-0.02371276,-0.005045514,-0.000458924,...,-0.0011293626,-0.005619832,-0.012311209,0.005141876,-0.011879507],[-0.035806317,-0.013767879,-0.022873918,-0.04212508,-0.015909238,...,-0.0069120233,0.017467866,-0.020753622,-0.0057360316,-0.005750073],[-0.0043372875,-0.027142268,-0.028737951,-0.011021169,-0.0050569093,...,0.01438462,0.01051274,-0.017067559,0.0013815571,0.01347727],[-0.023537485,-0.0028252907,-0.021381864,-0.024298294,-0.027468326,...,0.006823493,-0.0018584311,-0.007219747,0.025804058,-0.0018237588]]]\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "db = lancedb.connect(\"../data/lancedb\")\n",
    "\n",
    "table = db.open_table(\"embedded_handbook_with_urls\")\n",
    "print(table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector dimension: 3072\n"
     ]
    }
   ],
   "source": [
    "# Get a sample embedding\n",
    "sample_row = table.to_pandas().iloc[0]  # Get the first row\n",
    "sample_embedding = sample_row[\"vector\"]\n",
    "\n",
    "# Print the dimension\n",
    "print(f\"Vector dimension: {len(sample_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanitising text for reranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>creating history table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChatHistory(LanceModel):\n",
    "    session_id: str\n",
    "    share_token: str\n",
    "    history: str\n",
    "\n",
    "db = lancedb.connect(\"../data/lancedb\")\n",
    "db.drop_table('history_table')\n",
    "table = db.create_table(\"history_table\", schema = ChatHistory)\n",
    "table.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import json\n",
    "\n",
    "history = []\n",
    "history.append({'role': \"user\", \"content\": 'question'})\n",
    "session_id = str(uuid.uuid4())\n",
    "share_token = str(uuid.uuid4())\n",
    "data = [ChatHistory(session_id = session_id, share_token = share_token, history= json.dumps(history))]\n",
    "table.add(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check history table after testing chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = table.head(n=10)\n",
    "print(table.head())\n",
    "print(len(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = table.search().where(f\"session_id = '{session_id}'\").limit(1).to_pydantic(ChatHistory)\n",
    "\n",
    "if results:\n",
    "    print(results[0].history)\n",
    "\n",
    "history_list = []\n",
    "history_list.append({'role': \"assistant\", \"content\": 'answer'})\n",
    "print(history_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_chat_history(table, session_id: str, share_token: str, new_history: str):\n",
    "        results = table.search().where(f\"session_id = '{session_id}'\").limit(1).to_list()\n",
    "        if results:\n",
    "            table.update(where=f\"session_id = '{session_id}'\", values={'history': new_history})\n",
    "        else:\n",
    "           \n",
    "            table.add([{\"session_id\" : session_id, \"share_token\": share_token, \"history\": new_history}])\n",
    "\n",
    "update_chat_history(table, session_id, share_token, json.dumps(history_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> migrating to PostGres and SQLalchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, LargeBinary\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker, Session, scoped_session\n",
    "from sqlalchemy.dialects.postgresql import UUID\n",
    "from pgvector.sqlalchemy import Vector\n",
    "\n",
    "from sqlalchemy.sql import func\n",
    "import uuid\n",
    "from sqlalchemy import (\n",
    "    Column,\n",
    "    Integer,\n",
    "    String,\n",
    "    Boolean,\n",
    "    DateTime,\n",
    "    ForeignKey,\n",
    "    JSON,\n",
    "    Text,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()\n",
    "\n",
    "# declare models\n",
    "class ChatSession(Base):\n",
    "    __tablename__ = \"chat_session\"\n",
    "    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n",
    "    share_token = Column(String, unique=True, index=True)\n",
    "    created_at = Column(DateTime(timezone=True), server_default=func.now())\n",
    "    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n",
    "\n",
    "    class Config:\n",
    "        orm_mode = True\n",
    "\n",
    "\n",
    "class ChatMessage(Base):\n",
    "    __tablename__ = \"chat_message\"\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    session_id = Column(UUID(as_uuid=True), ForeignKey(\"chat_session.id\"), index=True)\n",
    "    question = Column(Text)\n",
    "    answer = Column(Text)\n",
    "    language = Column(String)\n",
    "    message_metadata = Column(JSON)\n",
    "    sources = Column(JSON)\n",
    "    tools_used = Column(JSON)\n",
    "    able_to_answer = Column(Boolean, default=True)\n",
    "    question_classification = Column(String)\n",
    "    trace_id = Column(String, index=True)\n",
    "    created_at = Column(DateTime(timezone=True), server_default=func.now())\n",
    "\n",
    "    class Config:\n",
    "        orm_mode = True\n",
    "\n",
    "class Chunk(Base):\n",
    "    __tablename__ = \"chunks\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    source = Column(String, nullable=False)\n",
    "    chunk = Column(String, nullable=False)\n",
    "    embedding = Column(Vector(3072))  # Adjust vector dimension to match your embeddings\n",
    "    \n",
    "    class Config:\n",
    "        orm_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_URL = \"postgresql+psycopg://postgres:password@localhost:5432/handbook_db\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "SessionLocal = sessionmaker(bind=engine)\n",
    "\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to lancedb\n",
    "db = lancedb.connect(\"../data/lancedb\")\n",
    "table = db.open_table(\"embedded_handbook_with_urls\")\n",
    "handbook_df = table.to_lance().to_table().to_pandas()\n",
    "\n",
    "# connect to postgreSQL\n",
    "session = SessionLocal()\n",
    "\n",
    "# migrate data\n",
    "for row in handbook_df.itertuples(index=False):\n",
    "    new_chunk = Chunk(\n",
    "        source=row.source_url,\n",
    "        chunk=row.chunk,\n",
    "        embedding=row.vector  # Ensure this is a NumPy array or list\n",
    "    )\n",
    "    session.add(new_chunk)\n",
    "\n",
    "session.commit()\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---\\ntitle: The Handbook\\nno_list: true\\nmenu:\\n  main:\\n    name: Handbook\\n    pre: \\'<i class=\"fa-solid fa-book\"></i>\\'\\ncascade:\\n      type: docs\\n---\\n\\n## Introduction\\nThe GitLab team handbook is the central repository for how we run the company. Printed, it consists of over\\n[2,000 pages of text](/handbook/about/#count-handbook-pages). As part of our value of being\\ntransparent the handbook is [open to the world](https://gitlab.com/gitlab-com/content-sites/handbook/), and we welcome\\nfeedback. Please make a [merge request](https://gitlab.com/gitlab-com/content-sites/handbook/merge_requests) to suggest\\nimprovements or add clarifications. Please use [issues](https://gitlab.com/gitlab-com/content-sites/handbook/issues) to\\nask questions.\\n\\nFor a very specific set of [internal](/handbook/communication/confidentiality-levels/#internal)\\ninformation we also maintain an [Internal Handbook](https://internal.gitlab.com/handbook/)\\n## Handbook Contents\\n{{< cardpane >}}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handbook_df.iloc[0]['chunk']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_factory = scoped_session(SessionLocal)\n",
    "session = session_factory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "engine = create_engine(DATABASE_URL)\n",
    "SessionLocal = sessionmaker(bind=engine)\n",
    "session = SessionLocal()\n",
    "session_factory = scoped_session(SessionLocal)\n",
    "session = session_factory()\n",
    "query_embedding = openai.embeddings.create(\n",
    "                input = 'gitlab core values',\n",
    "                model=\"text-embedding-3-large\"\n",
    "\n",
    "            ).data[0].embedding\n",
    "\n",
    "query_vector = np.array(query_embedding).tolist()\n",
    "            \n",
    "results = (\n",
    "    session.query(Chunk)\n",
    "    .order_by(Chunk.embedding.l2_distance(query_vector))  # L2 distance for similarity\n",
    "    .limit(5)\n",
    "    .all()\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## General Principles\n",
      "We believe the [GitLab core values](/handbook/values/) should guide all our\n",
      "coding decisions:\n",
      "\n",
      "- Collaboration\n",
      "  - Collaborate with teammates, review each other‚Äôs code, and share knowledge.\n",
      "  - Always be open to feedback and improve your code based on it.\n",
      "  - Ensure your contributions are readable and understandable to others on the\n",
      "    team\n",
      "- Results\n",
      "  - Focus on the impact and effectiveness of the code.\n",
      "  - Prioritize working solutions over perfect solutions (but try to avoid\n",
      "    shortcuts that undermine quality)\n",
      "- Efficiency\n",
      "  - Write clear and maintainable code.\n",
      "  - Strive for simplicity without sacrificing functionality.\n",
      "- Transparency\n",
      "  - Code should be easy to understand and accessible to all.\n",
      "  - Document assumptions, choices, and design decisions clearly when needed.\n",
      "- Iteration\n",
      "  - Code should be flexible and easy to refactor.\n",
      "  - We aim for continuous improvement and don't shy away from updating or\n",
      "    reworking code when necessary.\n",
      "We do this by having quality code, tests, documentation, popular frameworks,\n",
      "and offering a comprehensive [GitLab Development Kit](https://gitlab.com/gitlab-org/gitlab-development-kit)\n",
      "and a dedicated [GitLab Design System](https://design.gitlab.com/).\n",
      "We use GitLab at GitLab Inc., we [dogfood](/handbook/product/product-processes/#dogfood-everything)\n",
      "it and make it a tool we continue to love. We celebrate contributions by\n",
      "recognizing a Most Valuable Person (MVP) every month.\n",
      "We allow everyone to anticipate, propose, discuss, and contribute features by having everything on\n",
      "a public issue tracker. We ship a new version every month so contributions\n",
      "and feedback are visible fast. To contribute to open source software, people\n",
      "must be empowered to learn programming.\n",
      "That is why we sponsor initiatives such as Rails Girls.\n",
      "There are a few significant, but often overlooked, nuances of the **enabling everyone to contribute to GitLab, the application** mantra:\n",
      "---\n",
      "title: GitLab Values\n",
      "description: Learn more about how we live our values at GitLab\n",
      "canonical_path: \"/handbook/values/\"\n",
      "images:\n",
      "    - /images/opengraph/gitlab_values_handbook_social_card.png\n",
      "no_list: true\n",
      "weight: -20\n",
      "---\n",
      "\n",
      "## CREDIT\n",
      "<!-- markdownlint-disable MD001 MD051 -->\n",
      "GitLab's six core values are\n",
      "[**ü§ù Collaboration**](#collaboration),\n",
      "[**üìà Results for Customers**](#results),\n",
      "[**‚è±Ô∏è Efficiency**](#efficiency),\n",
      "[**üåê Diversity, Inclusion & Belonging**](#diversity-inclusion),\n",
      "[**üë£ Iteration**](#iteration), and\n",
      "[**üëÅÔ∏è Transparency**](#transparency),\n",
      "and together they spell the **CREDIT** we give each other by assuming\n",
      "good intent. We react to them [with values emoji](/handbook/communication/#values-emoji)\n",
      "and they are made actionable below.\n",
      "New team members should read [GitLab's guide to starting a new remote role](/handbook/company/culture/all-remote/getting-started/), and reference [interviews](https://www.youtube.com/playlist?list=PL05JrBw4t0Kq7QUX-Ux5fOunQotqJbECc) centered on values within the [GitLab Unfiltered YouTube channel](https://www.youtube.com/channel/UCMtZ0sc1HHNtGGWZFDRTh5A/search?query=values).\n",
      "## Mission\n",
      "Our [mission](/handbook/company/mission/#mission) is to **enable everyone to contribute to and co-create the software that powers our world**. This mission guides our path, and we live our values along that path.\n",
      "## Mitigating Concerns\n",
      "We have a page which documents our [Mitigating Concerns](https://internal.gitlab.com/handbook/leadership/mitigating-concerns/). Many of our values help to mitigate some of these concerns.\n",
      "## GitLab Values Quiz\n",
      "Culture is created when a company's values are prescriptively articulated and visibly [reinforced](values#how-do-we-reinforce-our-values) through elements such as [discretionary bonuses](/handbook/total-rewards/incentives/#discretionary-bonuses) and linking promotions to values.\n",
      "\n",
      "For new team members, this is even more crucial: if your culture is not self-reinforcing in a remote environment, it may not translate during the onboarding period.\n",
      "## Remote first company values\n",
      "GitLab's six core values are\n",
      "[**ü§ù Collaboration**](/handbook/values/#collaboration),\n",
      "[**üìà Results**](/handbook/values/#results),\n",
      "[**‚è±Ô∏è Efficiency**](/handbook/values/#efficiency),\n",
      "[**üåê Diversity, Inclusion & Belonging**](/handbook/values/#diversity-inclusion),\n",
      "[**üë£ Iteration**](/handbook/values/#iteration), and\n",
      "[**üëÅÔ∏è Transparency**](/handbook/values/#transparency),\n",
      "and together they spell the **CREDIT** we give each other by assuming good intent.\n"
     ]
    }
   ],
   "source": [
    "for chunk in results:\n",
    "    print(chunk.chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
